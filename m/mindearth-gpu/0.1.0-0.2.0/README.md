# Comparing `tmp/mindearth_gpu-0.1.0-py3-none-any.whl.zip` & `tmp/mindearth_gpu-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,37 @@
-Zip file size: 63444 bytes, number of entries: 35
--rw-r--r--  2.0 unx     2769 b- defN 23-Oct-16 07:06 mindearth/__init__.py
--rw-r--r--  2.0 unx     2195 b- defN 23-Oct-16 07:06 mindearth/setup.py
--rw-r--r--  2.0 unx     1042 b- defN 23-Oct-16 07:06 mindearth/cell/__init__.py
--rw-r--r--  2.0 unx     3495 b- defN 23-Oct-16 07:06 mindearth/cell/activation.py
--rw-r--r--  2.0 unx    16851 b- defN 23-Oct-16 07:06 mindearth/cell/utils.py
--rw-r--r--  2.0 unx      729 b- defN 23-Oct-16 07:06 mindearth/cell/demnet/__init__.py
--rw-r--r--  2.0 unx     5222 b- defN 23-Oct-16 07:06 mindearth/cell/demnet/demnet.py
--rw-r--r--  2.0 unx      784 b- defN 23-Oct-16 07:06 mindearth/cell/dgmr/__init__.py
--rw-r--r--  2.0 unx    36610 b- defN 23-Oct-16 07:06 mindearth/cell/dgmr/dgmr.py
--rw-r--r--  2.0 unx     6525 b- defN 23-Oct-16 07:06 mindearth/cell/dgmr/dgmrnet.py
--rw-r--r--  2.0 unx      747 b- defN 23-Oct-16 07:06 mindearth/cell/graphcast/__init__.py
--rw-r--r--  2.0 unx    13760 b- defN 23-Oct-16 07:06 mindearth/cell/graphcast/graphcast.py
--rw-r--r--  2.0 unx    10037 b- defN 23-Oct-16 07:06 mindearth/cell/graphcast/graphcastnet.py
--rw-r--r--  2.0 unx      770 b- defN 23-Oct-16 07:06 mindearth/cell/neural_operators/__init__.py
--rw-r--r--  2.0 unx    11252 b- defN 23-Oct-16 07:06 mindearth/cell/neural_operators/afno2d.py
--rw-r--r--  2.0 unx     4910 b- defN 23-Oct-16 07:06 mindearth/cell/neural_operators/afnonet.py
--rw-r--r--  2.0 unx    15127 b- defN 23-Oct-16 07:06 mindearth/cell/neural_operators/dft.py
--rw-r--r--  2.0 unx     8364 b- defN 23-Oct-16 07:06 mindearth/cell/neural_operators/vit_kno.py
--rw-r--r--  2.0 unx      877 b- defN 23-Oct-16 07:06 mindearth/core/__init__.py
--rw-r--r--  2.0 unx     3315 b- defN 23-Oct-16 07:06 mindearth/core/losses.py
--rw-r--r--  2.0 unx     4669 b- defN 23-Oct-16 07:06 mindearth/core/lr_scheduler.py
--rw-r--r--  2.0 unx      850 b- defN 23-Oct-16 07:06 mindearth/data/__init__.py
--rw-r--r--  2.0 unx    23212 b- defN 23-Oct-16 07:06 mindearth/data/dataset.py
--rw-r--r--  2.0 unx      788 b- defN 23-Oct-16 07:06 mindearth/module/__init__.py
--rw-r--r--  2.0 unx    13229 b- defN 23-Oct-16 07:06 mindearth/module/forecast.py
--rw-r--r--  2.0 unx    12123 b- defN 23-Oct-16 07:06 mindearth/module/pretrain.py
--rw-r--r--  2.0 unx      980 b- defN 23-Oct-16 07:06 mindearth/utils/__init__.py
--rw-r--r--  2.0 unx     1657 b- defN 23-Oct-16 07:06 mindearth/utils/logger.py
--rw-r--r--  2.0 unx     3564 b- defN 23-Oct-16 07:06 mindearth/utils/tools.py
--rw-r--r--  2.0 unx     3400 b- defN 23-Oct-16 07:06 mindearth/utils/visual.py
--rw-r--r--  2.0 unx     6686 b- defN 23-Oct-16 07:06 mindearth/utils/wind_quiver.py
--rw-r--r--  2.0 unx      659 b- defN 23-Oct-16 07:06 mindearth_gpu-0.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Oct-16 07:06 mindearth_gpu-0.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Oct-16 07:06 mindearth_gpu-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3022 b- defN 23-Oct-16 07:06 mindearth_gpu-0.1.0.dist-info/RECORD
-35 files, 220322 bytes uncompressed, 58592 bytes compressed:  73.4%
+Zip file size: 63718 bytes, number of entries: 35
+-rw-r--r--  2.0 unx     2769 b- defN 24-Mar-15 07:24 mindearth/__init__.py
+-rw-r--r--  2.0 unx     2195 b- defN 24-Mar-15 07:24 mindearth/setup.py
+-rw-r--r--  2.0 unx     1042 b- defN 24-Mar-15 07:24 mindearth/cell/__init__.py
+-rw-r--r--  2.0 unx     3495 b- defN 24-Mar-15 07:24 mindearth/cell/activation.py
+-rw-r--r--  2.0 unx    16851 b- defN 24-Mar-15 07:24 mindearth/cell/utils.py
+-rw-r--r--  2.0 unx      729 b- defN 24-Mar-15 07:24 mindearth/cell/demnet/__init__.py
+-rw-r--r--  2.0 unx     5263 b- defN 24-Mar-15 07:24 mindearth/cell/demnet/demnet.py
+-rw-r--r--  2.0 unx      784 b- defN 24-Mar-15 07:24 mindearth/cell/dgmr/__init__.py
+-rw-r--r--  2.0 unx    36610 b- defN 24-Mar-15 07:24 mindearth/cell/dgmr/dgmr.py
+-rw-r--r--  2.0 unx     6522 b- defN 24-Mar-15 07:24 mindearth/cell/dgmr/dgmrnet.py
+-rw-r--r--  2.0 unx      747 b- defN 24-Mar-15 07:24 mindearth/cell/graphcast/__init__.py
+-rw-r--r--  2.0 unx    14105 b- defN 24-Mar-15 07:24 mindearth/cell/graphcast/graphcast.py
+-rw-r--r--  2.0 unx    10037 b- defN 24-Mar-15 07:24 mindearth/cell/graphcast/graphcastnet.py
+-rw-r--r--  2.0 unx      770 b- defN 24-Mar-15 07:24 mindearth/cell/neural_operators/__init__.py
+-rw-r--r--  2.0 unx    11296 b- defN 24-Mar-15 07:24 mindearth/cell/neural_operators/afno2d.py
+-rw-r--r--  2.0 unx     5050 b- defN 24-Mar-15 07:24 mindearth/cell/neural_operators/afnonet.py
+-rw-r--r--  2.0 unx    15127 b- defN 24-Mar-15 07:24 mindearth/cell/neural_operators/dft.py
+-rw-r--r--  2.0 unx     8411 b- defN 24-Mar-15 07:24 mindearth/cell/neural_operators/vit_kno.py
+-rw-r--r--  2.0 unx      877 b- defN 24-Mar-15 07:24 mindearth/core/__init__.py
+-rw-r--r--  2.0 unx     3316 b- defN 24-Mar-15 07:24 mindearth/core/losses.py
+-rw-r--r--  2.0 unx     4683 b- defN 24-Mar-15 07:24 mindearth/core/lr_scheduler.py
+-rw-r--r--  2.0 unx      850 b- defN 24-Mar-15 07:24 mindearth/data/__init__.py
+-rw-r--r--  2.0 unx    23739 b- defN 24-Mar-15 07:24 mindearth/data/dataset.py
+-rw-r--r--  2.0 unx      788 b- defN 24-Mar-15 07:24 mindearth/module/__init__.py
+-rw-r--r--  2.0 unx    13715 b- defN 24-Mar-15 07:24 mindearth/module/forecast.py
+-rw-r--r--  2.0 unx    12123 b- defN 24-Mar-15 07:24 mindearth/module/pretrain.py
+-rw-r--r--  2.0 unx      980 b- defN 24-Mar-15 07:24 mindearth/utils/__init__.py
+-rw-r--r--  2.0 unx     1657 b- defN 24-Mar-15 07:24 mindearth/utils/logger.py
+-rw-r--r--  2.0 unx     3564 b- defN 24-Mar-15 07:24 mindearth/utils/tools.py
+-rw-r--r--  2.0 unx     3400 b- defN 24-Mar-15 07:24 mindearth/utils/visual.py
+-rw-r--r--  2.0 unx     6686 b- defN 24-Mar-15 07:24 mindearth/utils/wind_quiver.py
+-rw-r--r--  2.0 unx      659 b- defN 24-Mar-15 07:24 mindearth_gpu-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-15 07:24 mindearth_gpu-0.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-Mar-15 07:24 mindearth_gpu-0.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3022 b- defN 24-Mar-15 07:24 mindearth_gpu-0.2.0.dist-info/RECORD
+35 files, 221964 bytes uncompressed, 58866 bytes compressed:  73.5%
```

## zipnote {}

```diff
@@ -87,20 +87,20 @@
 
 Filename: mindearth/utils/visual.py
 Comment: 
 
 Filename: mindearth/utils/wind_quiver.py
 Comment: 
 
-Filename: mindearth_gpu-0.1.0.dist-info/METADATA
+Filename: mindearth_gpu-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: mindearth_gpu-0.1.0.dist-info/WHEEL
+Filename: mindearth_gpu-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: mindearth_gpu-0.1.0.dist-info/top_level.txt
+Filename: mindearth_gpu-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: mindearth_gpu-0.1.0.dist-info/RECORD
+Filename: mindearth_gpu-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mindearth/cell/demnet/demnet.py

```diff
@@ -38,15 +38,15 @@
 
     Inputs:
          - **input** (Tensor) - Tensor of shape :math:`(batch\_size, channels, height\_size, width\_size)`.
 
     Outputs:
         Tensor, the output of the DEMNet.
 
-         - **output** (Tensor) - Tensor of shape :math:`(batch\_size, channels, new_height\_size, new_width\_size)`.
+         - **output** (Tensor) - Tensor of shape :math:`(batch\_size, channels, new\_height\_size, new_width\_size)`.
 
     Supported Platforms:
         ``Ascend`` ``GPU``
 
     Examples:
         >>> import numpy as np
         >>> import mindspore as ms
@@ -88,20 +88,21 @@
         in_channels(int): The channels of input image.
         out_channels (int): The number of output channels.
         kernel_size (int): Kernel size.
         scale (int): The scale factor of new size of the tensor.
         num_blocks (int): The number of blocks in the DEMNet.
 
     Inputs:
-         - **x** (Tensor) - Tensor of shape :math:`(batch\_size, out_channels, height\_size, width\_size)`.
+         - **x** (Tensor) - Tensor of shape :math:`(batch\_size, out\_channels, height\_size, width\_size)`.
 
     Outputs:
         Tensor, the output of the DEMNet.
 
-         - **output** (Tensor) - Tensor of shape :math:`(batch\_size, out_channels, new_height\_size, new_width\_size)`.
+        - **output** (Tensor) - Tensor of shape
+          :math:`(batch\_size, out\_channels, new\_height\_size, new_width\_size)`.
 
     Supported Platforms:
         ``Ascend`` ``GPU``
 
     Examples:
         >>> import numpy as np
         >>> import mindspore as ms
@@ -112,24 +113,24 @@
         >>> net = DEMNet(in_channels=1, out_channels=256, kernel_size=3, scale=5, num_blocks=42)
         >>> out = net(Tensor(input_images, ms.float32))
         >>> print(out.shape)
         (64, 1, 160, 160)
     """
     def __init__(self,
                  in_channels=1,
-                 channels=256,
+                 out_channels=256,
                  kernel_size=3,
                  scale=5,
                  num_blocks=42):
         super(DEMNet, self).__init__()
         self.scale = scale
-        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size, pad_mode='same')
-        self.conv2 = nn.Conv2d(channels, channels, kernel_size, pad_mode='same')
-        self.conv_up = nn.Conv2d(channels, channels, kernel_size, pad_mode='same')
-        self.conv_out = nn.Conv2d(channels, in_channels, kernel_size, pad_mode='same')
+        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, pad_mode='same')
+        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, pad_mode='same')
+        self.conv_up = nn.Conv2d(out_channels, out_channels, kernel_size, pad_mode='same')
+        self.conv_out = nn.Conv2d(out_channels, in_channels, kernel_size, pad_mode='same')
         self.body = self.make_layer(ResBlock, num_blocks)
 
     def make_layer(self, block, layers):
         res_block = []
         for _ in range(layers):
             res_block.append(block())
         return nn.SequentialCell(*res_block)
```

## mindearth/cell/dgmr/dgmrnet.py

```diff
@@ -32,15 +32,15 @@
 
     Inputs:
          - **x** (Tensor) - Tensor of shape :math:`(2, frames\_size, channels, height\_size, width\_size)`.
 
     Outputs:
         Tensor, the output of the DgmrDiscriminator.
 
-         - **output** (Tensor) - Tensor of shape :math:`(2, 2, 1)`
+        - **output** (Tensor) - Tensor of shape :math:`(2, 2, 1)`.
 
     Supported Platforms:
         ``Ascend`` ``GPU``
 
     Examples:
         >>> import numpy as np
         >>> import mindspore as ms
@@ -96,21 +96,21 @@
         generation_steps (int): Number of generation steps to use in forward pass,
                                 in paper is 6 and the best is chosen for the loss,
                                 this results in huge amounts of GPU memory though,
                                 so less might work better for training.
 
     Inputs:
          - **x** (Tensor) - Tensor of shape :math:`(batch\_size, input\_frames,
-           out_channels, height\_size, width\_size)`.
+           out\_channels, height\_size, width\_size)`.
 
     Outputs:
-        Tensor，the output of Dgmr Generator。
+        Tensor, the output of Dgmr Generator.
 
-         - **output** (Tensor) - Tensor of shape :math:`(batch\_size, output\_frames,
-           out_channels, height\_size, width\_size)`.
+        - **output** (Tensor) - Tensor of shape :math:`(batch\_size, output\_frames,
+          out\_channels, height\_size, width\_size)`.
 
     Supported Platforms:
         ``Ascend`` ``GPU``
 
     Examples:
         >>> import numpy as np
         >>> import mindspore as ms
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## mindearth/cell/graphcast/graphcast.py

```diff
@@ -21,14 +21,34 @@
 import mindspore.nn as nn
 from mindspore import ops, set_seed, Tensor
 
 set_seed(0)
 np.random.seed(0)
 
 
+class GraphCastSiLU(nn.Cell):
+    r"""
+    A self-defined SwiGlu.
+
+    Inputs:
+        - **x** (Tensor) - Tensor.
+
+    Outputs:
+        Tensor. x = x * sigmod(x).
+    """
+    def __init__(self):
+        super().__init__()
+        self.sigmoid = nn.Sigmoid()
+        self.mul = ops.Mul()
+
+    def construct(self, x):
+        """GraphCastSiLU forward function."""
+        return self.mul(x, self.sigmoid(x))
+
+
 class MLPNet(nn.Cell):
     """
     The MLPNet Network. Applies a series of fully connected layers to the incoming data among which hidden layers have
         same number of dims.
 
     Args:
         in_channels (int): the number of input layer channel.
@@ -52,42 +72,41 @@
         >>> inputs = Tensor(np.array([[180, 234, 154], [244, 48, 247]], np.float32))
         >>> net = MLPNet(in_channels=3, out_channels=8, latent_dims=32)
         >>> output = net(inputs)
         >>> print(output.shape)
         (2, 8)
 
     """
-
     def __init__(self,
                  in_channels,
                  out_channels,
-                 latent_dims,
-                 has_layernorm=True):
+                 latent_dims):
         super(MLPNet, self).__init__()
-        cell_list = [nn.Dense(in_channels,
-                              latent_dims,
-                              has_bias=False,
-                              activation=None),
-                     nn.SiLU(),
-                     nn.Dense(latent_dims,
-                              out_channels,
-                              has_bias=False,
-                              activation=None),
-                     ]
-        if has_layernorm:
-            cell_list.append(nn.LayerNorm([out_channels]))
-        self.network = nn.SequentialCell(cell_list)
+        self.dense_in = nn.Dense(in_channels,
+                                 latent_dims,
+                                 has_bias=False,
+                                 activation=None)
+        self.silu = GraphCastSiLU()
+        self.dense_out = nn.Dense(latent_dims,
+                                  out_channels,
+                                  has_bias=False,
+                                  activation=None)
+        self.layer_norm = nn.LayerNorm([out_channels])
 
     def construct(self, x: Tensor):
         '''MLPNet forward function
 
         Args:
             x (Tensor): Input Tensor.
         '''
-        return self.network(x)
+        x = self.dense_in(x)
+        x = self.silu(x)
+        x = self.dense_out(x)
+        x = self.layer_norm(x)
+        return x
 
 
 class Embedder(nn.Cell):
     """
     Embed raw features of the grid nodes, mesh nodes, multi-mesh edges, grid2mesh edges and mesh2grid edges.
     """
     def __init__(self,
@@ -328,15 +347,14 @@
                               edge_in_channels,
                               edge_out_channels,
                               latent_dims,
                               src_idx,
                               dst_idx)
         self.node_fn = MLPNet(in_channels=node_in_channels,
                               out_channels=node_final_dims,
-                              latent_dims=latent_dims,
-                              has_layernorm=False)
+                              latent_dims=latent_dims)
 
     def construct(self, m2g_edge_feats, mesh_node_feats, grid_node_feats):
         '''Decoder forward function'''
         m2g_edge_feats, mesh_node_feats, grid_node_feats = self.m2g_gnn(m2g_edge_feats, mesh_node_feats,
                                                                         grid_node_feats)
         return self.node_fn(grid_node_feats)
```

## mindearth/cell/neural_operators/afno2d.py

```diff
@@ -204,14 +204,16 @@
 
     Examples:
 
 
     """
     def __init__(self,
                  grid_size,
+                 h_size,
+                 w_size,
                  in_channels,
                  patch_size,
                  depth,
                  embed_dims,
                  mlp_ratio=4,
                  dropout_rate=1.0,
                  compute_dtype=mstype.float16):
@@ -219,16 +221,16 @@
         self.patch_embed = PatchEmbed(in_channels, embed_dims, patch_size, compute_dtype=compute_dtype)
         self.pos_embed = Parameter(
             initializer(TruncatedNormal(sigma=0.02), [1, grid_size[0] * grid_size[1], embed_dims], dtype=compute_dtype),
             requires_grad=True)
         self.layer = nn.CellList([])
         self.encoder_norm = nn.LayerNorm([embed_dims], epsilon=1e-6).to_float(compute_dtype)
         for _ in range(depth):
-            self.layer.append(AFNOBlock(embed_dims, mlp_ratio, dropout_rate, patch_size=patch_size,
-                                        compute_dtype=compute_dtype))
+            self.layer.append(AFNOBlock(embed_dims, mlp_ratio, dropout_rate, h_size=h_size, w_size=w_size,
+                                        patch_size=patch_size, compute_dtype=compute_dtype))
 
         self.pos_drop = nn.Dropout(keep_prob=dropout_rate)
         self.h = grid_size[0]
         self.w = grid_size[1]
         self.embed_dims = embed_dims
 
     def construct(self, x):
@@ -305,15 +307,15 @@
 
         self.relu = ops.ReLU()
 
         self.sparsity_threshold = 0.01
         self.hard_thresholding_fraction = 1.0
 
         self.high_freq = high_freq
-        self.w = nn.Conv2d(embed_dims, embed_dims, 1)  # High Frequency
+        self.w = nn.Dense(embed_dims, embed_dims, has_bias=False)  # High Frequency
 
         self.cast = ops.Cast()
 
     @staticmethod
     def mul2d(inputs, weights):
         weight = weights.expand_dims(0)
         data = inputs.expand_dims(5)
@@ -322,15 +324,15 @@
 
     def construct(self, x: Tensor):
         '''construct'''
         if self.high_freq:
             b, n, c = x.shape
             h, w = self.h_size, self.w_size
             x = x.reshape(b, h, w, c)
-            bias = self.w(x.transpose((0, 3, 1, 2))).transpose((0, 2, 3, 1))
+            bias = self.w(x)
             bias = bias.reshape(b, h * w, c)
         else:
             bias = x
             b, n, c = x.shape
             h, w = self.h_size, self.w_size
             x = x.reshape(b, h, w, c)
```

## mindearth/cell/neural_operators/afnonet.py

```diff
@@ -88,14 +88,16 @@
 
         self.encoder_depths = encoder_depths
         self.encoder_embed_dim = encoder_embed_dim
 
         self.transpose = ops.Transpose()
 
         self.forward_features = ForwardFeatures(grid_size=grid_size,
+                                                h_size=image_size[0],
+                                                w_size=image_size[1],
                                                 in_channels=in_channels,
                                                 patch_size=patch_size,
                                                 depth=encoder_depths,
                                                 embed_dims=encoder_embed_dim,
                                                 mlp_ratio=mlp_ratio,
                                                 dropout_rate=dropout_rate,
                                                 compute_dtype=compute_dtype)
```

## mindearth/cell/neural_operators/vit_kno.py

```diff
@@ -27,29 +27,29 @@
     The ViT-KNO is a deep learning model that based on the Koopman theory and the Vision Transformer structure.
     This model is based on the Koopman neural operator which mapped the original nonlinear dynamical system to
     linear dynamical system and conducted the time deduction in linear domain.
     The details can be found in `KoopmanLab: machine learning for
     solving complex physics equations <https://arxiv.org/pdf/2301.01104.pdf>`_.
 
     Args:
-        image_size (tuple[int], optional): The size of the input image. Default: (128, 256).
-        patch_size (int, optional): The patch size of image. Default: 8.
-        in_channels (int, optional): The number of channels in the input space. Default: 1.
-        out_channels (int, optional): The number of channels in the output space. Default: 1.
-        encoder_depths (int, optional): The encoder depth of encoder layer. Default: 12.
+        image_size (tuple[int], optional): The size of the input image. Default: ``(128, 256)``.
+        patch_size (int, optional): The patch size of image. Default: ``8``.
+        in_channels (int, optional): The number of channels in the input space. Default: ``1``.
+        out_channels (int, optional): The number of channels in the output space. Default: ``1``.
+        encoder_depths (int, optional): The encoder depth of encoder layer. Default: ``12``.
         encoder_embed_dims (int, optional): The encoder embedding dimension of encoder layer. Default: 768.
-        mlp_ratio (int, optional): The rate of mlp layer. Default: 4.
-        dropout_rate (float, optional): The rate of dropout layer. Default: 1.0.
-        drop_path_rate (float, optional): The rate of drop path layer. Default: 0.0.
-        num_blocks: (int, optional): The number of blocks. Default: 16.
-        settings: (str, optional): The construction of first decoder layer. Default: 'MLP'.
-        high_freq (bool, optional): if high-frequency information complement is applied. Default: True.
-        encoder_network (bool, optional): if encoder_network is applied. Default: False
+        mlp_ratio (int, optional): The rate of mlp layer. Default: ``4``.
+        dropout_rate (float, optional): The rate of dropout layer. Default: ``1.0``.
+        drop_path_rate (float, optional): The rate of drop path layer. Default: ``0.0``.
+        num_blocks (int, optional): The number of blocks. Default: ``16``.
+        settings (str, optional): The construction of first decoder layer. Default: ``'MLP'``.
+        high_freq (bool, optional): if high-frequency information complement is applied. Default: ``True``.
+        encoder_network (bool, optional): if encoder_network is applied. Default: ``False``.
         compute_dtype (dtype, optional): The data type for encoder, decoding_embedding, decoder and dense layer.
-                Default: mindspore.float32.
+            Default: ``mindspore.float32``.
 
     Inputs:
         - **x** (Tensor) - Tensor of shape :math:`(batch\_size, feature\_size, image\_height, image\_width)`.
 
     Outputs:
         - **output** (Tensor) - Tensor of shape :math:`(batch\_size, patch\_size, embed\_dim)`.
           where :math:`patch\_size = (image\_height * image\_width) / (patch\_size * patch\_size)`.
```

## mindearth/core/losses.py

```diff
@@ -30,15 +30,15 @@
     the loss of :math:`x` and :math:`y` is given as:
 
     .. math::
         loss = \sqrt{\frac{\frac{1}{N}\sum_{i=1}^{N}{(x_i-y_i)^2}}{sum_{i=1}^{N}{(y_i)^2}}}
 
     Args:
         reduction (str): Type of reduction to be applied to loss. The optional values are ``"mean"``,
-            ``"sum"``, and ``"none"``. Default: ``"sum"``.
+            ``"sum"``, and ``"none"``. Default: ``"mean"``.
 
     Inputs:
         - **prediction** (Tensor) - Tensor of shape :math:`(N, *)` where :math:`*` means, any number of
           additional dimensions.
         - **labels** (Tensor) - Tensor of shape :math:`(N, *)`, same shape as the `prediction` in common cases.
           However, it supports the shape of `prediction` is different from the shape of `label`
           and they should be broadcasted to each other.
```

## mindearth/core/lr_scheduler.py

```diff
@@ -63,17 +63,17 @@
     .. math::
         warmup\_learning\_rate[i] = (lr\_init - warmup\_lr\_init) * i / warmup\_steps + warmup\_lr\_init
 
     Args:
         lr_init (float): init learning rate, positive float value.
         steps_per_epoch (int): number of steps to each epoch, positive int value.
         last_epoch (int): total epoch of training, positive int value.
-        warmup_epochs (int): total epoch of warming up, default:0.
-        warmup_lr_init (float): warmup init learning rate, default:0.0.
-        eta_min (float): minimum learning rate, default: 1e-6.
+        warmup_epochs (int): total epoch of warming up, default: ``0``.
+        warmup_lr_init (float): warmup init learning rate, default: ``0.0``.
+        eta_min (float): minimum learning rate, default: ``1e-6``.
 
     Returns:
         Numpy.array, learning rate array.
 
     Raises:
         TypeError: If `lr_init` or `warmup_lr_init` or `eta_min` is not a float.
         TypeError: If `steps_per_epoch` or `warmup_epochs` or `last_epoch` is not an int.
```

## mindearth/data/dataset.py

```diff
@@ -202,14 +202,17 @@
         self.train_interval = data_params.get('train_interval') * self.data_frequency
         self.pred_lead_time = data_params.get('pred_lead_time')
         self.train_period = data_params.get('train_period')
         self.valid_period = data_params.get('valid_period')
         self.test_period = data_params.get('test_period')
         self.feature_dims = data_params.get('feature_dims')
         self.output_dims = data_params.get('feature_dims')
+        self.surface_feature_size = data_params.get('surface_feature_size', 4)
+        self.level_feature_size = (self.feature_dims -
+                                   self.surface_feature_size) // data_params.get('pressure_level_num', 13)
         self.patch = data_params.get('patch')
         if self.patch:
             self.patch_size = data_params.get('patch_size')
 
         if run_mode == 'train':
             self.t_out = data_params.get('t_out_train')
             self.path = self.train_dir
@@ -260,31 +263,36 @@
         label_lst = []
         label_surface_lst = []
         idx = idx * self.interval
 
         for t in range(self.t_in):
             cur_input_data_idx = idx + t * self.pred_lead_time
             input_date, year_name = get_datapath_from_date(self.start_date, cur_input_data_idx.item())
-            x = np.load(os.path.join(self.path, input_date)).astype(np.float32)
-            x_surface = np.load(os.path.join(self.surface_path, input_date)).astype(np.float32)
+            x = np.load(os.path.join(self.path, input_date))[:, :, :self.h_size].astype(np.float32)
+            x_surface = np.load(os.path.join(self.surface_path,
+                                             input_date))[:, :self.h_size].astype(np.float32)
             x_static = np.load(os.path.join(self.static_path, year_name)).astype(np.float32)
-            x_surface_static = np.load(os.path.join(self.static_surface_path, year_name)).astype(np.float32)
+            x_surface_static = np.load(os.path.join(self.static_surface_path,
+                                                    year_name)).astype(np.float32)
             x = self._get_origin_data(x, x_static)
             x_surface = self._get_origin_data(x_surface, x_surface_static)
             x, x_surface = self._normalize(x, x_surface)
             inputs_lst.append(x)
             inputs_surface_lst.append(x_surface)
 
         for t in range(self.t_out):
             cur_label_data_idx = idx + (self.t_in + t) * self.pred_lead_time
             label_date, year_name = get_datapath_from_date(self.start_date, cur_label_data_idx.item())
-            label = np.load(os.path.join(self.path, label_date)).astype(np.float32)
-            label_surface = np.load(os.path.join(self.surface_path, label_date)).astype(np.float32)
-            label_static = np.load(os.path.join(self.static_path, year_name)).astype(np.float32)
-            label_surface_static = np.load(os.path.join(self.static_surface_path, year_name)).astype(np.float32)
+            label = np.load(os.path.join(self.path, label_date))[:, :, :self.h_size].astype(np.float32)
+            label_surface = np.load(os.path.join(self.surface_path,
+                                                 label_date))[:, :self.h_size].astype(np.float32)
+            label_static = np.load(os.path.join(self.static_path,
+                                                year_name)).astype(np.float32)
+            label_surface_static = np.load(os.path.join(self.static_surface_path,
+                                                        year_name)).astype(np.float32)
             label = self._get_origin_data(label, label_static)
             label_surface = self._get_origin_data(label_surface, label_surface_static)
             label, label_surface = self._normalize(label, label_surface)
 
             label_lst.append(label)
             label_surface_lst.append(label_surface)
 
@@ -474,16 +482,16 @@
         if run_mode == 'train':
             path = os.path.join(self.train_dir, "train.h5")
         elif run_mode == 'valid':
             path = os.path.join(self.valid_dir, "valid.h5")
         else:
             path = os.path.join(self.test_dir, "test.h5")
         data = h5py.File(path, 'r')
-        data_lr = data.get('32_32').astype(np.float32)
-        data_hr = data.get('160_160').astype(np.float32)
+        data_lr = data.get('32_32')
+        data_hr = data.get('160_160')
 
         self.__data_lr = data_lr
         self.__data_hr = data_hr
 
     def __getitem__(self, index):
         return (self.__data_lr[index, :, :, :], self.__data_hr[index, :, :, :])
```

## mindearth/module/forecast.py

```diff
@@ -143,18 +143,24 @@
                  model,
                  config,
                  logger
                  ):
         self.model = amp.auto_mixed_precision(model, config['train']['amp_level'])
         self.logger = logger
         self.config = config
+        self.adjust_size = False
+        self.h_size, self.w_size = SIZE_DICT[config['data'].get('grid_resolution', 1.4)]
+        if self.config['data'].get('patch', False):
+            patch_size = [config['data'].get('patch_size', 8)]
+            self.h_size = self.h_size - self.h_size % patch_size[0]
+            self.adjust_size = True
+
         self.total_std = self._get_total_sample_description(config, "std")
         self.total_mean = self._get_total_sample_description(config, "mean")
-        self.climate_mean = self._get_history_climate_mean(config)
-        self.h_size, self.w_size = SIZE_DICT[config['data'].get('grid_resolution', 1.4)]
+        self.climate_mean = self._get_history_climate_mean(config, self.w_size, self.adjust_size)
         self.t_out_test = config['data'].get("t_out_test", 20)
         self.pred_lead_time = config['data']['pred_lead_time']
 
     @staticmethod
     def _get_total_sample_description(config, info_mode):
         """get total sample std or mean description."""
         root_dir = config.get('data').get('root_dir')
@@ -166,19 +172,22 @@
         sample_info_surface = np.load(os.path.join(root_dir, "statistic",
                                                    info_mode + "_s.npy"))
         total_sample_info = np.append(sample_info_pressure_levels, sample_info_surface)
 
         return total_sample_info
 
     @staticmethod
-    def _get_history_climate_mean(config):
+    def _get_history_climate_mean(config, w_size, adjust_size=False):
         """get history climate mean."""
         data_params = config.get('data')
         climate_mean = np.load(os.path.join(data_params.get("root_dir"), "statistic",
                                             f"climate_{data_params.get('grid_resolution')}.npy"))
+        feature_dims = climate_mean.shape[-1]
+        if adjust_size:
+            climate_mean = climate_mean.reshape(-1, w_size, feature_dims)[:-1].reshape(-1, feature_dims)
 
         return climate_mean
 
     def _get_absolute_idx(self, idx):
         return idx[1] * self.config['data']['pressure_level_num'] + idx[0]
 
     def _print_key_metrics(self, rmse, acc):
```

## Comparing `mindearth_gpu-0.1.0.dist-info/METADATA` & `mindearth_gpu-0.2.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mindearth-gpu
-Version: 0.1.0
+Version: 0.2.0
 Summary: An AI framework for Earth simulation
 Home-page: https://www.mindspore.cn/
 Download-URL: https://gitee.com/mindspore/mindscience/tags
 Author: The MindSpore Authors
 Author-email: contact@mindspore.cn
 License: Apache 2.0
 Project-URL: Sources, https://gitee.com/mindspore/mindscience
```

## Comparing `mindearth_gpu-0.1.0.dist-info/RECORD` & `mindearth_gpu-0.2.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 mindearth/__init__.py,sha256=5nHMFaIWLSnKzqXmchPtQcNfb0yXONMRVqNTvI-tbRE,2769
 mindearth/setup.py,sha256=xZnSmFFHOWkkFUWAzEvGpFZ9EYaVcxILpjhZzl4FEyE,2195
 mindearth/cell/__init__.py,sha256=4WKxBu5Qvkw5N2oeV8C2kUSTfr05EpBlovClV1vw3bs,1042
 mindearth/cell/activation.py,sha256=wluvra6Q-_zsFKMqiurrhlKZ7ked4LMZSSKOnoL1sY0,3495
 mindearth/cell/utils.py,sha256=cp4nYnWUlCbQWpaxp0vR68MIJEZwdb617c19ZaOkVBo,16851
 mindearth/cell/demnet/__init__.py,sha256=_K4pPLsK9xyKYJ125pXBpH1KSJ9Wrkags2v8pRzisFk,729
-mindearth/cell/demnet/demnet.py,sha256=3AvGKcjVdmV202HJuz_gIKVfKcY7rPuzVOQeGsCO-b0,5222
+mindearth/cell/demnet/demnet.py,sha256=HABKX7QGQ9qU3ZNvAS_rzBg04B2dD8JDvAygEl5fN1k,5263
 mindearth/cell/dgmr/__init__.py,sha256=9M7FNhEfDrxni4RhVKMbRMLKbmKxmA5wXVeBodSwYzk,784
 mindearth/cell/dgmr/dgmr.py,sha256=ZFE84m9g8hvitLR0t8yo08ubE4yzZbaEHBw_u4jkdJw,36610
-mindearth/cell/dgmr/dgmrnet.py,sha256=61HXirfsxvz8JWPIyqndp4Y4mW1YiPYUvU75UFWYx4I,6525
+mindearth/cell/dgmr/dgmrnet.py,sha256=Xy7KhefWOD5MLiPuNOufNAbAPZLg1c2B9lFMhs-3Z7k,6522
 mindearth/cell/graphcast/__init__.py,sha256=34QnLn9DD9zqsgxN0Wik9J1WB_RtmRdA8ere09bcsaY,747
-mindearth/cell/graphcast/graphcast.py,sha256=rXEblyHjVVS-oHo_wSjSlT-Q21Tb74zfSURiFnwJMIo,13760
+mindearth/cell/graphcast/graphcast.py,sha256=vFCbkgwj9hTUrkvQABvZqHE_lPKweYeowciebfMI1-M,14105
 mindearth/cell/graphcast/graphcastnet.py,sha256=j_n403r6fUMQmQa-rKUynaTxA6KMBfNWRnIh9YWPXHE,10037
 mindearth/cell/neural_operators/__init__.py,sha256=1X8ceI1lIjduv7IQ2snvzPpOYLqDcuDF3ZMaXYrY130,770
-mindearth/cell/neural_operators/afno2d.py,sha256=E171-1L5A4kWuv1QdDVYR9RB-J62mV5fOxpPm_G2jzM,11252
-mindearth/cell/neural_operators/afnonet.py,sha256=zGDEnlLNunw_pSvy147boabGLiVrTEgmePX7wkzVvmE,4910
+mindearth/cell/neural_operators/afno2d.py,sha256=hnHtmFZqXVCcE37QIT6HbJe3m0wOZgPSp5XLMkO_q7E,11296
+mindearth/cell/neural_operators/afnonet.py,sha256=xQhzGH-1htDeJNWN--LlfDDnSFBxUOA89N27S4OjRE0,5050
 mindearth/cell/neural_operators/dft.py,sha256=DFGcBrpVybAGP9RN0OFJFCUnQV44qcgw3Lf8BZy6Hto,15127
-mindearth/cell/neural_operators/vit_kno.py,sha256=zPQjOQTwR8v0GFD4QCHs9whzryT1iuFYOCXTM1xHbQE,8364
+mindearth/cell/neural_operators/vit_kno.py,sha256=EGYxKL7QkeIWB8wWRsHM3AXAD_37dPg6f0PkFumVxGQ,8411
 mindearth/core/__init__.py,sha256=YT4ER-I7uLCKsMn6_pYFxdFKY9TOGzCRlsqCuXuX5aQ,877
-mindearth/core/losses.py,sha256=FxFs0qHkHsaUlwCDeJFBSixLifAxJEOHmcWTYSuvBos,3315
-mindearth/core/lr_scheduler.py,sha256=C7kLwwzfyRvM4NR2_OL6C5qF8u2fWthSenSlLJQHl90,4669
+mindearth/core/losses.py,sha256=MGLxhTyEr40uezwZzaHEt2x58EkR0fZkj-9Lr3tHygQ,3316
+mindearth/core/lr_scheduler.py,sha256=FNTUZMUytcz2m5Me4QGROT2PvkMKd3Gy_yIucuJxpgg,4683
 mindearth/data/__init__.py,sha256=exIOrfmnKFrL5OT2b5wMS2ByvyxRRd6czQp0Lv-tNBQ,850
-mindearth/data/dataset.py,sha256=iN5Jd2y_92IXBFIo8Xi4lkRR_bhatvLl0MnJHBEAVQQ,23212
+mindearth/data/dataset.py,sha256=_g_09Kd47HcXIyTj15ohBt5QRnpqZzKSMaUwZw7Np8A,23739
 mindearth/module/__init__.py,sha256=BdFunTUamMhJNir32rU5Pb7jTJSPPOc3tKSvKD43av4,788
-mindearth/module/forecast.py,sha256=qIWKqp_NeihDVG0ZuKGb6ttq1y_f5xbfc57e5x-8qoo,13229
+mindearth/module/forecast.py,sha256=FDCsp-FmO1uQNqHonhExSQKbjY3Kmes4TExp9BE9gHs,13715
 mindearth/module/pretrain.py,sha256=knbTdK_i4ZX4XNuw7n96zW5pb1GuPrBg2BeNb9Je79E,12123
 mindearth/utils/__init__.py,sha256=bp233uck2oOiv8JXpg_jGdjvs-AIQt7aqfwk6eeQ2JQ,980
 mindearth/utils/logger.py,sha256=n3XENN_nB1DA2xneRoSHQNhpu1dw5GIB_hhjSPzTx0g,1657
 mindearth/utils/tools.py,sha256=-Qlgf9mPDzUnemt-SpUwB8oKWee59mhKqxGD_ef6vA4,3564
 mindearth/utils/visual.py,sha256=4Ibul_EazGQK8uRlp99cIN6RKxQJcSB8Ao_wI_zDQbU,3400
 mindearth/utils/wind_quiver.py,sha256=vN7cGjIEjxFp97LGKLBx5thfNo7DTouZbN00tI9_MFs,6686
-mindearth_gpu-0.1.0.dist-info/METADATA,sha256=CmfuzIR3b6D5psgYWJ-vmtZVOhdL4UP6HjI4n4vE6lw,659
-mindearth_gpu-0.1.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mindearth_gpu-0.1.0.dist-info/top_level.txt,sha256=LDfasJzd104Rb2y3E7hMoFAjW3FXvwrVn6Uo4CoG54I,10
-mindearth_gpu-0.1.0.dist-info/RECORD,,
+mindearth_gpu-0.2.0.dist-info/METADATA,sha256=I5-YFNcxTntQkeyN9tE1fTUIdkm4YZbQNQJ4P3wPpmg,659
+mindearth_gpu-0.2.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mindearth_gpu-0.2.0.dist-info/top_level.txt,sha256=LDfasJzd104Rb2y3E7hMoFAjW3FXvwrVn6Uo4CoG54I,10
+mindearth_gpu-0.2.0.dist-info/RECORD,,
```

