# Comparing `tmp/mindformers-1.0.0-py3-none-any.whl.zip` & `tmp/mindformers-1.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,412 +1,414 @@
-Zip file size: 1145049 bytes, number of entries: 410
--rw-r--r--  2.0 unx    14400 b- defN 24-Jan-22 12:06 configs/README.md
--rw-r--r--  2.0 unx     4241 b- defN 24-Jan-22 12:06 configs/bert/run_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4257 b- defN 24-Jan-22 12:06 configs/bert/run_bert_tiny_uncased.yaml
--rw-r--r--  2.0 unx     6074 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
--rw-r--r--  2.0 unx     6139 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
--rw-r--r--  2.0 unx     4951 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     6214 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
--rw-r--r--  2.0 unx     4705 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
--rw-r--r--  2.0 unx     6272 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
--rw-r--r--  2.0 unx     6272 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
--rw-r--r--  2.0 unx     4761 b- defN 24-Jan-22 12:06 configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
--rw-r--r--  2.0 unx     4055 b- defN 24-Jan-22 12:06 configs/bloom/run_bloom_560m.yaml
--rw-r--r--  2.0 unx     4054 b- defN 24-Jan-22 12:06 configs/bloom/run_bloom_7.1b.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-Jan-22 12:06 configs/bloom/run_bloom_7.1b_910b.yaml
--rw-r--r--  2.0 unx     4156 b- defN 24-Jan-22 12:06 configs/bloom/run_bloom_7.1b_910b_fa.yaml
--rw-r--r--  2.0 unx     4137 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4148 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4295 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4139 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4286 b- defN 24-Jan-22 12:06 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     6097 b- defN 24-Jan-22 12:06 configs/codegeex2/run_codegeex2_6b.yaml
--rw-r--r--  2.0 unx     5815 b- defN 24-Jan-22 12:06 configs/codegeex2/run_codegeex2_6b_eval.yaml
--rw-r--r--  2.0 unx     5866 b- defN 24-Jan-22 12:06 configs/codegeex2/run_codegeex2_6b_finetune.yaml
--rw-r--r--  2.0 unx     5870 b- defN 24-Jan-22 12:06 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
--rw-r--r--  2.0 unx     3827 b- defN 24-Jan-22 12:06 configs/codellama/predict_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx     5330 b- defN 24-Jan-22 12:06 configs/codellama/run_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx     2637 b- defN 24-Jan-22 12:06 configs/general/run_general_task.yaml
--rw-r--r--  2.0 unx     5756 b- defN 24-Jan-22 12:06 configs/glm/run_glm_6b_finetune.yaml
--rw-r--r--  2.0 unx     5677 b- defN 24-Jan-22 12:06 configs/glm/run_glm_6b_infer.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Jan-22 12:06 configs/glm/run_glm_6b_lora.yaml
--rw-r--r--  2.0 unx     5861 b- defN 24-Jan-22 12:06 configs/glm/run_glm_6b_lora_infer.yaml
--rw-r--r--  2.0 unx     1564 b- defN 24-Jan-22 12:06 configs/glm2/export_glm2_6b.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b.yaml
--rw-r--r--  2.0 unx     5965 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_finetune.yaml
--rw-r--r--  2.0 unx     5972 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_finetune_2k.yaml
--rw-r--r--  2.0 unx     5971 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_finetune_2k_910b.yaml
--rw-r--r--  2.0 unx     5919 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_finetune_910b.yaml
--rw-r--r--  2.0 unx     5865 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_finetune_eval.yaml
--rw-r--r--  2.0 unx     6246 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_lora.yaml
--rw-r--r--  2.0 unx     6254 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_lora_2k.yaml
--rw-r--r--  2.0 unx     6252 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_lora_2k_910b.yaml
--rw-r--r--  2.0 unx     6240 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_lora_910b.yaml
--rw-r--r--  2.0 unx     6146 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_lora_eval.yaml
--rw-r--r--  2.0 unx     6200 b- defN 24-Jan-22 12:06 configs/glm2/run_glm2_6b_ptuning2.yaml
--rw-r--r--  2.0 unx     1640 b- defN 24-Jan-22 12:06 configs/glm3/export_glm3_6b.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-Jan-22 12:06 configs/glm3/run_glm3_6b.yaml
--rw-r--r--  2.0 unx     5956 b- defN 24-Jan-22 12:06 configs/glm3/run_glm3_6b_finetune_2k_910b.yaml
--rw-r--r--  2.0 unx     4301 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2.yaml
--rw-r--r--  2.0 unx     4678 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_13b.yaml
--rw-r--r--  2.0 unx     4822 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_13b_910b.yaml
--rw-r--r--  2.0 unx     4628 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_52b.yaml
--rw-r--r--  2.0 unx     4331 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_lora.yaml
--rw-r--r--  2.0 unx     4261 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_txtcls.yaml
--rw-r--r--  2.0 unx     4631 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_xl.yaml
--rw-r--r--  2.0 unx     4824 b- defN 24-Jan-22 12:06 configs/gpt2/run_gpt2_xl_lora.yaml
--rwxr-xr-x  2.0 unx     5173 b- defN 24-Jan-22 12:06 configs/llama/run_llama_13b.yaml
--rw-r--r--  2.0 unx     5176 b- defN 24-Jan-22 12:06 configs/llama/run_llama_13b_910b.yaml
--rwxr-xr-x  2.0 unx     5169 b- defN 24-Jan-22 12:06 configs/llama/run_llama_7b.yaml
--rw-r--r--  2.0 unx     5167 b- defN 24-Jan-22 12:06 configs/llama/run_llama_7b_910b.yaml
--rw-r--r--  2.0 unx     5625 b- defN 24-Jan-22 12:06 configs/llama/run_llama_7b_lora.yaml
--rw-r--r--  2.0 unx     2708 b- defN 24-Jan-22 12:06 configs/llama2/export_llama2_13b.yaml
--rw-r--r--  2.0 unx     2539 b- defN 24-Jan-22 12:06 configs/llama2/export_llama2_7b.yaml
--rw-r--r--  2.0 unx     3875 b- defN 24-Jan-22 12:06 configs/llama2/predict_llama2_70b_910b.yaml
--rw-r--r--  2.0 unx     5424 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_13b.yaml
--rw-r--r--  2.0 unx     5087 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_13b_910b.yaml
--rw-r--r--  2.0 unx     5655 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5137 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_13b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5420 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_13b_lora_910b.yaml
--rw-r--r--  2.0 unx     5473 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_70b.yaml
--rw-r--r--  2.0 unx     5332 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_70b_910b.yaml
--rw-r--r--  2.0 unx     5897 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5378 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_70b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5414 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_7b.yaml
--rw-r--r--  2.0 unx     5100 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_7b_910b.yaml
--rw-r--r--  2.0 unx     5632 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5113 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_7b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5452 b- defN 24-Jan-22 12:06 configs/llama2/run_llama2_7b_lora_910b.yaml
--rw-r--r--  2.0 unx     4785 b- defN 24-Jan-22 12:06 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
--rw-r--r--  2.0 unx     4942 b- defN 24-Jan-22 12:06 configs/pangualpha/run_pangualpha_13b.yaml
--rw-r--r--  2.0 unx     4791 b- defN 24-Jan-22 12:06 configs/pangualpha/run_pangualpha_2_6b.yaml
--rw-r--r--  2.0 unx     4229 b- defN 24-Jan-22 12:06 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
--rw-r--r--  2.0 unx     4495 b- defN 24-Jan-22 12:06 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
--rw-r--r--  2.0 unx     5531 b- defN 24-Jan-22 12:06 configs/qa/run_qa_bert_base_uncased.yaml
--rwxr-xr-x  2.0 unx     6818 b- defN 24-Jan-22 12:06 configs/sam/run_sam_vit-b.yaml
--rwxr-xr-x  2.0 unx     6821 b- defN 24-Jan-22 12:06 configs/sam/run_sam_vit-h.yaml
--rw-r--r--  2.0 unx     6821 b- defN 24-Jan-22 12:06 configs/sam/run_sam_vit-l.yaml
--rw-r--r--  2.0 unx     6149 b- defN 24-Jan-22 12:06 configs/swin/run_swin_base_p4w7_224_100ep.yaml
--rw-r--r--  2.0 unx     4494 b- defN 24-Jan-22 12:06 configs/t5/run_t5_small_on_wmt16.yaml
--rw-r--r--  2.0 unx     4455 b- defN 24-Jan-22 12:06 configs/t5/run_t5_tiny_on_wmt16.yaml
--rw-r--r--  2.0 unx     5772 b- defN 24-Jan-22 12:06 configs/tokcls/run_tokcls_bert_base_chinese.yaml
--rw-r--r--  2.0 unx     5788 b- defN 24-Jan-22 12:06 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
--rw-r--r--  2.0 unx     4428 b- defN 24-Jan-22 12:06 configs/txtcls/run_txtcls_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4438 b- defN 24-Jan-22 12:06 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
--rw-r--r--  2.0 unx     6020 b- defN 24-Jan-22 12:06 configs/vit/run_vit_base_p16_224_100ep.yaml
--r--------  2.0 unx      275 b- defN 24-Jan-22 12:10 mindformers/.commit_id
--r--------  2.0 unx     1402 b- defN 24-Jan-22 12:06 mindformers/__init__.py
--r--------  2.0 unx    39712 b- defN 24-Jan-22 12:06 mindformers/auto_class.py
--r--------  2.0 unx    67667 b- defN 24-Jan-22 12:06 mindformers/mindformer_book.py
--r--------  2.0 unx     9793 b- defN 24-Jan-22 12:06 mindformers/version_control.py
--r--------  2.0 unx     1297 b- defN 24-Jan-22 12:06 mindformers/core/__init__.py
--r--------  2.0 unx     3956 b- defN 24-Jan-22 12:06 mindformers/core/clip_grad.py
--r--------  2.0 unx     2995 b- defN 24-Jan-22 12:06 mindformers/core/parallel_config.py
--r--------  2.0 unx      809 b- defN 24-Jan-22 12:06 mindformers/core/callback/__init__.py
--r--------  2.0 unx     3309 b- defN 24-Jan-22 12:06 mindformers/core/callback/build_callback.py
--r--------  2.0 unx    37103 b- defN 24-Jan-22 12:06 mindformers/core/callback/callback.py
--r--------  2.0 unx      833 b- defN 24-Jan-22 12:06 mindformers/core/context/__init__.py
--r--------  2.0 unx     8659 b- defN 24-Jan-22 12:06 mindformers/core/context/build_context.py
--r--------  2.0 unx      789 b- defN 24-Jan-22 12:06 mindformers/core/loss/__init__.py
--r--------  2.0 unx     2866 b- defN 24-Jan-22 12:06 mindformers/core/loss/build_loss.py
--r--------  2.0 unx    18164 b- defN 24-Jan-22 12:06 mindformers/core/loss/loss.py
--r--------  2.0 unx      802 b- defN 24-Jan-22 12:06 mindformers/core/lr/__init__.py
--r--------  2.0 unx     3844 b- defN 24-Jan-22 12:06 mindformers/core/lr/build_lr.py
--r--------  2.0 unx    14711 b- defN 24-Jan-22 12:06 mindformers/core/lr/lr_schedule.py
--r--------  2.0 unx      800 b- defN 24-Jan-22 12:06 mindformers/core/metric/__init__.py
--r--------  2.0 unx     2683 b- defN 24-Jan-22 12:06 mindformers/core/metric/build_metric.py
--r--------  2.0 unx    35009 b- defN 24-Jan-22 12:06 mindformers/core/metric/metric.py
--r--------  2.0 unx     1768 b- defN 24-Jan-22 12:06 mindformers/core/metric/utils.py
--r--------  2.0 unx      847 b- defN 24-Jan-22 12:06 mindformers/core/optim/__init__.py
--r--------  2.0 unx     4623 b- defN 24-Jan-22 12:06 mindformers/core/optim/build_optim.py
--r--------  2.0 unx    21063 b- defN 24-Jan-22 12:06 mindformers/core/optim/came.py
--r--------  2.0 unx    31399 b- defN 24-Jan-22 12:06 mindformers/core/optim/optim.py
--r--------  2.0 unx     2427 b- defN 24-Jan-22 12:06 mindformers/dataset/__init__.py
--r--------  2.0 unx     3379 b- defN 24-Jan-22 12:06 mindformers/dataset/base_dataset.py
--r--------  2.0 unx     2560 b- defN 24-Jan-22 12:06 mindformers/dataset/build_dataset.py
--r--------  2.0 unx    12908 b- defN 24-Jan-22 12:06 mindformers/dataset/causal_language_model_dataset.py
--r--------  2.0 unx     9758 b- defN 24-Jan-22 12:06 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
--r--------  2.0 unx     9994 b- defN 24-Jan-22 12:06 mindformers/dataset/img_cls_dataset.py
--r--------  2.0 unx    21930 b- defN 24-Jan-22 12:06 mindformers/dataset/keyword_gen_dataset.py
--r--------  2.0 unx    16192 b- defN 24-Jan-22 12:06 mindformers/dataset/labels.py
--r--------  2.0 unx     8478 b- defN 24-Jan-22 12:06 mindformers/dataset/mask_language_model_dataset.py
--r--------  2.0 unx     9100 b- defN 24-Jan-22 12:06 mindformers/dataset/mim_dataset.py
--r--------  2.0 unx     7973 b- defN 24-Jan-22 12:06 mindformers/dataset/question_answering_dataset.py
--r--------  2.0 unx    13957 b- defN 24-Jan-22 12:06 mindformers/dataset/reward_model_dataset.py
--r--------  2.0 unx     7515 b- defN 24-Jan-22 12:06 mindformers/dataset/text_classification_dataset.py
--r--------  2.0 unx     9984 b- defN 24-Jan-22 12:06 mindformers/dataset/token_classification_dataset.py
--r--------  2.0 unx    11591 b- defN 24-Jan-22 12:06 mindformers/dataset/translation_dataset.py
--r--------  2.0 unx     1949 b- defN 24-Jan-22 12:06 mindformers/dataset/utils.py
--r--------  2.0 unx     8502 b- defN 24-Jan-22 12:06 mindformers/dataset/zero_shot_image_classification_dataset.py
--r--------  2.0 unx     1489 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/__init__.py
--r--------  2.0 unx     6075 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/adgen_dataloader.py
--r--------  2.0 unx     2930 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/build_dataloader.py
--r--------  2.0 unx     7893 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/cifar100_dataloader.py
--r--------  2.0 unx     6961 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/cluener_dataloader.py
--r--------  2.0 unx     5344 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/datareaders.py
--r--------  2.0 unx     7099 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/flickr8k_dataloader.py
--r--------  2.0 unx     6483 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
--r--------  2.0 unx    11335 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/multi_source_dataloader.py
--r--------  2.0 unx    12659 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/sft_dataloader.py
--r--------  2.0 unx     7423 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/sft_map_functions.py
--r--------  2.0 unx    23879 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/squad_dataloader.py
--r--------  2.0 unx    20177 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/training_dataloader.py
--r--------  2.0 unx     4360 b- defN 24-Jan-22 12:06 mindformers/dataset/dataloader/wmt16_dataloader.py
--r--------  2.0 unx      808 b- defN 24-Jan-22 12:06 mindformers/dataset/mask/__init__.py
--r--------  2.0 unx     2196 b- defN 24-Jan-22 12:06 mindformers/dataset/mask/build_mask.py
--r--------  2.0 unx     3760 b- defN 24-Jan-22 12:06 mindformers/dataset/mask/vision_mask.py
--r--------  2.0 unx      754 b- defN 24-Jan-22 12:06 mindformers/dataset/sampler/__init__.py
--r--------  2.0 unx     2719 b- defN 24-Jan-22 12:06 mindformers/dataset/sampler/build_sampler.py
--r--------  2.0 unx     1192 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/__init__.py
--r--------  2.0 unx    33409 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/auto_augment.py
--r--------  2.0 unx     3364 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/build_transforms.py
--r--------  2.0 unx    11513 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/mixup.py
--r--------  2.0 unx     4846 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/random_erasing.py
--r--------  2.0 unx     6231 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/text_transforms.py
--r--------  2.0 unx    12281 b- defN 24-Jan-22 12:06 mindformers/dataset/transforms/vision_transforms.py
--r--------  2.0 unx      984 b- defN 24-Jan-22 12:06 mindformers/generation/__init__.py
--r--------  2.0 unx    18947 b- defN 24-Jan-22 12:06 mindformers/generation/beam_search.py
--r--------  2.0 unx     8660 b- defN 24-Jan-22 12:06 mindformers/generation/generation_config.py
--r--------  2.0 unx     9729 b- defN 24-Jan-22 12:06 mindformers/generation/logits_process.py
--r--------  2.0 unx    11746 b- defN 24-Jan-22 12:06 mindformers/generation/streamers.py
--r--------  2.0 unx    56513 b- defN 24-Jan-22 12:06 mindformers/generation/text_generator.py
--r--------  2.0 unx     2956 b- defN 24-Jan-22 12:06 mindformers/generation/utils.py
--r--------  2.0 unx      862 b- defN 24-Jan-22 12:06 mindformers/inference/__init__.py
--r--------  2.0 unx     1406 b- defN 24-Jan-22 12:06 mindformers/inference/context.py
--r--------  2.0 unx     2267 b- defN 24-Jan-22 12:06 mindformers/inference/infer_config.py
--r--------  2.0 unx     1930 b- defN 24-Jan-22 12:06 mindformers/inference/infer_task.py
--r--------  2.0 unx    10700 b- defN 24-Jan-22 12:06 mindformers/inference/pipeline.py
--r--------  2.0 unx     2561 b- defN 24-Jan-22 12:06 mindformers/inference/postprocess_sampler.py
--r--------  2.0 unx      704 b- defN 24-Jan-22 12:06 mindformers/inference/infers/__init__.py
--r--------  2.0 unx     8334 b- defN 24-Jan-22 12:06 mindformers/inference/infers/base_infer.py
--r--------  2.0 unx    26511 b- defN 24-Jan-22 12:06 mindformers/inference/infers/text_generator_infer.py
--r--------  2.0 unx     1912 b- defN 24-Jan-22 12:06 mindformers/models/__init__.py
--r--------  2.0 unx    10530 b- defN 24-Jan-22 12:06 mindformers/models/base_config.py
--r--------  2.0 unx    38049 b- defN 24-Jan-22 12:06 mindformers/models/base_fast_tokenizer.py
--r--------  2.0 unx    17280 b- defN 24-Jan-22 12:06 mindformers/models/base_model.py
--r--------  2.0 unx    13391 b- defN 24-Jan-22 12:06 mindformers/models/base_processor.py
--r--------  2.0 unx   207274 b- defN 24-Jan-22 12:06 mindformers/models/base_tokenizer.py
--r--------  2.0 unx     2681 b- defN 24-Jan-22 12:06 mindformers/models/build_config.py
--r--------  2.0 unx     4428 b- defN 24-Jan-22 12:06 mindformers/models/build_model.py
--r--------  2.0 unx     2461 b- defN 24-Jan-22 12:06 mindformers/models/build_processor.py
--r--------  2.0 unx     3570 b- defN 24-Jan-22 12:06 mindformers/models/build_tokenizer.py
--r--------  2.0 unx    16629 b- defN 24-Jan-22 12:06 mindformers/models/convert_slow_tokenizer.py
--r--------  2.0 unx    50997 b- defN 24-Jan-22 12:06 mindformers/models/sentencepiece_model_pb2.py
--r--------  2.0 unx     6645 b- defN 24-Jan-22 12:06 mindformers/models/sentencepiece_model_pb2_new.py
--r--------  2.0 unx     1352 b- defN 24-Jan-22 12:06 mindformers/models/utils.py
--r--------  2.0 unx     1191 b- defN 24-Jan-22 12:06 mindformers/models/bert/__init__.py
--r--------  2.0 unx    29342 b- defN 24-Jan-22 12:06 mindformers/models/bert/bert.py
--r--------  2.0 unx     8036 b- defN 24-Jan-22 12:06 mindformers/models/bert/bert_config.py
--r--------  2.0 unx     3608 b- defN 24-Jan-22 12:06 mindformers/models/bert/bert_processor.py
--r--------  2.0 unx    25708 b- defN 24-Jan-22 12:06 mindformers/models/bert/bert_tokenizer.py
--r--------  2.0 unx     8224 b- defN 24-Jan-22 12:06 mindformers/models/bert/bert_tokenizer_fast.py
--r--------  2.0 unx     8638 b- defN 24-Jan-22 12:06 mindformers/models/bert/convert_weight.py
--r--------  2.0 unx     1189 b- defN 24-Jan-22 12:06 mindformers/models/blip2/__init__.py
--r--------  2.0 unx     3727 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2.py
--r--------  2.0 unx     6665 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_config.py
--r--------  2.0 unx     9561 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_itm_evaluator.py
--r--------  2.0 unx     8252 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_llama.py
--r--------  2.0 unx    11459 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_llm.py
--r--------  2.0 unx     7077 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_processor.py
--r--------  2.0 unx    23696 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_qformer.py
--r--------  2.0 unx     1501 b- defN 24-Jan-22 12:06 mindformers/models/blip2/blip2_vit.py
--r--------  2.0 unx     5519 b- defN 24-Jan-22 12:06 mindformers/models/blip2/convert_weight.py
--r--------  2.0 unx     4032 b- defN 24-Jan-22 12:06 mindformers/models/blip2/layers.py
--r--------  2.0 unx    70194 b- defN 24-Jan-22 12:06 mindformers/models/blip2/qformer.py
--r--------  2.0 unx     5239 b- defN 24-Jan-22 12:06 mindformers/models/blip2/qformer_config.py
--r--------  2.0 unx     1137 b- defN 24-Jan-22 12:06 mindformers/models/bloom/__init__.py
--r--------  2.0 unx    16962 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom.py
--r--------  2.0 unx     9127 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom_config.py
--r--------  2.0 unx     3474 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom_processor.py
--r--------  2.0 unx     4595 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom_reward.py
--r--------  2.0 unx    10501 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom_tokenizer.py
--r--------  2.0 unx     6806 b- defN 24-Jan-22 12:06 mindformers/models/bloom/bloom_tokenizer_fast.py
--r--------  2.0 unx     6527 b- defN 24-Jan-22 12:06 mindformers/models/bloom/convert_weight.py
--r--------  2.0 unx    32878 b- defN 24-Jan-22 12:06 mindformers/models/bloom/layers.py
--r--------  2.0 unx     1030 b- defN 24-Jan-22 12:06 mindformers/models/clip/__init__.py
--r--------  2.0 unx    10123 b- defN 24-Jan-22 12:06 mindformers/models/clip/clip.py
--r--------  2.0 unx     9082 b- defN 24-Jan-22 12:06 mindformers/models/clip/clip_config.py
--r--------  2.0 unx    10123 b- defN 24-Jan-22 12:06 mindformers/models/clip/clip_modules.py
--r--------  2.0 unx     4894 b- defN 24-Jan-22 12:06 mindformers/models/clip/clip_processor.py
--r--------  2.0 unx    11864 b- defN 24-Jan-22 12:06 mindformers/models/clip/clip_tokenizer.py
--r--------  2.0 unx     2989 b- defN 24-Jan-22 12:06 mindformers/models/clip/convert_weight.py
--r--------  2.0 unx     1036 b- defN 24-Jan-22 12:06 mindformers/models/glm/__init__.py
--r--------  2.0 unx    20251 b- defN 24-Jan-22 12:06 mindformers/models/glm/attention.py
--r--------  2.0 unx    16123 b- defN 24-Jan-22 12:06 mindformers/models/glm/chatglm_6b_tokenizer.py
--r--------  2.0 unx     2224 b- defN 24-Jan-22 12:06 mindformers/models/glm/convert_weight.py
--r--------  2.0 unx    22268 b- defN 24-Jan-22 12:06 mindformers/models/glm/glm.py
--r--------  2.0 unx    11300 b- defN 24-Jan-22 12:06 mindformers/models/glm/glm_config.py
--r--------  2.0 unx     4309 b- defN 24-Jan-22 12:06 mindformers/models/glm/glm_processor.py
--r--------  2.0 unx    13117 b- defN 24-Jan-22 12:06 mindformers/models/glm/layers.py
--r--------  2.0 unx      901 b- defN 24-Jan-22 12:06 mindformers/models/glm2/__init__.py
--r--------  2.0 unx    13135 b- defN 24-Jan-22 12:06 mindformers/models/glm2/glm2.py
--r--------  2.0 unx     4685 b- defN 24-Jan-22 12:06 mindformers/models/glm2/glm2_config.py
--r--------  2.0 unx     6386 b- defN 24-Jan-22 12:06 mindformers/models/glm2/glm2_modules.py
--r--------  2.0 unx    10928 b- defN 24-Jan-22 12:06 mindformers/models/glm2/glm2_tokenizer.py
--r--------  2.0 unx    32231 b- defN 24-Jan-22 12:06 mindformers/models/glm2/glm2_transformer.py
--r--------  2.0 unx      789 b- defN 24-Jan-22 12:06 mindformers/models/glm3/__init__.py
--r--------  2.0 unx    16842 b- defN 24-Jan-22 12:06 mindformers/models/glm3/glm3_tokenizer.py
--r--------  2.0 unx     1106 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/__init__.py
--r--------  2.0 unx     7212 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/convert_weight.py
--r--------  2.0 unx    28341 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt2.py
--r--------  2.0 unx     8920 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt2_config.py
--r--------  2.0 unx     3430 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt2_processor.py
--r--------  2.0 unx    14380 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt2_tokenizer.py
--r--------  2.0 unx     9074 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt2_tokenizer_fast.py
--r--------  2.0 unx    11091 b- defN 24-Jan-22 12:06 mindformers/models/gpt2/gpt_modules.py
--r--------  2.0 unx     1061 b- defN 24-Jan-22 12:06 mindformers/models/llama/__init__.py
--r--------  2.0 unx     6999 b- defN 24-Jan-22 12:06 mindformers/models/llama/convert_weight.py
--r--------  2.0 unx    23716 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama.py
--r--------  2.0 unx     9984 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_config.py
--r--------  2.0 unx    34676 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_interleave.py
--r--------  2.0 unx    24852 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_layer.py
--r--------  2.0 unx     3440 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_processor.py
--r--------  2.0 unx    17102 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_tokenizer.py
--r--------  2.0 unx     9034 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_tokenizer_fast.py
--r--------  2.0 unx    29200 b- defN 24-Jan-22 12:06 mindformers/models/llama/llama_transformer.py
--r--------  2.0 unx      878 b- defN 24-Jan-22 12:06 mindformers/models/mae/__init__.py
--r--------  2.0 unx     3416 b- defN 24-Jan-22 12:06 mindformers/models/mae/convert_weight.py
--r--------  2.0 unx    18082 b- defN 24-Jan-22 12:06 mindformers/models/mae/mae.py
--r--------  2.0 unx     7343 b- defN 24-Jan-22 12:06 mindformers/models/mae/mae_config.py
--r--------  2.0 unx    36274 b- defN 24-Jan-22 12:06 mindformers/models/mae/mae_modules.py
--r--------  2.0 unx     6194 b- defN 24-Jan-22 12:06 mindformers/models/mae/mae_processor.py
--r--------  2.0 unx     1022 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/__init__.py
--r--------  2.0 unx     5469 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/convert_weight.py
--r--------  2.0 unx    28383 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/pangualpha.py
--r--------  2.0 unx     4372 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/pangualpha_config.py
--r--------  2.0 unx     2394 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/pangualpha_processor.py
--r--------  2.0 unx     8177 b- defN 24-Jan-22 12:06 mindformers/models/pangualpha/pangualpha_tokenizer.py
--r--------  2.0 unx     1180 b- defN 24-Jan-22 12:06 mindformers/models/sam/__init__.py
--r--------  2.0 unx     3489 b- defN 24-Jan-22 12:06 mindformers/models/sam/conver_weight.py
--r--------  2.0 unx     5967 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam.py
--r--------  2.0 unx     7758 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_config.py
--r--------  2.0 unx    17459 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_image_encoder.py
--r--------  2.0 unx     2944 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_layers.py
--r--------  2.0 unx    22703 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_mask_decoder.py
--r--------  2.0 unx     8556 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_processor.py
--r--------  2.0 unx    10575 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_prompt_encoder.py
--r--------  2.0 unx    22757 b- defN 24-Jan-22 12:06 mindformers/models/sam/sam_utils.py
--r--------  2.0 unx      885 b- defN 24-Jan-22 12:06 mindformers/models/swin/__init__.py
--r--------  2.0 unx     4949 b- defN 24-Jan-22 12:06 mindformers/models/swin/convert_weight.py
--r--------  2.0 unx    16303 b- defN 24-Jan-22 12:06 mindformers/models/swin/swin.py
--r--------  2.0 unx     7783 b- defN 24-Jan-22 12:06 mindformers/models/swin/swin_config.py
--r--------  2.0 unx    31158 b- defN 24-Jan-22 12:06 mindformers/models/swin/swin_modules.py
--r--------  2.0 unx     5472 b- defN 24-Jan-22 12:06 mindformers/models/swin/swin_processor.py
--r--------  2.0 unx     1192 b- defN 24-Jan-22 12:06 mindformers/models/t5/__init__.py
--r--------  2.0 unx     8077 b- defN 24-Jan-22 12:06 mindformers/models/t5/convert_weight.py
--r--------  2.0 unx    10964 b- defN 24-Jan-22 12:06 mindformers/models/t5/mt5.py
--r--------  2.0 unx    90291 b- defN 24-Jan-22 12:06 mindformers/models/t5/t5.py
--r--------  2.0 unx    11544 b- defN 24-Jan-22 12:06 mindformers/models/t5/t5_config.py
--r--------  2.0 unx     4164 b- defN 24-Jan-22 12:06 mindformers/models/t5/t5_processor.py
--r--------  2.0 unx    20276 b- defN 24-Jan-22 12:06 mindformers/models/t5/t5_tokenizer.py
--r--------  2.0 unx     9881 b- defN 24-Jan-22 12:06 mindformers/models/t5/t5_tokenizer_fast.py
--r--------  2.0 unx      948 b- defN 24-Jan-22 12:06 mindformers/models/vit/__init__.py
--r--------  2.0 unx     3364 b- defN 24-Jan-22 12:06 mindformers/models/vit/convert_weight.py
--r--------  2.0 unx    13411 b- defN 24-Jan-22 12:06 mindformers/models/vit/vit.py
--r--------  2.0 unx     8231 b- defN 24-Jan-22 12:06 mindformers/models/vit/vit_config.py
--r--------  2.0 unx    37548 b- defN 24-Jan-22 12:06 mindformers/models/vit/vit_modules.py
--r--------  2.0 unx     4925 b- defN 24-Jan-22 12:06 mindformers/models/vit/vit_processor.py
--r--------  2.0 unx      957 b- defN 24-Jan-22 12:06 mindformers/modules/__init__.py
--r--------  2.0 unx    50360 b- defN 24-Jan-22 12:06 mindformers/modules/activation.py
--r--------  2.0 unx    12959 b- defN 24-Jan-22 12:06 mindformers/modules/kvcache_mgr.py
--r--------  2.0 unx    48060 b- defN 24-Jan-22 12:06 mindformers/modules/layers.py
--r--------  2.0 unx    14414 b- defN 24-Jan-22 12:06 mindformers/modules/local_block_sparse_attention.py
--r--------  2.0 unx     1335 b- defN 24-Jan-22 12:06 mindformers/modules/transformer/__init__.py
--r--------  2.0 unx    39235 b- defN 24-Jan-22 12:06 mindformers/modules/transformer/moe.py
--r--------  2.0 unx     8481 b- defN 24-Jan-22 12:06 mindformers/modules/transformer/op_parallel_config.py
--r--------  2.0 unx   202706 b- defN 24-Jan-22 12:06 mindformers/modules/transformer/transformer.py
--r--------  2.0 unx      897 b- defN 24-Jan-22 12:06 mindformers/pet/__init__.py
--r--------  2.0 unx     1176 b- defN 24-Jan-22 12:06 mindformers/pet/constants.py
--r--------  2.0 unx     4920 b- defN 24-Jan-22 12:06 mindformers/pet/pet_config.py
--r--------  2.0 unx     3928 b- defN 24-Jan-22 12:06 mindformers/pet/pet_model.py
--r--------  2.0 unx     1096 b- defN 24-Jan-22 12:06 mindformers/pet/utils.py
--r--------  2.0 unx      696 b- defN 24-Jan-22 12:06 mindformers/pet/models/__init__.py
--r--------  2.0 unx     3511 b- defN 24-Jan-22 12:06 mindformers/pet/models/lora.py
--r--------  2.0 unx      955 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/__init__.py
--r--------  2.0 unx     1176 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/ada_adapter.py
--r--------  2.0 unx     1194 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/adalora_adapter.py
--r--------  2.0 unx     5435 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/lora_adapter.py
--r--------  2.0 unx     1649 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/pet_adapter.py
--r--------  2.0 unx     1165 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/prefix_tuning_adapter.py
--r--------  2.0 unx     1350 b- defN 24-Jan-22 12:06 mindformers/pet/tuners/ptuning2_adapter.py
--r--------  2.0 unx     2160 b- defN 24-Jan-22 12:06 mindformers/pipeline/__init__.py
--r--------  2.0 unx    10291 b- defN 24-Jan-22 12:06 mindformers/pipeline/base_pipeline.py
--r--------  2.0 unx     2393 b- defN 24-Jan-22 12:06 mindformers/pipeline/build_pipeline.py
--r--------  2.0 unx     6343 b- defN 24-Jan-22 12:06 mindformers/pipeline/fill_mask_pipeline.py
--r--------  2.0 unx     7544 b- defN 24-Jan-22 12:06 mindformers/pipeline/image_classification_pipeline.py
--r--------  2.0 unx     7168 b- defN 24-Jan-22 12:06 mindformers/pipeline/image_to_text_generation_pipeline.py
--r--------  2.0 unx     6591 b- defN 24-Jan-22 12:06 mindformers/pipeline/masked_image_modeling_pipeline.py
--r--------  2.0 unx     6226 b- defN 24-Jan-22 12:06 mindformers/pipeline/pipeline.py
--r--------  2.0 unx    18559 b- defN 24-Jan-22 12:06 mindformers/pipeline/question_answering_pipeline.py
--r--------  2.0 unx    27170 b- defN 24-Jan-22 12:06 mindformers/pipeline/segment_anything_pipeline.py
--r--------  2.0 unx    10648 b- defN 24-Jan-22 12:06 mindformers/pipeline/text_classification_pipeline.py
--r--------  2.0 unx    10875 b- defN 24-Jan-22 12:06 mindformers/pipeline/text_generation_pipeline.py
--r--------  2.0 unx    10121 b- defN 24-Jan-22 12:06 mindformers/pipeline/token_classification_pipeline.py
--r--------  2.0 unx     8167 b- defN 24-Jan-22 12:06 mindformers/pipeline/translation_pipeline.py
--r--------  2.0 unx     9276 b- defN 24-Jan-22 12:06 mindformers/pipeline/zero_shot_image_classification_pipeline.py
--r--------  2.0 unx     1127 b- defN 24-Jan-22 12:06 mindformers/tools/__init__.py
--r--------  2.0 unx    10202 b- defN 24-Jan-22 12:06 mindformers/tools/check_rules.py
--r--------  2.0 unx     4399 b- defN 24-Jan-22 12:06 mindformers/tools/download_tools.py
--r--------  2.0 unx     6243 b- defN 24-Jan-22 12:06 mindformers/tools/download_tools_multithread.py
--r--------  2.0 unx    12212 b- defN 24-Jan-22 12:06 mindformers/tools/export.py
--r--------  2.0 unx     6520 b- defN 24-Jan-22 12:06 mindformers/tools/hccl_tools.py
--r--------  2.0 unx     1925 b- defN 24-Jan-22 12:06 mindformers/tools/image_tools.py
--r--------  2.0 unx    22670 b- defN 24-Jan-22 12:06 mindformers/tools/logger.py
--r--------  2.0 unx     2316 b- defN 24-Jan-22 12:06 mindformers/tools/merge_hccl.py
--r--------  2.0 unx     5598 b- defN 24-Jan-22 12:06 mindformers/tools/moe_token_distribution_tools.py
--r--------  2.0 unx     3164 b- defN 24-Jan-22 12:06 mindformers/tools/transform_ckpt.py
--r--------  2.0 unx    13716 b- defN 24-Jan-22 12:06 mindformers/tools/utils.py
--r--------  2.0 unx      842 b- defN 24-Jan-22 12:06 mindformers/tools/cloud_adapter/__init__.py
--r--------  2.0 unx     8522 b- defN 24-Jan-22 12:06 mindformers/tools/cloud_adapter/cloud_adapter.py
--r--------  2.0 unx     3464 b- defN 24-Jan-22 12:06 mindformers/tools/cloud_adapter/cloud_monitor.py
--r--------  2.0 unx      905 b- defN 24-Jan-22 12:06 mindformers/tools/register/__init__.py
--r--------  2.0 unx    11037 b- defN 24-Jan-22 12:06 mindformers/tools/register/config.py
--r--------  2.0 unx     7056 b- defN 24-Jan-22 12:06 mindformers/tools/register/register.py
--r--------  2.0 unx     1982 b- defN 24-Jan-22 12:06 mindformers/trainer/__init__.py
--r--------  2.0 unx    51109 b- defN 24-Jan-22 12:06 mindformers/trainer/base_trainer.py
--r--------  2.0 unx     4428 b- defN 24-Jan-22 12:06 mindformers/trainer/build_trainer.py
--r--------  2.0 unx    55851 b- defN 24-Jan-22 12:06 mindformers/trainer/config_args.py
--r--------  2.0 unx     5925 b- defN 24-Jan-22 12:06 mindformers/trainer/optimizer_grouped_parameters.py
--r--------  2.0 unx    51978 b- defN 24-Jan-22 12:06 mindformers/trainer/trainer.py
--r--------  2.0 unx    15070 b- defN 24-Jan-22 12:06 mindformers/trainer/training_args.py
--r--------  2.0 unx    34430 b- defN 24-Jan-22 12:06 mindformers/trainer/utils.py
--r--------  2.0 unx      821 b- defN 24-Jan-22 12:06 mindformers/trainer/causal_language_modeling/__init__.py
--r--------  2.0 unx    20146 b- defN 24-Jan-22 12:06 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
--r--------  2.0 unx      866 b- defN 24-Jan-22 12:06 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
--r--------  2.0 unx     4609 b- defN 24-Jan-22 12:06 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
--r--------  2.0 unx      795 b- defN 24-Jan-22 12:06 mindformers/trainer/general_task_trainer/__init__.py
--r--------  2.0 unx     9462 b- defN 24-Jan-22 12:06 mindformers/trainer/general_task_trainer/general_task_trainer.py
--r--------  2.0 unx      928 b- defN 24-Jan-22 12:06 mindformers/trainer/image_classification/__init__.py
--r--------  2.0 unx     4622 b- defN 24-Jan-22 12:06 mindformers/trainer/image_classification/group_ic_params.py
--r--------  2.0 unx     9476 b- defN 24-Jan-22 12:06 mindformers/trainer/image_classification/image_classification.py
--r--------  2.0 unx     8209 b- defN 24-Jan-22 12:06 mindformers/trainer/image_classification/zero_shot_image_classification.py
--r--------  2.0 unx      823 b- defN 24-Jan-22 12:06 mindformers/trainer/image_to_text_generation/__init__.py
--r--------  2.0 unx     6107 b- defN 24-Jan-22 12:06 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
--r--------  2.0 unx      818 b- defN 24-Jan-22 12:06 mindformers/trainer/image_to_text_retrieval/__init__.py
--r--------  2.0 unx    12076 b- defN 24-Jan-22 12:06 mindformers/trainer/image_to_text_retrieval/eval_utils.py
--r--------  2.0 unx     7783 b- defN 24-Jan-22 12:06 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
--r--------  2.0 unx      818 b- defN 24-Jan-22 12:06 mindformers/trainer/masked_image_modeling/__init__.py
--r--------  2.0 unx     2197 b- defN 24-Jan-22 12:06 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
--r--------  2.0 unx     7432 b- defN 24-Jan-22 12:06 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
--r--------  2.0 unx      830 b- defN 24-Jan-22 12:06 mindformers/trainer/masked_language_modeling/__init__.py
--r--------  2.0 unx     6859 b- defN 24-Jan-22 12:06 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
--r--------  2.0 unx      799 b- defN 24-Jan-22 12:06 mindformers/trainer/question_answering/__init__.py
--r--------  2.0 unx     9218 b- defN 24-Jan-22 12:06 mindformers/trainer/question_answering/question_answering.py
--r--------  2.0 unx      803 b- defN 24-Jan-22 12:06 mindformers/trainer/text_classfication/__init__.py
--r--------  2.0 unx     9386 b- defN 24-Jan-22 12:06 mindformers/trainer/text_classfication/text_classification.py
--r--------  2.0 unx      807 b- defN 24-Jan-22 12:06 mindformers/trainer/token_classification/__init__.py
--r--------  2.0 unx     9500 b- defN 24-Jan-22 12:06 mindformers/trainer/token_classification/token_classification.py
--r--------  2.0 unx      795 b- defN 24-Jan-22 12:06 mindformers/trainer/translation/__init__.py
--r--------  2.0 unx     6738 b- defN 24-Jan-22 12:06 mindformers/trainer/translation/translation_finetune.py
--r--------  2.0 unx      913 b- defN 24-Jan-22 12:06 mindformers/wrapper/__init__.py
--r--------  2.0 unx    14329 b- defN 24-Jan-22 12:06 mindformers/wrapper/adaptive_loss_scale.py
--r--------  2.0 unx     4117 b- defN 24-Jan-22 12:06 mindformers/wrapper/build_wrapper.py
--r--------  2.0 unx    12204 b- defN 24-Jan-22 12:06 mindformers/wrapper/wrapper.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Jan-22 12:10 mindformers-1.0.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    24851 b- defN 24-Jan-22 12:10 mindformers-1.0.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Jan-22 12:10 mindformers-1.0.0.dist-info/WHEEL
--r--------  2.0 unx       12 b- defN 24-Jan-22 12:10 mindformers-1.0.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    40402 b- defN 24-Jan-22 12:10 mindformers-1.0.0.dist-info/RECORD
-410 files, 4195702 bytes uncompressed, 1079827 bytes compressed:  74.3%
+Zip file size: 1150263 bytes, number of entries: 412
+-rw-r--r--  2.0 unx    14676 b- defN 24-Mar-15 02:43 configs/README.md
+-rw-r--r--  2.0 unx     4241 b- defN 24-Mar-15 02:43 configs/bert/run_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4257 b- defN 24-Mar-15 02:43 configs/bert/run_bert_tiny_uncased.yaml
+-rw-r--r--  2.0 unx     6074 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
+-rw-r--r--  2.0 unx     6139 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
+-rw-r--r--  2.0 unx     4951 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6214 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
+-rw-r--r--  2.0 unx     4705 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     4761 b- defN 24-Mar-15 02:43 configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     4055 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_560m.yaml
+-rw-r--r--  2.0 unx     4054 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b_910b.yaml
+-rw-r--r--  2.0 unx     4156 b- defN 24-Mar-15 02:43 configs/bloom/run_bloom_7.1b_910b_fa.yaml
+-rw-r--r--  2.0 unx     4137 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4148 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4295 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4139 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4286 b- defN 24-Mar-15 02:43 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6097 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b.yaml
+-rw-r--r--  2.0 unx     5811 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_eval.yaml
+-rw-r--r--  2.0 unx     5866 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5870 b- defN 24-Mar-15 02:43 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
+-rw-r--r--  2.0 unx     4009 b- defN 24-Mar-15 02:43 configs/codellama/predict_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     5330 b- defN 24-Mar-15 02:43 configs/codellama/run_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     2637 b- defN 24-Mar-15 02:43 configs/general/run_general_task.yaml
+-rw-r--r--  2.0 unx     5756 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5677 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_infer.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_lora.yaml
+-rw-r--r--  2.0 unx     5861 b- defN 24-Mar-15 02:43 configs/glm/run_glm_6b_lora_infer.yaml
+-rw-r--r--  2.0 unx     1564 b- defN 24-Mar-15 02:43 configs/glm2/export_glm2_6b.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b.yaml
+-rw-r--r--  2.0 unx     5965 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5972 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_2k.yaml
+-rw-r--r--  2.0 unx     5971 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_2k_910b.yaml
+-rw-r--r--  2.0 unx     5919 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_910b.yaml
+-rw-r--r--  2.0 unx     5865 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_finetune_eval.yaml
+-rw-r--r--  2.0 unx     6246 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora.yaml
+-rw-r--r--  2.0 unx     6254 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_2k.yaml
+-rw-r--r--  2.0 unx     6252 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_2k_910b.yaml
+-rw-r--r--  2.0 unx     6240 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_910b.yaml
+-rw-r--r--  2.0 unx     6146 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_lora_eval.yaml
+-rw-r--r--  2.0 unx     6200 b- defN 24-Mar-15 02:43 configs/glm2/run_glm2_6b_ptuning2.yaml
+-rw-r--r--  2.0 unx     1640 b- defN 24-Mar-15 02:43 configs/glm3/export_glm3_6b.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Mar-15 02:43 configs/glm3/run_glm3_6b.yaml
+-rw-r--r--  2.0 unx     5956 b- defN 24-Mar-15 02:43 configs/glm3/run_glm3_6b_finetune_2k_910b.yaml
+-rw-r--r--  2.0 unx     4301 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2.yaml
+-rw-r--r--  2.0 unx     4678 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_13b.yaml
+-rw-r--r--  2.0 unx     4822 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_13b_910b.yaml
+-rw-r--r--  2.0 unx     4628 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_52b.yaml
+-rw-r--r--  2.0 unx     4331 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_lora.yaml
+-rw-r--r--  2.0 unx     4261 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_txtcls.yaml
+-rw-r--r--  2.0 unx     4631 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_xl.yaml
+-rw-r--r--  2.0 unx     4824 b- defN 24-Mar-15 02:43 configs/gpt2/run_gpt2_xl_lora.yaml
+-rwxr-xr-x  2.0 unx     5173 b- defN 24-Mar-15 02:43 configs/llama/run_llama_13b.yaml
+-rw-r--r--  2.0 unx     5176 b- defN 24-Mar-15 02:43 configs/llama/run_llama_13b_910b.yaml
+-rwxr-xr-x  2.0 unx     5169 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b.yaml
+-rw-r--r--  2.0 unx     5167 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     5625 b- defN 24-Mar-15 02:43 configs/llama/run_llama_7b_lora.yaml
+-rw-r--r--  2.0 unx     2890 b- defN 24-Mar-15 02:43 configs/llama2/export_llama2_13b.yaml
+-rw-r--r--  2.0 unx     2721 b- defN 24-Mar-15 02:43 configs/llama2/export_llama2_7b.yaml
+-rw-r--r--  2.0 unx     4057 b- defN 24-Mar-15 02:43 configs/llama2/predict_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5424 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b.yaml
+-rw-r--r--  2.0 unx     5087 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b.yaml
+-rw-r--r--  2.0 unx     5655 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5137 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5420 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_13b_lora_910b.yaml
+-rw-r--r--  2.0 unx     5473 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b.yaml
+-rw-r--r--  2.0 unx     5332 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5897 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5378 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_70b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5414 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b.yaml
+-rw-r--r--  2.0 unx     5100 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b.yaml
+-rw-r--r--  2.0 unx     5632 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5113 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5452 b- defN 24-Mar-15 02:43 configs/llama2/run_llama2_7b_lora_910b.yaml
+-rw-r--r--  2.0 unx     4785 b- defN 24-Mar-15 02:43 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
+-rw-r--r--  2.0 unx     4942 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_13b.yaml
+-rw-r--r--  2.0 unx     4791 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b.yaml
+-rw-r--r--  2.0 unx     4229 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
+-rw-r--r--  2.0 unx     4495 b- defN 24-Mar-15 02:43 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
+-rw-r--r--  2.0 unx     5531 b- defN 24-Mar-15 02:43 configs/qa/run_qa_bert_base_uncased.yaml
+-rwxr-xr-x  2.0 unx     6818 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-b.yaml
+-rwxr-xr-x  2.0 unx     6821 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-h.yaml
+-rw-r--r--  2.0 unx     6821 b- defN 24-Mar-15 02:43 configs/sam/run_sam_vit-l.yaml
+-rw-r--r--  2.0 unx     6149 b- defN 24-Mar-15 02:43 configs/swin/run_swin_base_p4w7_224_100ep.yaml
+-rw-r--r--  2.0 unx     4494 b- defN 24-Mar-15 02:43 configs/t5/run_t5_small_on_wmt16.yaml
+-rw-r--r--  2.0 unx     4455 b- defN 24-Mar-15 02:43 configs/t5/run_t5_tiny_on_wmt16.yaml
+-rw-r--r--  2.0 unx     5772 b- defN 24-Mar-15 02:43 configs/tokcls/run_tokcls_bert_base_chinese.yaml
+-rw-r--r--  2.0 unx     5788 b- defN 24-Mar-15 02:43 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
+-rw-r--r--  2.0 unx     4428 b- defN 24-Mar-15 02:43 configs/txtcls/run_txtcls_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4438 b- defN 24-Mar-15 02:43 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
+-rw-r--r--  2.0 unx     6020 b- defN 24-Mar-15 02:43 configs/vit/run_vit_base_p16_224_100ep.yaml
+-r--------  2.0 unx      277 b- defN 24-Mar-15 02:44 mindformers/.commit_id
+-r--------  2.0 unx     1402 b- defN 24-Mar-15 02:43 mindformers/__init__.py
+-r--------  2.0 unx    39712 b- defN 24-Mar-15 02:43 mindformers/auto_class.py
+-r--------  2.0 unx    67667 b- defN 24-Mar-15 02:43 mindformers/mindformer_book.py
+-r--------  2.0 unx    11096 b- defN 24-Mar-15 02:43 mindformers/version_control.py
+-r--------  2.0 unx     1297 b- defN 24-Mar-15 02:43 mindformers/core/__init__.py
+-r--------  2.0 unx     3956 b- defN 24-Mar-15 02:43 mindformers/core/clip_grad.py
+-r--------  2.0 unx     2995 b- defN 24-Mar-15 02:43 mindformers/core/parallel_config.py
+-r--------  2.0 unx      809 b- defN 24-Mar-15 02:43 mindformers/core/callback/__init__.py
+-r--------  2.0 unx     3309 b- defN 24-Mar-15 02:43 mindformers/core/callback/build_callback.py
+-r--------  2.0 unx    37078 b- defN 24-Mar-15 02:43 mindformers/core/callback/callback.py
+-r--------  2.0 unx      833 b- defN 24-Mar-15 02:43 mindformers/core/context/__init__.py
+-r--------  2.0 unx     8659 b- defN 24-Mar-15 02:43 mindformers/core/context/build_context.py
+-r--------  2.0 unx      789 b- defN 24-Mar-15 02:43 mindformers/core/loss/__init__.py
+-r--------  2.0 unx     2866 b- defN 24-Mar-15 02:43 mindformers/core/loss/build_loss.py
+-r--------  2.0 unx    18164 b- defN 24-Mar-15 02:43 mindformers/core/loss/loss.py
+-r--------  2.0 unx      802 b- defN 24-Mar-15 02:43 mindformers/core/lr/__init__.py
+-r--------  2.0 unx     3844 b- defN 24-Mar-15 02:43 mindformers/core/lr/build_lr.py
+-r--------  2.0 unx    14711 b- defN 24-Mar-15 02:43 mindformers/core/lr/lr_schedule.py
+-r--------  2.0 unx      800 b- defN 24-Mar-15 02:43 mindformers/core/metric/__init__.py
+-r--------  2.0 unx     2683 b- defN 24-Mar-15 02:43 mindformers/core/metric/build_metric.py
+-r--------  2.0 unx    35009 b- defN 24-Mar-15 02:43 mindformers/core/metric/metric.py
+-r--------  2.0 unx     1768 b- defN 24-Mar-15 02:43 mindformers/core/metric/utils.py
+-r--------  2.0 unx      847 b- defN 24-Mar-15 02:43 mindformers/core/optim/__init__.py
+-r--------  2.0 unx     4623 b- defN 24-Mar-15 02:43 mindformers/core/optim/build_optim.py
+-r--------  2.0 unx    21063 b- defN 24-Mar-15 02:43 mindformers/core/optim/came.py
+-r--------  2.0 unx    31399 b- defN 24-Mar-15 02:43 mindformers/core/optim/optim.py
+-r--------  2.0 unx     2427 b- defN 24-Mar-15 02:43 mindformers/dataset/__init__.py
+-r--------  2.0 unx     3379 b- defN 24-Mar-15 02:43 mindformers/dataset/base_dataset.py
+-r--------  2.0 unx     2560 b- defN 24-Mar-15 02:43 mindformers/dataset/build_dataset.py
+-r--------  2.0 unx    12908 b- defN 24-Mar-15 02:43 mindformers/dataset/causal_language_model_dataset.py
+-r--------  2.0 unx     9758 b- defN 24-Mar-15 02:43 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
+-r--------  2.0 unx     9994 b- defN 24-Mar-15 02:43 mindformers/dataset/img_cls_dataset.py
+-r--------  2.0 unx    21930 b- defN 24-Mar-15 02:43 mindformers/dataset/keyword_gen_dataset.py
+-r--------  2.0 unx    16192 b- defN 24-Mar-15 02:43 mindformers/dataset/labels.py
+-r--------  2.0 unx     8478 b- defN 24-Mar-15 02:43 mindformers/dataset/mask_language_model_dataset.py
+-r--------  2.0 unx     9100 b- defN 24-Mar-15 02:43 mindformers/dataset/mim_dataset.py
+-r--------  2.0 unx     7973 b- defN 24-Mar-15 02:43 mindformers/dataset/question_answering_dataset.py
+-r--------  2.0 unx    13957 b- defN 24-Mar-15 02:43 mindformers/dataset/reward_model_dataset.py
+-r--------  2.0 unx     7515 b- defN 24-Mar-15 02:43 mindformers/dataset/text_classification_dataset.py
+-r--------  2.0 unx     9984 b- defN 24-Mar-15 02:43 mindformers/dataset/token_classification_dataset.py
+-r--------  2.0 unx    11591 b- defN 24-Mar-15 02:43 mindformers/dataset/translation_dataset.py
+-r--------  2.0 unx     1949 b- defN 24-Mar-15 02:43 mindformers/dataset/utils.py
+-r--------  2.0 unx     8502 b- defN 24-Mar-15 02:43 mindformers/dataset/zero_shot_image_classification_dataset.py
+-r--------  2.0 unx     1489 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/__init__.py
+-r--------  2.0 unx     6075 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/adgen_dataloader.py
+-r--------  2.0 unx     2930 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/build_dataloader.py
+-r--------  2.0 unx     7893 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/cifar100_dataloader.py
+-r--------  2.0 unx     6961 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/cluener_dataloader.py
+-r--------  2.0 unx     5344 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/datareaders.py
+-r--------  2.0 unx     7099 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/flickr8k_dataloader.py
+-r--------  2.0 unx     6483 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
+-r--------  2.0 unx    11335 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/multi_source_dataloader.py
+-r--------  2.0 unx    12659 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/sft_dataloader.py
+-r--------  2.0 unx     7423 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/sft_map_functions.py
+-r--------  2.0 unx    23879 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/squad_dataloader.py
+-r--------  2.0 unx    20177 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/training_dataloader.py
+-r--------  2.0 unx     4360 b- defN 24-Mar-15 02:43 mindformers/dataset/dataloader/wmt16_dataloader.py
+-r--------  2.0 unx      808 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/__init__.py
+-r--------  2.0 unx     2196 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/build_mask.py
+-r--------  2.0 unx     3760 b- defN 24-Mar-15 02:43 mindformers/dataset/mask/vision_mask.py
+-r--------  2.0 unx      754 b- defN 24-Mar-15 02:43 mindformers/dataset/sampler/__init__.py
+-r--------  2.0 unx     2719 b- defN 24-Mar-15 02:43 mindformers/dataset/sampler/build_sampler.py
+-r--------  2.0 unx     1192 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/__init__.py
+-r--------  2.0 unx    33409 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/auto_augment.py
+-r--------  2.0 unx     3364 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/build_transforms.py
+-r--------  2.0 unx    11513 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/mixup.py
+-r--------  2.0 unx     4846 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/random_erasing.py
+-r--------  2.0 unx     6231 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/text_transforms.py
+-r--------  2.0 unx    12281 b- defN 24-Mar-15 02:43 mindformers/dataset/transforms/vision_transforms.py
+-r--------  2.0 unx      984 b- defN 24-Mar-15 02:43 mindformers/generation/__init__.py
+-r--------  2.0 unx    18947 b- defN 24-Mar-15 02:43 mindformers/generation/beam_search.py
+-r--------  2.0 unx     8660 b- defN 24-Mar-15 02:43 mindformers/generation/generation_config.py
+-r--------  2.0 unx     9729 b- defN 24-Mar-15 02:43 mindformers/generation/logits_process.py
+-r--------  2.0 unx    11746 b- defN 24-Mar-15 02:43 mindformers/generation/streamers.py
+-r--------  2.0 unx    56513 b- defN 24-Mar-15 02:43 mindformers/generation/text_generator.py
+-r--------  2.0 unx     2956 b- defN 24-Mar-15 02:43 mindformers/generation/utils.py
+-r--------  2.0 unx      862 b- defN 24-Mar-15 02:43 mindformers/inference/__init__.py
+-r--------  2.0 unx     1406 b- defN 24-Mar-15 02:43 mindformers/inference/context.py
+-r--------  2.0 unx     2533 b- defN 24-Mar-15 02:43 mindformers/inference/infer_config.py
+-r--------  2.0 unx     1930 b- defN 24-Mar-15 02:43 mindformers/inference/infer_task.py
+-r--------  2.0 unx    10700 b- defN 24-Mar-15 02:43 mindformers/inference/pipeline.py
+-r--------  2.0 unx     2561 b- defN 24-Mar-15 02:43 mindformers/inference/postprocess_sampler.py
+-r--------  2.0 unx      704 b- defN 24-Mar-15 02:43 mindformers/inference/infers/__init__.py
+-r--------  2.0 unx     8435 b- defN 24-Mar-15 02:43 mindformers/inference/infers/base_infer.py
+-r--------  2.0 unx     3234 b- defN 24-Mar-15 02:43 mindformers/inference/infers/cache_engine.py
+-r--------  2.0 unx    31700 b- defN 24-Mar-15 02:43 mindformers/inference/infers/text_generator_infer.py
+-r--------  2.0 unx     1912 b- defN 24-Mar-15 02:43 mindformers/models/__init__.py
+-r--------  2.0 unx    10530 b- defN 24-Mar-15 02:43 mindformers/models/base_config.py
+-r--------  2.0 unx    38049 b- defN 24-Mar-15 02:43 mindformers/models/base_fast_tokenizer.py
+-r--------  2.0 unx    17280 b- defN 24-Mar-15 02:43 mindformers/models/base_model.py
+-r--------  2.0 unx    13391 b- defN 24-Mar-15 02:43 mindformers/models/base_processor.py
+-r--------  2.0 unx   207274 b- defN 24-Mar-15 02:43 mindformers/models/base_tokenizer.py
+-r--------  2.0 unx     2681 b- defN 24-Mar-15 02:43 mindformers/models/build_config.py
+-r--------  2.0 unx     4428 b- defN 24-Mar-15 02:43 mindformers/models/build_model.py
+-r--------  2.0 unx     2461 b- defN 24-Mar-15 02:43 mindformers/models/build_processor.py
+-r--------  2.0 unx     3570 b- defN 24-Mar-15 02:43 mindformers/models/build_tokenizer.py
+-r--------  2.0 unx    16629 b- defN 24-Mar-15 02:43 mindformers/models/convert_slow_tokenizer.py
+-r--------  2.0 unx    50997 b- defN 24-Mar-15 02:43 mindformers/models/sentencepiece_model_pb2.py
+-r--------  2.0 unx     6645 b- defN 24-Mar-15 02:43 mindformers/models/sentencepiece_model_pb2_new.py
+-r--------  2.0 unx     1352 b- defN 24-Mar-15 02:43 mindformers/models/utils.py
+-r--------  2.0 unx     1191 b- defN 24-Mar-15 02:43 mindformers/models/bert/__init__.py
+-r--------  2.0 unx    29342 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert.py
+-r--------  2.0 unx     8036 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_config.py
+-r--------  2.0 unx     3608 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_processor.py
+-r--------  2.0 unx    25708 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_tokenizer.py
+-r--------  2.0 unx     8224 b- defN 24-Mar-15 02:43 mindformers/models/bert/bert_tokenizer_fast.py
+-r--------  2.0 unx     8638 b- defN 24-Mar-15 02:43 mindformers/models/bert/convert_weight.py
+-r--------  2.0 unx     1189 b- defN 24-Mar-15 02:43 mindformers/models/blip2/__init__.py
+-r--------  2.0 unx     3727 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2.py
+-r--------  2.0 unx     6665 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_config.py
+-r--------  2.0 unx     9561 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_itm_evaluator.py
+-r--------  2.0 unx     8252 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_llama.py
+-r--------  2.0 unx    11459 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_llm.py
+-r--------  2.0 unx     7077 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_processor.py
+-r--------  2.0 unx    23696 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_qformer.py
+-r--------  2.0 unx     1501 b- defN 24-Mar-15 02:43 mindformers/models/blip2/blip2_vit.py
+-r--------  2.0 unx     5519 b- defN 24-Mar-15 02:43 mindformers/models/blip2/convert_weight.py
+-r--------  2.0 unx     4032 b- defN 24-Mar-15 02:43 mindformers/models/blip2/layers.py
+-r--------  2.0 unx    70194 b- defN 24-Mar-15 02:43 mindformers/models/blip2/qformer.py
+-r--------  2.0 unx     5239 b- defN 24-Mar-15 02:43 mindformers/models/blip2/qformer_config.py
+-r--------  2.0 unx     1137 b- defN 24-Mar-15 02:43 mindformers/models/bloom/__init__.py
+-r--------  2.0 unx    16962 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom.py
+-r--------  2.0 unx     9127 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_config.py
+-r--------  2.0 unx     3474 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_processor.py
+-r--------  2.0 unx     4595 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_reward.py
+-r--------  2.0 unx    10501 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_tokenizer.py
+-r--------  2.0 unx     6806 b- defN 24-Mar-15 02:43 mindformers/models/bloom/bloom_tokenizer_fast.py
+-r--------  2.0 unx     6527 b- defN 24-Mar-15 02:43 mindformers/models/bloom/convert_weight.py
+-r--------  2.0 unx    32878 b- defN 24-Mar-15 02:43 mindformers/models/bloom/layers.py
+-r--------  2.0 unx     1030 b- defN 24-Mar-15 02:43 mindformers/models/clip/__init__.py
+-r--------  2.0 unx    10123 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip.py
+-r--------  2.0 unx     9082 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_config.py
+-r--------  2.0 unx    10123 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_modules.py
+-r--------  2.0 unx     4894 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_processor.py
+-r--------  2.0 unx    11864 b- defN 24-Mar-15 02:43 mindformers/models/clip/clip_tokenizer.py
+-r--------  2.0 unx     2989 b- defN 24-Mar-15 02:43 mindformers/models/clip/convert_weight.py
+-r--------  2.0 unx     1036 b- defN 24-Mar-15 02:43 mindformers/models/glm/__init__.py
+-r--------  2.0 unx    20251 b- defN 24-Mar-15 02:43 mindformers/models/glm/attention.py
+-r--------  2.0 unx    16123 b- defN 24-Mar-15 02:43 mindformers/models/glm/chatglm_6b_tokenizer.py
+-r--------  2.0 unx     2224 b- defN 24-Mar-15 02:43 mindformers/models/glm/convert_weight.py
+-r--------  2.0 unx    22268 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm.py
+-r--------  2.0 unx    11300 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm_config.py
+-r--------  2.0 unx     4309 b- defN 24-Mar-15 02:43 mindformers/models/glm/glm_processor.py
+-r--------  2.0 unx    13117 b- defN 24-Mar-15 02:43 mindformers/models/glm/layers.py
+-r--------  2.0 unx      901 b- defN 24-Mar-15 02:43 mindformers/models/glm2/__init__.py
+-r--------  2.0 unx    13135 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2.py
+-r--------  2.0 unx     4685 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_config.py
+-r--------  2.0 unx     6386 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_modules.py
+-r--------  2.0 unx    10928 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_tokenizer.py
+-r--------  2.0 unx    32231 b- defN 24-Mar-15 02:43 mindformers/models/glm2/glm2_transformer.py
+-r--------  2.0 unx      789 b- defN 24-Mar-15 02:43 mindformers/models/glm3/__init__.py
+-r--------  2.0 unx    16842 b- defN 24-Mar-15 02:43 mindformers/models/glm3/glm3_tokenizer.py
+-r--------  2.0 unx     1106 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/__init__.py
+-r--------  2.0 unx     7212 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/convert_weight.py
+-r--------  2.0 unx    28341 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2.py
+-r--------  2.0 unx     8920 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_config.py
+-r--------  2.0 unx     3430 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_processor.py
+-r--------  2.0 unx    14380 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_tokenizer.py
+-r--------  2.0 unx     9074 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt2_tokenizer_fast.py
+-r--------  2.0 unx    11091 b- defN 24-Mar-15 02:43 mindformers/models/gpt2/gpt_modules.py
+-r--------  2.0 unx     1061 b- defN 24-Mar-15 02:43 mindformers/models/llama/__init__.py
+-r--------  2.0 unx     6999 b- defN 24-Mar-15 02:43 mindformers/models/llama/convert_weight.py
+-r--------  2.0 unx    25605 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama.py
+-r--------  2.0 unx    10886 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_config.py
+-r--------  2.0 unx    34676 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_interleave.py
+-r--------  2.0 unx    24857 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_layer.py
+-r--------  2.0 unx     3440 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_processor.py
+-r--------  2.0 unx    17102 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_tokenizer.py
+-r--------  2.0 unx     9034 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_tokenizer_fast.py
+-r--------  2.0 unx    30964 b- defN 24-Mar-15 02:43 mindformers/models/llama/llama_transformer.py
+-r--------  2.0 unx      878 b- defN 24-Mar-15 02:43 mindformers/models/mae/__init__.py
+-r--------  2.0 unx     3416 b- defN 24-Mar-15 02:43 mindformers/models/mae/convert_weight.py
+-r--------  2.0 unx    18082 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae.py
+-r--------  2.0 unx     7343 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_config.py
+-r--------  2.0 unx    36274 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_modules.py
+-r--------  2.0 unx     6194 b- defN 24-Mar-15 02:43 mindformers/models/mae/mae_processor.py
+-r--------  2.0 unx     1022 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/__init__.py
+-r--------  2.0 unx     5469 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/convert_weight.py
+-r--------  2.0 unx    28383 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha.py
+-r--------  2.0 unx     4372 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_config.py
+-r--------  2.0 unx     2394 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_processor.py
+-r--------  2.0 unx     8177 b- defN 24-Mar-15 02:43 mindformers/models/pangualpha/pangualpha_tokenizer.py
+-r--------  2.0 unx     1180 b- defN 24-Mar-15 02:43 mindformers/models/sam/__init__.py
+-r--------  2.0 unx     3489 b- defN 24-Mar-15 02:43 mindformers/models/sam/conver_weight.py
+-r--------  2.0 unx     5967 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam.py
+-r--------  2.0 unx     7758 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_config.py
+-r--------  2.0 unx    17459 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_image_encoder.py
+-r--------  2.0 unx     2944 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_layers.py
+-r--------  2.0 unx    22703 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_mask_decoder.py
+-r--------  2.0 unx     8556 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_processor.py
+-r--------  2.0 unx    10575 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_prompt_encoder.py
+-r--------  2.0 unx    22757 b- defN 24-Mar-15 02:43 mindformers/models/sam/sam_utils.py
+-r--------  2.0 unx      885 b- defN 24-Mar-15 02:43 mindformers/models/swin/__init__.py
+-r--------  2.0 unx     4949 b- defN 24-Mar-15 02:43 mindformers/models/swin/convert_weight.py
+-r--------  2.0 unx    16303 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin.py
+-r--------  2.0 unx     7783 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_config.py
+-r--------  2.0 unx    31158 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_modules.py
+-r--------  2.0 unx     5472 b- defN 24-Mar-15 02:43 mindformers/models/swin/swin_processor.py
+-r--------  2.0 unx     1192 b- defN 24-Mar-15 02:43 mindformers/models/t5/__init__.py
+-r--------  2.0 unx     8077 b- defN 24-Mar-15 02:43 mindformers/models/t5/convert_weight.py
+-r--------  2.0 unx    10964 b- defN 24-Mar-15 02:43 mindformers/models/t5/mt5.py
+-r--------  2.0 unx    90291 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5.py
+-r--------  2.0 unx    11544 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_config.py
+-r--------  2.0 unx     4164 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_processor.py
+-r--------  2.0 unx    20276 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_tokenizer.py
+-r--------  2.0 unx     9881 b- defN 24-Mar-15 02:43 mindformers/models/t5/t5_tokenizer_fast.py
+-r--------  2.0 unx      948 b- defN 24-Mar-15 02:43 mindformers/models/vit/__init__.py
+-r--------  2.0 unx     3364 b- defN 24-Mar-15 02:43 mindformers/models/vit/convert_weight.py
+-r--------  2.0 unx    13411 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit.py
+-r--------  2.0 unx     8231 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_config.py
+-r--------  2.0 unx    37548 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_modules.py
+-r--------  2.0 unx     4925 b- defN 24-Mar-15 02:43 mindformers/models/vit/vit_processor.py
+-r--------  2.0 unx      992 b- defN 24-Mar-15 02:43 mindformers/modules/__init__.py
+-r--------  2.0 unx    50360 b- defN 24-Mar-15 02:43 mindformers/modules/activation.py
+-r--------  2.0 unx    13356 b- defN 24-Mar-15 02:43 mindformers/modules/kvcache_mgr.py
+-r--------  2.0 unx    48060 b- defN 24-Mar-15 02:43 mindformers/modules/layers.py
+-r--------  2.0 unx    14414 b- defN 24-Mar-15 02:43 mindformers/modules/local_block_sparse_attention.py
+-r--------  2.0 unx     4545 b- defN 24-Mar-15 02:43 mindformers/modules/paged_attention_mgr.py
+-r--------  2.0 unx     1335 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/__init__.py
+-r--------  2.0 unx    39235 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/moe.py
+-r--------  2.0 unx     8481 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/op_parallel_config.py
+-r--------  2.0 unx   202706 b- defN 24-Mar-15 02:43 mindformers/modules/transformer/transformer.py
+-r--------  2.0 unx      897 b- defN 24-Mar-15 02:43 mindformers/pet/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Mar-15 02:43 mindformers/pet/constants.py
+-r--------  2.0 unx     4920 b- defN 24-Mar-15 02:43 mindformers/pet/pet_config.py
+-r--------  2.0 unx     3928 b- defN 24-Mar-15 02:43 mindformers/pet/pet_model.py
+-r--------  2.0 unx     1096 b- defN 24-Mar-15 02:43 mindformers/pet/utils.py
+-r--------  2.0 unx      696 b- defN 24-Mar-15 02:43 mindformers/pet/models/__init__.py
+-r--------  2.0 unx     3511 b- defN 24-Mar-15 02:43 mindformers/pet/models/lora.py
+-r--------  2.0 unx      955 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/ada_adapter.py
+-r--------  2.0 unx     1194 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/adalora_adapter.py
+-r--------  2.0 unx     5435 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/lora_adapter.py
+-r--------  2.0 unx     1649 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/pet_adapter.py
+-r--------  2.0 unx     1165 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/prefix_tuning_adapter.py
+-r--------  2.0 unx     1350 b- defN 24-Mar-15 02:43 mindformers/pet/tuners/ptuning2_adapter.py
+-r--------  2.0 unx     2160 b- defN 24-Mar-15 02:43 mindformers/pipeline/__init__.py
+-r--------  2.0 unx    10291 b- defN 24-Mar-15 02:43 mindformers/pipeline/base_pipeline.py
+-r--------  2.0 unx     2393 b- defN 24-Mar-15 02:43 mindformers/pipeline/build_pipeline.py
+-r--------  2.0 unx     6343 b- defN 24-Mar-15 02:43 mindformers/pipeline/fill_mask_pipeline.py
+-r--------  2.0 unx     7544 b- defN 24-Mar-15 02:43 mindformers/pipeline/image_classification_pipeline.py
+-r--------  2.0 unx     7168 b- defN 24-Mar-15 02:43 mindformers/pipeline/image_to_text_generation_pipeline.py
+-r--------  2.0 unx     6591 b- defN 24-Mar-15 02:43 mindformers/pipeline/masked_image_modeling_pipeline.py
+-r--------  2.0 unx     6226 b- defN 24-Mar-15 02:43 mindformers/pipeline/pipeline.py
+-r--------  2.0 unx    18559 b- defN 24-Mar-15 02:43 mindformers/pipeline/question_answering_pipeline.py
+-r--------  2.0 unx    27170 b- defN 24-Mar-15 02:43 mindformers/pipeline/segment_anything_pipeline.py
+-r--------  2.0 unx    10648 b- defN 24-Mar-15 02:43 mindformers/pipeline/text_classification_pipeline.py
+-r--------  2.0 unx    10971 b- defN 24-Mar-15 02:43 mindformers/pipeline/text_generation_pipeline.py
+-r--------  2.0 unx    10121 b- defN 24-Mar-15 02:43 mindformers/pipeline/token_classification_pipeline.py
+-r--------  2.0 unx     8167 b- defN 24-Mar-15 02:43 mindformers/pipeline/translation_pipeline.py
+-r--------  2.0 unx     9276 b- defN 24-Mar-15 02:43 mindformers/pipeline/zero_shot_image_classification_pipeline.py
+-r--------  2.0 unx     1127 b- defN 24-Mar-15 02:43 mindformers/tools/__init__.py
+-r--------  2.0 unx    10202 b- defN 24-Mar-15 02:43 mindformers/tools/check_rules.py
+-r--------  2.0 unx     4399 b- defN 24-Mar-15 02:43 mindformers/tools/download_tools.py
+-r--------  2.0 unx     6243 b- defN 24-Mar-15 02:43 mindformers/tools/download_tools_multithread.py
+-r--------  2.0 unx    12033 b- defN 24-Mar-15 02:43 mindformers/tools/export.py
+-r--------  2.0 unx     6520 b- defN 24-Mar-15 02:43 mindformers/tools/hccl_tools.py
+-r--------  2.0 unx     1925 b- defN 24-Mar-15 02:43 mindformers/tools/image_tools.py
+-r--------  2.0 unx    22670 b- defN 24-Mar-15 02:43 mindformers/tools/logger.py
+-r--------  2.0 unx     2316 b- defN 24-Mar-15 02:43 mindformers/tools/merge_hccl.py
+-r--------  2.0 unx     5598 b- defN 24-Mar-15 02:43 mindformers/tools/moe_token_distribution_tools.py
+-r--------  2.0 unx     3164 b- defN 24-Mar-15 02:43 mindformers/tools/transform_ckpt.py
+-r--------  2.0 unx    13716 b- defN 24-Mar-15 02:43 mindformers/tools/utils.py
+-r--------  2.0 unx      842 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/__init__.py
+-r--------  2.0 unx     8522 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/cloud_adapter.py
+-r--------  2.0 unx     3464 b- defN 24-Mar-15 02:43 mindformers/tools/cloud_adapter/cloud_monitor.py
+-r--------  2.0 unx      905 b- defN 24-Mar-15 02:43 mindformers/tools/register/__init__.py
+-r--------  2.0 unx    11037 b- defN 24-Mar-15 02:43 mindformers/tools/register/config.py
+-r--------  2.0 unx     7056 b- defN 24-Mar-15 02:43 mindformers/tools/register/register.py
+-r--------  2.0 unx     1982 b- defN 24-Mar-15 02:43 mindformers/trainer/__init__.py
+-r--------  2.0 unx    52479 b- defN 24-Mar-15 02:43 mindformers/trainer/base_trainer.py
+-r--------  2.0 unx     4428 b- defN 24-Mar-15 02:43 mindformers/trainer/build_trainer.py
+-r--------  2.0 unx    55851 b- defN 24-Mar-15 02:43 mindformers/trainer/config_args.py
+-r--------  2.0 unx     5925 b- defN 24-Mar-15 02:43 mindformers/trainer/optimizer_grouped_parameters.py
+-r--------  2.0 unx    51978 b- defN 24-Mar-15 02:43 mindformers/trainer/trainer.py
+-r--------  2.0 unx    15070 b- defN 24-Mar-15 02:43 mindformers/trainer/training_args.py
+-r--------  2.0 unx    35015 b- defN 24-Mar-15 02:43 mindformers/trainer/utils.py
+-r--------  2.0 unx      821 b- defN 24-Mar-15 02:43 mindformers/trainer/causal_language_modeling/__init__.py
+-r--------  2.0 unx    20146 b- defN 24-Mar-15 02:43 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
+-r--------  2.0 unx      866 b- defN 24-Mar-15 02:43 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
+-r--------  2.0 unx     4609 b- defN 24-Mar-15 02:43 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
+-r--------  2.0 unx      795 b- defN 24-Mar-15 02:43 mindformers/trainer/general_task_trainer/__init__.py
+-r--------  2.0 unx     9462 b- defN 24-Mar-15 02:43 mindformers/trainer/general_task_trainer/general_task_trainer.py
+-r--------  2.0 unx      928 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/__init__.py
+-r--------  2.0 unx     4622 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/group_ic_params.py
+-r--------  2.0 unx     9476 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/image_classification.py
+-r--------  2.0 unx     8209 b- defN 24-Mar-15 02:43 mindformers/trainer/image_classification/zero_shot_image_classification.py
+-r--------  2.0 unx      823 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_generation/__init__.py
+-r--------  2.0 unx     6107 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
+-r--------  2.0 unx      818 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/__init__.py
+-r--------  2.0 unx    12076 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/eval_utils.py
+-r--------  2.0 unx     7783 b- defN 24-Mar-15 02:43 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
+-r--------  2.0 unx      818 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/__init__.py
+-r--------  2.0 unx     2197 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
+-r--------  2.0 unx     7432 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
+-r--------  2.0 unx      830 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_language_modeling/__init__.py
+-r--------  2.0 unx     6859 b- defN 24-Mar-15 02:43 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
+-r--------  2.0 unx      799 b- defN 24-Mar-15 02:43 mindformers/trainer/question_answering/__init__.py
+-r--------  2.0 unx     9218 b- defN 24-Mar-15 02:43 mindformers/trainer/question_answering/question_answering.py
+-r--------  2.0 unx      803 b- defN 24-Mar-15 02:43 mindformers/trainer/text_classfication/__init__.py
+-r--------  2.0 unx     9386 b- defN 24-Mar-15 02:43 mindformers/trainer/text_classfication/text_classification.py
+-r--------  2.0 unx      807 b- defN 24-Mar-15 02:43 mindformers/trainer/token_classification/__init__.py
+-r--------  2.0 unx     9500 b- defN 24-Mar-15 02:43 mindformers/trainer/token_classification/token_classification.py
+-r--------  2.0 unx      795 b- defN 24-Mar-15 02:43 mindformers/trainer/translation/__init__.py
+-r--------  2.0 unx     6738 b- defN 24-Mar-15 02:43 mindformers/trainer/translation/translation_finetune.py
+-r--------  2.0 unx      913 b- defN 24-Mar-15 02:43 mindformers/wrapper/__init__.py
+-r--------  2.0 unx    14329 b- defN 24-Mar-15 02:43 mindformers/wrapper/adaptive_loss_scale.py
+-r--------  2.0 unx     4117 b- defN 24-Mar-15 02:43 mindformers/wrapper/build_wrapper.py
+-r--------  2.0 unx    12204 b- defN 24-Mar-15 02:43 mindformers/wrapper/wrapper.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    19262 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/WHEEL
+-r--------  2.0 unx       12 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    40604 b- defN 24-Mar-15 02:44 mindformers-1.0.1.dist-info/RECORD
+412 files, 4212794 bytes uncompressed, 1084717 bytes compressed:  74.3%
```

## zipnote {}

```diff
@@ -540,14 +540,17 @@
 
 Filename: mindformers/inference/infers/__init__.py
 Comment: 
 
 Filename: mindformers/inference/infers/base_infer.py
 Comment: 
 
+Filename: mindformers/inference/infers/cache_engine.py
+Comment: 
+
 Filename: mindformers/inference/infers/text_generator_infer.py
 Comment: 
 
 Filename: mindformers/models/__init__.py
 Comment: 
 
 Filename: mindformers/models/base_config.py
@@ -936,14 +939,17 @@
 
 Filename: mindformers/modules/layers.py
 Comment: 
 
 Filename: mindformers/modules/local_block_sparse_attention.py
 Comment: 
 
+Filename: mindformers/modules/paged_attention_mgr.py
+Comment: 
+
 Filename: mindformers/modules/transformer/__init__.py
 Comment: 
 
 Filename: mindformers/modules/transformer/moe.py
 Comment: 
 
 Filename: mindformers/modules/transformer/op_parallel_config.py
@@ -1209,23 +1215,23 @@
 
 Filename: mindformers/wrapper/build_wrapper.py
 Comment: 
 
 Filename: mindformers/wrapper/wrapper.py
 Comment: 
 
-Filename: mindformers-1.0.0.dist-info/LICENSE
+Filename: mindformers-1.0.1.dist-info/LICENSE
 Comment: 
 
-Filename: mindformers-1.0.0.dist-info/METADATA
+Filename: mindformers-1.0.1.dist-info/METADATA
 Comment: 
 
-Filename: mindformers-1.0.0.dist-info/WHEEL
+Filename: mindformers-1.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: mindformers-1.0.0.dist-info/top_level.txt
+Filename: mindformers-1.0.1.dist-info/top_level.txt
 Comment: 
 
-Filename: mindformers-1.0.0.dist-info/RECORD
+Filename: mindformers-1.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## configs/README.md

```diff
@@ -130,14 +130,17 @@
           *\# 以下配置针对大规模语言模型推理*
         - top_k: 从概率最大的top_k个tokens中采样
         - top_p: 从概率最大且概率累计不超过top_p的tokens中采样
         - do_sample: 使能top_k或top_p采样，为False时top_k和top_p均重置为1
         - use_past: 使能增量推理，为True时为增量推理，否则为自回归推理，使用时请参考[模型支持列表](https://gitee.com/mindspore/mindformers/tree/dev/docs#text-generator)
         - max_decode_length: 文本生成最大长度（输入长度统计在内）
         - repetition_penalty: 重复文本惩罚系数，该值不小于1，等于1时不惩罚
+        - use_paged_attention: 是否开启Paged Attention推理，当前仅支持MS Lite推理时使用
+        - pa_block_size: 使用Paged Attention推理时需设置，每块block的大小
+        - pa_num_blocks: 使用Paged Attention推理时需设置，blocks的总数
 - lr_schedule: 学习率配置
     - type: 学习率类
 - layer_scale: 是否开启层衰减
 - layer_decay: 层衰减系数
 - optimizer: 优化器配置
     - type: 优化器类
     - weight_decay: 权重衰减值
```

## configs/codegeex2/run_codegeex2_6b_eval.yaml

```diff
@@ -1,42 +1,42 @@
 seed: 0
-run_mode: 'train'
+run_mode: 'eval'
 output_dir: './output' # path to save checkpoint/strategy
 load_checkpoint: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
 
 # ==== context config ====
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
   graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
   max_call_depth: 10000
   max_device_memory: "59GB"
-  save_graphs: True
+  save_graphs: False
   device_id: 0
 
 # aicc
 remote_save_url: "Please input obs url on AICC platform."
 
 # ==== model config ====
 model:
   model_config:
     type: ChatGLM2Config
-    batch_size: 4   # only for incremental infer
+    batch_size: 8   # only for incremental infer
     num_layers: 28
     padded_vocab_size: 65024
     hidden_size: 4096
     interleaved_qkv: False
     ffn_hidden_size: 13696
     kv_channels: 128
     num_attention_heads: 32
-    seq_length: 2048
+    seq_length: 1024
     hidden_dropout: 0.0
     attention_dropout: 0.0
     layernorm_epsilon: 1e-5
     rmsnorm: True
     apply_residual_connection_post_layernorm: False
     post_layer_norm: True
     add_bias_linear: False
@@ -89,71 +89,71 @@
     unk_token: '<unk>'
   type: GLMProcessor
 
 # ==== dataset config ====
 train_dataset: &train_dataset
   data_loader:
     type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/train.json"
+    dataset_dir: "/path/to/CodeAlpaca/train.json"
     shuffle: False
     phase: "train"
     version: 2
     origin_columns: ["PROMPT", "ANSWER"]
   tokenizer:
     type: ChatGLM2Tokenizer
     vocab_file: "/path/to/tokenizer.model"
   input_columns: ["input_ids", "labels"]
   max_source_length: 1023
   max_target_length: 1024
   ignore_pad_token_for_loss: True
   num_parallel_workers: 8
   python_multiprocessing: False
   drop_remainder: True
-  batch_size: 4
+  batch_size: 8
   repeat: 1
   numa_enable: False
   prefetch_size: 1
   seed: 0
 
 train_dataset_task:
   type: KeyWordGenDataset
   dataset_config: *train_dataset
 
 eval_dataset: &eval_dataset
   data_loader:
     type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/dev.json"
+    dataset_dir: "/path/to/CodeAlpaca/dev.json"
     shuffle: False
     phase: "eval"
     version: 2
     origin_columns: ["PROMPT", "ANSWER"]
   tokenizer:
     type: ChatGLM2Tokenizer
     vocab_file: "/path/to/tokenizer.model"
-  max_source_length: 2048
-  max_target_length: 2048
+  max_source_length: 1024
+  max_target_length: 1024
   ignore_pad_token_for_loss: True
   input_columns: ["input_ids", "labels"]
   num_parallel_workers: 8
   python_multiprocessing: False
   drop_remainder: True
-  batch_size: 4
+  batch_size: 8
   repeat: 1
   numa_enable: False
   prefetch_size: 1
   seed: 0
 
 eval_dataset_task:
   type: KeyWordGenDataset
   dataset_config: *eval_dataset
 
 # ==== runner config ====
 runner_config:
   epochs: 1
-  batch_size: 4
+  batch_size: 8
   sink_mode: True
   sink_size: 1
 
 runner_wrapper:
   type: MFTrainOneStepCell
   scale_sense:
     type: DynamicLossScaleUpdateCell
```

## configs/codellama/predict_codellama_34b_910b.yaml

```diff
@@ -27,15 +27,15 @@
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
+  enable_parallel_optimizer: False
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
 # default parallel of device num = 8 for Atlas 800T A2
 parallel_config:
   data_parallel: 1
@@ -95,14 +95,20 @@
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
+    block_size: 16
+    num_blocks: 512
+    is_dynamic: False
+    use_kvcache_op: False
+    is_flexible_shape: False
     offset: 0
     use_rope_slice: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
```

## configs/llama2/export_llama2_13b.yaml

```diff
@@ -19,15 +19,15 @@
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
+  enable_parallel_optimizer: False
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
 # default parallel of device num = 16 for Atlas 800T A2
 parallel_config:
   data_parallel: 8
@@ -75,14 +75,20 @@
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
     pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
     compute_in_2d: True
     use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
+    block_size: 16
+    num_blocks: 512
+    is_dynamic: False
+    use_kvcache_op: False
+    is_flexible_shape: False
     offset: 0
     use_past_shard: False
     checkpoint_name_or_path: "{path}/llama2_13b.ckpt" # 导出任务这里必填
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
```

## configs/llama2/export_llama2_7b.yaml

```diff
@@ -16,15 +16,15 @@
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
+  enable_parallel_optimizer: False
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
 # default parallel of device num = 8 for Atlas 800T A2
 parallel_config:
   data_parallel: 8
@@ -72,14 +72,20 @@
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
     pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
     compute_in_2d: True
     use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
+    block_size: 16
+    num_blocks: 512
+    is_dynamic: False
+    use_kvcache_op: False
+    is_flexible_shape: False
     offset: 0
     use_past_shard: False
     checkpoint_name_or_path: "{path}/llama2_7b.ckpt" # 导出任务这里必填
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
```

## configs/llama2/predict_llama2_70b_910b.yaml

```diff
@@ -28,15 +28,15 @@
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
+  enable_parallel_optimizer: False
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
 # default parallel of device num = 32 for Atlas 800T A2
 parallel_config:
   data_parallel: 1
@@ -97,14 +97,20 @@
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
+    block_size: 16
+    num_blocks: 512
+    is_dynamic: False
+    use_kvcache_op: False
+    is_flexible_shape: False
     offset: 0
     use_rope_slice: False
     checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
```

## mindformers/.commit_id

```diff
@@ -1,8 +1,8 @@
 r1.0
-commit 7f9b584
-Merge: 14b815f 4ceb21a
-Author: Lin <heqinglin4@huawei.com>
-Date:   Mon Jan 22 10:47:26 2024 +0000
+commit c9b9436
+Merge: 8bb203e c5b84a8
+Author: i-robot <huawei_ci_bot@163.com>
+Date:   Thu Mar 14 11:29:13 2024 +0000
 
-    !2175 回退 'Pull Request !2138 : 【r1.0】910b默认配置infnan mode同步提交'
-    Merge pull request !2175 from huanglei/revert-merge-2138-r1.0
+    !2428 【r1.0】【bugfix】边训练边评估流程中eval_network使用_virtual_dataset封装
+    Merge pull request !2428 from huanglei/virtual_ds
```

## mindformers/version_control.py

```diff
@@ -211,7 +211,38 @@
     if not version_valid:
         logger.warning("Current MindSpore do not support big kernel SiLU and RMSNorm, "
                        "please upgrade to 2.2.10 or later version.")
         result = False
     else:
         result = True
     return result
+
+def is_version_python(cur_ver, tar_ver):
+    """
+        return cur_ver >= tar_ver.
+        Check whether the current version is higher than or equal to the base version.
+        for cur_ver: 3.7.10, tar_ver: 3.9.0, it return False.
+        you can get python cur_ver through:
+            cur_py_ver = sys.version.split(' ')[0]
+    """
+    version_split_char = '.'
+    if version_split_char not in tar_ver or version_split_char not in cur_ver:
+        raise ValueError("The version string will contain the `.`."
+                         "For example, cur_ver: 3.7.10， tar_ver: 3.9.0")
+    for x, y in zip(cur_ver.split(version_split_char), tar_ver.split(version_split_char)):
+        if not x.isdigit() or not y.isdigit():
+            continue
+        if int(x) != int(y):
+            return int(x) >= int(y)
+    return True
+
+def check_valid_paged_attention():
+    """check mindspore version is valid for paged attention"""
+    version_valid = is_version_ge(ms.__version__, "2.2.12")
+    # below ms 2.2.12 is not support
+    if not version_valid:
+        logger.warning("Current MindSpore do not support PagedAttention, please upgrade to 2.2.12 or later version.")
+        logger.warning("Now running on self-attention mode.")
+        result = False
+    else:
+        result = True
+    return result
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## mindformers/core/callback/callback.py

```diff
@@ -316,29 +316,29 @@
                             int(per_step_seconds), current_lr, overflow, scaling_sens)
             else:
                 logger.info("{ Epoch:[%3d/%3d], step:[%5d/%5d], loss:[%5.3f/%5.3f], "
                             "per_step_time: %dms, lr: %s, overflow cond: %s, loss_scale: %s",
                             cur_epoch_num, origin_epochs, cur_step_num, steps_per_epoch, loss, np.mean(self.loss_list),
                             int(per_step_seconds), current_lr, overflow, scaling_sens)
             show_str = ('|%%-%ds|' % 50) % (int(50 * percent / 100) * "█")
-            logger.info("  %4.1f%% %s %.2f samples/s/p  %s }", percent, show_str, throughput,
+            logger.info("  %4.1f%% %s %.5f samples/s/p  %s }", percent, show_str, throughput,
                         datetime.timedelta(seconds=int(time_remain)))
         else:
             if cb_params.dataset_sink_mode:
                 logger.info("{ Epoch:[%3d/%3d], step:[%5d/%5d], loss: %5.3f, "
                             "per_step_time: %dms, overflow cond: %s, loss_scale: %s",
                             cur_epoch_num, origin_epochs, cur_step_num, steps_per_epoch, loss,
                             int(per_step_seconds), overflow, scaling_sens)
             else:
                 logger.info("{ Epoch:[%3d/%3d], step:[%5d/%5d], loss:[%5.3f/%5.3f], "
                             "per_step_time: %dms, overflow cond: %s, loss_scale: %s",
                             cur_epoch_num, origin_epochs, cur_step_num, steps_per_epoch, loss, np.mean(self.loss_list),
                             int(per_step_seconds), overflow, scaling_sens)
             show_str = ('|%%-%ds|' % 50) % (int(50 * percent / 100) * "█")
-            logger.info("  %4.1f%% %s %.2f samples/s/p  %s }", percent, show_str, throughput,
+            logger.info("  %4.1f%% %s %.5f samples/s/p  %s }", percent, show_str, throughput,
                         datetime.timedelta(seconds=int(time_remain)))
 
     def dump_info_to_modelarts(self, ma_step_num, ma_loss):
         """dump modelarts info to display evaluation result page"""
         ma_loss = float(ma_loss)
         obj = None
         modelarts_dir = os.path.join(get_output_root_path(), "modelarts")
@@ -579,15 +579,15 @@
             self._last_time_for_keep = time.time()
             self._last_triggered_step = cb_params.cur_step_num
 
             if context.get_context("enable_ge") and os.getenv("MS_ENABLE_REF_MODE", "0") == "0":
                 set_cur_net(cb_params.train_network)
                 cb_params.train_network.exec_checkpoint_graph()
             if "epoch_num" in self._append_dict:
-                self._append_dict["epoch_num"] = self._append_epoch_num + cb_params.cur_epoch_num
+                self._append_dict["epoch_num"] = cb_params.cur_epoch_num
             if "step_num" in self._append_dict:
                 self._append_dict["step_num"] = self._append_step_num + cb_params.cur_step_num
             if cb_params.optimizer is not None:
                 self._append_dict["global_step"] = cb_params.optimizer.global_step
             else:
                 self._append_dict["global_step"] = cb_params.network.optimizer.global_step
             if "loss_scale" in self._append_dict:
```

## mindformers/inference/infer_config.py

```diff
@@ -45,19 +45,25 @@
                  model_name: str = "common",
                  infer_seq_length: int = 1024,
                  target: str = "Ascend",
                  device_id: int = 0,
                  rank_id: int = 0,
                  ge_config_path: str = "",
                  dynamic: bool = False,
+                 paged_attention: bool = False,
+                 pa_block_size: int = 16,
+                 pa_num_blocks: int = 512,
                  **kwargs):
         super(InferConfig, self).__init__(**kwargs)
         self.prefill_model_path = prefill_model_path
         self.increment_model_path = increment_model_path
         self.model_type = convert_lite_model_type(model_type)
         self.model_name = model_name
         self.seq_length = infer_seq_length
         self.target = target
         self.device_id = device_id
         self.rank_id = rank_id
         self.config_path = ge_config_path
         self.dynamic = dynamic
+        self.paged_attention = paged_attention
+        self.block_size = pa_block_size
+        self.num_blocks = pa_num_blocks
```

## mindformers/inference/infers/base_infer.py

```diff
@@ -126,30 +126,30 @@
         if not res:
             logger.warning("ge config file %s has no item named ge.dynamicDims", ge_config_path)
             return None
         return res
 
 
 class BaseInfer(metaclass=abc.ABCMeta):
-    """
-    BaseInfer.
-    """
-
+    """BaseInfer."""
     def __init__(self,
                  config: InferConfig = None,
                  tokenizer: Optional[BaseTokenizer] = None,
                  image_processor: Optional[BaseImageProcessor] = None):
         if config is None:
             config = InferConfig()
         self.config = config
         self.model_type = config.model_type
         self.model_name = config.model_name
         self.seq_length = config.seq_length
         self.config_path = config.config_path
         self.dynamic = config.dynamic
+        self.paged_attention = config.paged_attention
+        self.block_size = config.block_size
+        self.num_blocks = config.num_blocks
 
         self.context = build_context(config=self.config)
         self.tokenizer = tokenizer
         self.image_processor = image_processor
         self.full_model = None
         self.cache_model = None
         if config.prefill_model_path and config.increment_model_path:
@@ -164,25 +164,22 @@
             )
         else:
             self.full_model = self._load_model(config.prefill_model_path)
         if self.dynamic:
             self.dynshape_gears = DynShapeGear(self.increment_config)
 
     def _load_model(self, model_path):
-        """ load single model from model path.
-        """
+        """ load single model from model path."""
         model = Model()
         model.build_from_file(model_path, model_type=self.model_type,
                               context=self.context, config_path=self.config_path)
         return model
 
     def _load_increment_models(self, full_model_path, cache_model_path, prefill_config, increment_config):
-        """
-        load kv cache models.
-        """
+        """load kv cache models."""
         full_model = Model()
         cache_model = Model()
 
         model_group = ModelGroup(ModelGroupFlag.SHARE_WEIGHT)
         model_group.add_model([full_model, cache_model])
 
         full_model.build_from_file(full_model_path, self.model_type, self.context, prefill_config)
```

## mindformers/inference/infers/text_generator_infer.py

```diff
@@ -19,22 +19,24 @@
 from typing import Union, List, Optional
 
 import numpy as np
 import mindspore_lite as mslite
 from mindspore_lite import Model
 
 from mindformers.tools.logger import logger
-from mindformers.models import BaseTokenizer
+from mindformers.models import BaseTokenizer, BaseImageProcessor
 from mindformers.generation import GenerationConfig, LogitsProcessorList
 from mindformers.generation.logits_process import RepetitionPenaltyLogitsProcessor, LogitNormalization, \
     TemperatureLogitsWarper, TopKLogitsWarper, TopPLogitsWarper
 from mindformers.generation.streamers import BaseStreamer
 from mindformers.generation.utils import softmax_with_threads
+from mindformers.inference.infer_config import InferConfig
 
 from .base_infer import BaseInfer
+from .cache_engine import BlockMemPool, CacheEngine
 
 
 class BaseInputsOfInfer:
     """
     BaseInputsOfInfer interface.
     """
     @abc.abstractmethod
@@ -75,22 +77,28 @@
 class LlamaInputsOfInfer(BaseInputsOfInfer):
     """
     common infer inputs of llm models.
     """
     # pylint: disable=W0613, W0221
     def get_inputs(self, model: Model, input_ids=None, current_index=None, valid_length=None,
                    init_reset=None, is_first_iteration=True, **kwargs):
-        if not is_first_iteration:
+        if is_first_iteration:
+            inputs = [input_ids, valid_length, kwargs['batch_index'], kwargs['zactivate_len']]
+            if kwargs.get("slot_mapping") is not None:
+                inputs = inputs[:-2] + [kwargs["slot_mapping"]]
+        else:
             inputs_tmp = []
             for i in range(len(valid_length)):
                 current_index_tmp = valid_length[i] - 1  # multibatch
                 # use numpy to slice array to avoid complie ascend slice op
                 inputs_tmp.append(input_ids[i][current_index_tmp:current_index_tmp + 1])
             input_ids = np.array(inputs_tmp, dtype=np.int32)
-        inputs = [input_ids, valid_length, kwargs['batch_index'], kwargs['zactivate_len']]
+            inputs = [input_ids, valid_length, kwargs['batch_index'], kwargs['zactivate_len']]
+            if kwargs.get("slot_mapping") is not None:
+                inputs = inputs[:-2] + [kwargs["block_tables"], kwargs["slot_mapping"]]
         lite_inputs = self.get_lite_tensor_list(inputs)
         return lite_inputs
 
     # pylint: disable=W0221
     def get_lite_tensor_list(self, inputs):
         input_tensors = []
         for item in inputs:
@@ -190,14 +198,15 @@
         "glm2": CommonInputsOfInfer,
         "glm3": CommonInputsOfInfer,
         "gpt2": CommonInputsOfInfer,
         "codegeex2": CommonInputsOfInfer,
         "glm": GLMInputsOfInfer,
         "baichuan2": LlamaInputsOfInfer,
         "internlm": LlamaInputsOfInfer,
+        "qwen": LlamaInputsOfInfer,
         "common": CommonInputsOfInfer
     }
 
     @classmethod
     def get_inputs(cls, model_name: str, model, **kwargs):
         """
         Get input tensor list of mslite.
@@ -223,14 +232,24 @@
         return InputOfInfer.MAPPING[name]().get_inputs(model, **kwargs)
 
 
 class TextGeneratorInfer(BaseInfer):
     """
     Text generator infer implement class.
     """
+    def __init__(self,
+                 config: InferConfig = None,
+                 tokenizer: Optional[BaseTokenizer] = None,
+                 image_processor: Optional[BaseImageProcessor] = None):
+        super(TextGeneratorInfer, self).__init__(config, tokenizer, image_processor)
+        if self.paged_attention:
+            self.max_num_blocks_per_seq = self.seq_length // self.block_size
+            self.block_mem_pool = BlockMemPool(self.num_blocks, self.block_size)
+            self.cache_engines = []
+
     # pylint: disable=W0221
     def infer(self,
               inputs: Union[str, List[str]],
               do_sample: bool = False,
               top_k: int = 1,
               top_p: float = 1.0,
               temperature: float = 1.0,
@@ -269,19 +288,30 @@
             is_sample_acceleration: The postprocess are processing in model. Default False.
             add_special_tokens: Add special tokens for preprocess.
             streamer: The streamer that generator uses.
 
         Returns:
             outputs of model infer
         """
+        if self.paged_attention:
+            for _ in range(len(inputs)):
+                self.cache_engines.append(CacheEngine(self.block_size, self.block_mem_pool))
+            logger.info("Initialize cache engines.")
+
         input_ids = self.preprocess(inputs, add_special_tokens)
         output_ids = self.generate(input_ids, do_sample, top_k, top_p, temperature,
                                    repetition_penalty, eos_token_id, pad_token_id,
                                    max_length, is_sample_acceleration, streamer, **kwargs)
         outputs = self.postprocess(output_ids)
+
+        if self.paged_attention:
+            for cache_engine in self.cache_engines:
+                cache_engine.release_cache()
+            self.cache_engines = []
+            logger.info("Clear cache engines.")
         return outputs
 
     # pylint: disable=W0613
     def preprocess(self, input_data, add_special_tokens=False, **kwargs):
         """preprocess."""
         if self.model_name.startswith('glm3'):
            return self.tokenizer.build_batch_input(input_data)["input_ids"]
@@ -358,14 +388,66 @@
         if generation_config.top_p is not None and generation_config.top_p < 1.0:
             warpers.append(TopPLogitsWarper(top_p=generation_config.top_p, min_tokens_to_keep=min_tokens_to_keep))
         # `LogitNormalization` should always be the last logit processor, when present
         if generation_config.renormalize_logits is True:
             warpers.append(LogitNormalization())
         return warpers
 
+    def _assemble_pa_inputs(self, is_first_iteration, batch_valid_length: np.array, is_finished: List[bool]):
+        if is_first_iteration:
+            return self._assemble_pa_full_inputs(batch_valid_length, is_finished)
+        return self._assemble_pa_inc_inputs(batch_valid_length, is_finished)
+
+    def _assemble_pa_full_inputs(self, batch_valid_length: np.array, is_finished: List[bool]):
+        """Prepare prefill inputs for Paged Attention."""
+        bs = batch_valid_length.shape[0]
+
+        block_tables = []
+        slot_mapping = []
+        for i in range(bs):
+            if not is_finished[i]:
+                self.cache_engines[i].prepare_cache(batch_valid_length[i] + self.block_size)
+
+            null_block_id = self.cache_engines[i].block_table[0]
+            block_table = self.cache_engines[i].block_table[1:]
+            padded_table = block_table + [-1 for _ in range(
+                self.max_num_blocks_per_seq - len(self.cache_engines[i].block_table) + 1)]
+            block_tables.append(padded_table)
+
+            slots = [block_table[k // self.block_size] * self.block_size + k % self.block_size 
+                     for k in range(batch_valid_length[i])]
+            null_slot_idx = null_block_id * self.block_size + null_block_id % self.block_size
+            slots = slots + [null_slot_idx for _ in range(self.seq_length - batch_valid_length[i])]
+            slot_mapping = slot_mapping + slots
+        block_tables = np.array(block_tables, dtype=np.int32)
+        slot_mapping = np.array(slot_mapping, dtype=np.int32)
+        return block_tables, slot_mapping
+
+    def _assemble_pa_inc_inputs(self, batch_valid_length: np.array, is_finished: List[bool]):
+        """Prepare incremental inputs for Paged Attention."""
+        bs = batch_valid_length.shape[0]
+
+        block_tables = []
+        slot_mapping = []
+        for i in range(bs):
+            if not is_finished[i]:
+                self.cache_engines[i].prepare_cache(1)
+
+            block_table = self.cache_engines[i].block_table[1:]
+            padded_table = block_table + [-1 for _ in range(
+                self.max_num_blocks_per_seq - len(self.cache_engines[i].block_table) + 1)]
+            block_tables.append(padded_table)
+
+            curent_idx = batch_valid_length[i] - 1
+            slots = [block_table[curent_idx // self.block_size] * self.block_size + curent_idx % self.block_size]
+            slot_mapping = slot_mapping + slots
+        block_tables = np.array(block_tables, dtype=np.int32)
+        slot_mapping = np.array(slot_mapping, dtype=np.int32)
+        return block_tables, slot_mapping
+
     def generate(self, input_ids, do_sample, top_k, top_p, temperature, repetition_penalty, eos_token_id,
                  pad_token_id, max_length, is_sample_acceleration, streamer, **kwargs):
         """token generator."""
         total_time = time.time()
         sampler_dict = {"do_sample": do_sample, "top_k": top_k, "top_p": top_p, "temperature": temperature,
                         "repetition_penalty": repetition_penalty, "max_length": max_length, **kwargs}
         generation_config = GenerationConfig(**sampler_dict)
@@ -389,15 +471,15 @@
             # As the nonzero returns the index and we need length
             valid_length.append(np.max(np.argwhere(np.array(input_ids[i]) != pad_token_id)) + 1)
         valid_length = np.array(valid_length, np.int32)
 
         if self.dynamic:
             pad_length = max_length - valid_length
             real_pad_length = max(valid_length) - valid_length
-            target_length = kwargs["max_new_tokens"] + max(valid_length) if kwargs.get(
+            target_length = min(kwargs["max_new_tokens"] + max(valid_length), max_length) if kwargs.get(
                 "max_new_tokens") else max_length
         else:
             target_length = self.seq_length if max_length > self.seq_length else max_length
             # pad original input ids to seq_length
             pad_length = self.seq_length - valid_length
         pad_input_ids = np.array([
             np.pad(input_ids[i], (0, pad_length[i]),
@@ -426,20 +508,27 @@
         while np.sum(is_finished) != batch_size:
             start_time = time.time()
             seq_length = input_ids.shape[1]
             current_index = [valid_length[i] - 1 + i * seq_length for i in range(batch_size)]
             current_index = np.array(current_index, np.int32)
             logger.debug("validate length: %s", valid_length)
 
+            if self.paged_attention:
+                block_tables, slot_mapping = self._assemble_pa_inputs(is_first_iteration, valid_length, is_finished)
+            else:
+                block_tables, slot_mapping = None, None
+            
             if use_past:
                 outputs = self._inc_infer(input_ids, current_index, valid_length, is_first_iteration,
-                                          batch_index=batch_index, zactivate_len=activate_len, **kwargs)
+                                          batch_index=batch_index, zactivate_len=activate_len,
+                                          block_tables=block_tables, slot_mapping=slot_mapping, **kwargs)
             else:
                 outputs = self._full_infer(input_ids, current_index, is_sample_acceleration,
-                                           batch_index=batch_index, zactivate_len=activate_len, **kwargs)
+                                           batch_index=batch_index, zactivate_len=activate_len,
+                                           block_tables=block_tables, slot_mapping=slot_mapping, **kwargs)
 
             if self.dynamic:
                 input_ids = pad_input_ids
 
             if not is_sample_acceleration:
                 logits = outputs[0].get_data_to_numpy()
                 vocab_size = logits.shape[-1]
@@ -509,38 +598,45 @@
                     total_time, generate_len, generate_len / total_time)
 
         if streamer:
             streamer.end()
 
         return output_ids
 
-    def _inc_infer(self, input_ids, current_index, valid_length, is_first_iteration, **kwargs):
+    def _inc_infer(self, input_ids, current_index, valid_length, is_first_iteration,
+                   block_tables, slot_mapping, **kwargs):
         """kvcache infer"""
         if is_first_iteration:
             init_reset = np.array([False])
             lite_inputs = self.get_predict_inputs(self.full_model, input_ids, current_index,
-                                                  valid_length, init_reset, is_first_iteration, **kwargs)
+                                                  valid_length, init_reset, is_first_iteration,
+                                                  block_tables, slot_mapping, **kwargs)
             outputs = self.full_model.predict(lite_inputs)
         else:
             init_reset = np.array([True])
             lite_inputs = self.get_predict_inputs(self.cache_model, input_ids, current_index,
-                                                  valid_length, init_reset, is_first_iteration, **kwargs)
+                                                  valid_length, init_reset, is_first_iteration,
+                                                  block_tables, slot_mapping, **kwargs)
             outputs = self.cache_model.predict(lite_inputs)
         return outputs
 
-    def _full_infer(self, input_ids, current_index, is_npu_acceleration, **kwargs):
+    def _full_infer(self, input_ids, current_index, is_npu_acceleration, block_tables, slot_mapping, **kwargs):
         """infer"""
         # get inputs
         if is_npu_acceleration:
-            lite_inputs = self.get_predict_inputs(self.full_model, input_ids, current_index, **kwargs)
+            lite_inputs = self.get_predict_inputs(self.full_model, input_ids, current_index,
+                                                  block_tables, slot_mapping, **kwargs)
         else:
-            lite_inputs = self.get_predict_inputs(self.full_model, input_ids, **kwargs)
+            lite_inputs = self.get_predict_inputs(self.full_model, input_ids,
+                                                  block_tables, slot_mapping, **kwargs)
         # do infer
         outputs = self.full_model.predict(lite_inputs)
         return outputs
 
     def get_predict_inputs(self, mode: Model, input_ids, current_index=None,
-                           valid_length=None, init_reset=None, is_first_iteration=True, **kwargs):
+                           valid_length=None, init_reset=None, is_first_iteration=True,
+                           block_tables=None, slot_mapping=None, **kwargs):
         """Get inputs of llm model for mslite."""
         return InputOfInfer.get_inputs(self.model_name, mode, input_ids=input_ids, current_index=current_index,
                                        valid_length=valid_length, init_reset=init_reset, tokenizer=self.tokenizer,
-                                       is_first_iteration=is_first_iteration, **kwargs)
+                                       is_first_iteration=is_first_iteration, block_tables=block_tables,
+                                       slot_mapping=slot_mapping, **kwargs)
```

## mindformers/models/llama/llama.py

```diff
@@ -36,14 +36,15 @@
 from mindformers.core.loss.loss import CrossEntropyLoss
 from mindformers.mindformer_book import MindFormerBook
 from mindformers.models.base_model import BaseModel
 from mindformers.modules.layers import Linear
 from mindformers.modules.transformer.op_parallel_config import _check_config
 from mindformers.modules.transformer.transformer import LowerTriangularMaskWithDynamic
 from mindformers.tools.register.register import MindFormerModuleType, MindFormerRegister
+from mindformers.version_control import check_valid_paged_attention
 
 from .llama_config import LlamaConfig
 from .llama_layer import LlamaEmbedding, LlamaRMSNorm, FreqsMgr
 from .llama_transformer import LLamaDecodeLayer
 from ...modules import KVCachePreprocess
 from .llama_interleave import LLamaDecodeLayerInterleave
 from ..utils import cell_reuse
@@ -127,14 +128,17 @@
         self.use_kvcache_op = config.use_kvcache_op
         self.is_flexible_shape = config.is_flexible_shape
         self.use_flash_attention = config.use_flash_attention and FLASHATTENTION_VALID
         if self.use_flash_attention:
             logger.info("Enable flash attention.")
         elif config.use_flash_attention:
             logger.info("Current MindSpore do not support flash attention.")
+        self.use_paged_attention = config.use_paged_attention and check_valid_paged_attention()
+        if self.use_paged_attention:
+            logger.info("Enable paged attention.")
 
         self.shape = P.Shape()
         self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
         self.cast = P.Cast()
         self.tile = P.Tile()
         self.expand_dims = P.ExpandDims()
         self.gather = P.Gather()
@@ -196,47 +200,55 @@
                                          compute_dtype=config.compute_dtype,
                                          layernorm_compute_dtype=config.layernorm_compute_type,
                                          softmax_compute_dtype=config.softmax_compute_type,
                                          rotary_dtype=config.rotary_dtype,
                                          param_init_type=config.param_init_type,
                                          use_past=config.use_past,
                                          use_flash_attention=config.use_flash_attention,
+                                         use_paged_attention=self.use_paged_attention,
+                                         block_size=config.block_size,
+                                         num_blocks=config.num_blocks,
                                          is_dynamic=config.is_dynamic,
                                          use_kvcache_op=config.use_kvcache_op,
                                          is_flexible_shape=config.is_flexible_shape,
                                          use_rope_slice=config.use_rope_slice,
                                          parallel_config=config.parallel_config)
             layer_compute_dtype(layer, layer_id, config.offset, config.parallel_config,
                                 config.num_layers, select_recompute=config.parallel_config.recompute.select_recompute)
             self.layers.append(layer)
         self.norm_out = LlamaRMSNorm(config.hidden_size, config.rms_norm_eps,
                                      compute_type=config.layernorm_compute_type, is_dynamic=config.is_dynamic)
         self.kvcache_preprocess = KVCachePreprocess(max_batch_size=config.batch_size,
                                                     max_seq_length=config.seq_length,
                                                     is_dynamic=config.is_dynamic,
                                                     use_kvcache_op=config.use_kvcache_op,
-                                                    is_flexible_shape=config.is_flexible_shape)
+                                                    is_flexible_shape=config.is_flexible_shape,
+                                                    use_paged_attention=self.use_paged_attention)
 
         dp = config.parallel_config.data_parallel
         if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
             self.tok_embeddings.pipeline_stage = 0
             if config.parallel_config.pipeline_stage > 1:
                 self.norm_out.pipeline_stage = config.parallel_config.pipeline_stage - 1
                 self.tok_embeddings.set_comm_fusion(2)
                 self.norm_out.set_comm_fusion(2)
             else:
                 self.tok_embeddings.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
                 self.norm_out.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
             self.tok_embeddings.shard(config.parallel_config)
             self.casual_mask.shard(config.parallel_config)
-            self.norm_out.shard((dp, 1, 1))
+            if config.fine_grain_interleave > 1 and config.parallel_config.model_parallel > 1:
+                self.norm_out.shard((dp, 1))
+            else:
+                self.norm_out.shard((dp, 1, 1))
 
     # pylint: disable=W0613
-    def construct(self, tokens: Tensor, batch_valid_length=None, batch_index=None, zactivate_len=None):
+    def construct(self, tokens: Tensor, batch_valid_length=None, batch_index=None, zactivate_len=None,
+                  block_tables=None, slot_mapping=None):
         """
         Forward of llama model.
 
         Args:
             tokens: the tokenized inputs with datatype int32
             input_position(Tensor): current position, used by model.predict.
             init_reset(bool, optional): A bool tensor with shape [1], used to clear the past key parameter and
@@ -265,15 +277,16 @@
                         self.kvcache_preprocess.range,
                         self.kvcache_preprocess.max_cache_length // bs, batch_valid_length,
                         zactivate_len)
                 else:
                     mask = self.casual_mask.increment(self.kvcache_preprocess.range, batch_valid_length, zactivate_len)
             mask = self.casual_mask.post_process(mask)
 
-            kvcache_inputs = self.kvcache_preprocess(bs, batch_valid_length, batch_index, zactivate_len)
+            kvcache_inputs = self.kvcache_preprocess(bs, batch_valid_length, batch_index, zactivate_len,
+                                                     block_tables, slot_mapping)
 
         # tokens: [bs, seq/1]
         h = self.tok_embeddings(tokens)
         h = self.reshape(h, (bs, seq_len, self.hidden_size))
         # h: [bs, seq/1, hidden_dim]
         for i in range(self.num_layers):
             h = self.layers[i](h, freqs_cis, mask, kvcache_inputs=kvcache_inputs)
@@ -369,39 +382,53 @@
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         return {
             "input_ids": Tensor(input_ids, mstype.int32)
         }
 
     def prepare_inputs_for_export(self, full_model=True):
         dyn = self.config.is_dynamic
+        use_paged_attention = self.config.use_paged_attention and check_valid_paged_attention()
         if dyn:
             logger.info(f"Exporting dynamic MindIR...")
+        if use_paged_attention:
+            logger.info(f"Exporting model with paged attention...")
         seq_length = self.seq_length
         bs = None if dyn else self.config.batch_size
         seq_len = None if dyn else self.seq_length
+        max_num_blocks_pre_batch = None if dyn else seq_len // self.config.block_size
+        logger.info(f"max num blocks pre batch: {max_num_blocks_pre_batch}")
 
         def dummy_tensor(shape, dtype):
             if None in shape:
                 return Tensor(shape=shape, dtype=dtype)
             return Tensor(np.ones(shape=tuple(shape)), dtype=dtype)
 
         batch_valid_length = dummy_tensor(shape=[bs], dtype=ms.int32)
-        batch_index = dummy_tensor(shape=[bs], dtype=ms.int64)
-        zactivate_len = dummy_tensor(shape=[seq_len], dtype=ms.int64)
+        batch_index = None if use_paged_attention else dummy_tensor(shape=[bs], dtype=ms.int64)
+        zactivate_len = None if use_paged_attention else dummy_tensor(shape=[seq_len], dtype=ms.int64)
+        pa_input = None if dyn else bs * seq_len
         if full_model:
             logger.info('\nexporting with batch_size = %s, seq = %s ...', self.config.batch_size, seq_length)
+            slot_mapping = dummy_tensor(shape=[pa_input], dtype=ms.int32) if use_paged_attention else None
             input_ids = dummy_tensor(shape=[bs, seq_len], dtype=ms.int32)
+            block_tables = None
         else:
             logger.info('\nexporting with batch_size = %s, seq = 1 ...', self.config.batch_size)
             input_ids = dummy_tensor(shape=[bs, 1], dtype=ms.int32)
-        return input_ids, None, None, None, None, None, None, batch_valid_length, batch_index, zactivate_len
+            slot_mapping = dummy_tensor(shape=[bs], dtype=ms.int32) if use_paged_attention else None
+            block_tables = dummy_tensor(shape=[bs, max_num_blocks_pre_batch],
+                                        dtype=ms.int32) if use_paged_attention else None
+
+        return input_ids, None, None, None, None, None, None, batch_valid_length, batch_index, zactivate_len, \
+               block_tables, slot_mapping
 
     # pylint: disable=W0613
     def construct(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None):
+                  input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None,
+                  block_tables=None, slot_mapping=None):
         r"""
         LlamaForCausalLM forward.
 
         Args:
             input_ids(Tensor): the tokenized inputs with datatype int32, Tensor of shape :math:`(batch, seq\_length)`.
             labels(Tensor): the tokenized labels with datatype int32, Tensor of shape :math:`(batch, seq\_length)`.
             input_position(Tensor): current position, used by model.predict.
@@ -424,15 +451,15 @@
             tokens = self.slice(input_ids, (0, 0), (bsz, seqlen - 1), (1, 1))
         else:
             tokens = input_ids
         if batch_valid_length is not None:
             batch_valid_length = self.reshape(batch_valid_length, (-1,))
         if not self.is_first_iteration:
             batch_valid_length = self.sub_batch_valid_len(batch_valid_length, 1)
-        output = self.model(tokens, batch_valid_length, batch_index, zactivate_len)
+        output = self.model(tokens, batch_valid_length, batch_index, zactivate_len, block_tables, slot_mapping)
         pre_gather = (not self.use_past or self.is_first_iteration) and batch_valid_length is not None
         if pre_gather:
             output = self.gather(output, self.sub_batch_valid_len(batch_valid_length, 1), 1)
         logits = self.lm_head(output)
 
         input_mask = self.cast(self.not_equal(tokens, self.pad_token_id), mstype.float32)
         if labels is None:
```

## mindformers/models/llama/llama_config.py

```diff
@@ -68,14 +68,15 @@
         parallel_config(TransformerOpParallelConfig):
             The parallel configure. Default `default_transformer_config`,
             an instance of `TransformerOpParallelConfig` with default args.
         pretrain_seqlen(int): The pretrained model seq length, default 2048.
         extend_method(str): The extend method of seq length of inferencem,default None.
         compute_in_2d(bool): Whether compute in 2-dims tensor, default False.
         use_flash_attention(bool): Whether enable flash attention ops, default False.
+        use_paged_attention(bool): Whether enable paged attention ops, default False.
         offset(int): Offset of transformer layer when set pipeline stage number.
         use_past_shard(bool): The configuration of kvcache parallel shard, default False.
         checkpoint_name_or_path (Optional[str]):
             checkpoint path or name used to load to the network.
         repetition_penalty (`float`, *optional*, defaults to 1.0):
             The parameter for repetition penalty. 1.0 means no penalty. See [this
             paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
@@ -85,15 +86,18 @@
         top_k (`int`, *optional*, defaults to 5):
             The number of highest probability vocabulary tokens to keep for top-k-filtering.
         top_p (`float`, *optional*, defaults to 1.0):
             If set to float < 1, only the smallest set of most probable tokens with probabilities
             that add up to `top_p` or higher are kept for generation.
         do_sample (`bool`, *optional*, defaults to `False`):
             Whether or not to use sampling ; use greedy decoding otherwise.
-
+        block_size (`int`, *optional*, defaults to 16):
+            The maximum number of tokens in one block can have when using paged attention.
+        num_blocks (`int`, *optional*, defaults to 512):
+            The maximum number of blocks when using paged attention.
         Returns:
             Class, LlamaConfig.
     """
 
     _support_list = MindFormerBook.get_config_support_list()['llama']
 
     def __init__(self,
@@ -129,19 +133,22 @@
                  extend_method: str = "None",
                  scaling_factor: float = 1.0,
                  is_dynamic: bool = False,
                  use_kvcache_op: bool = False,
                  is_flexible_shape: bool = False,
                  use_rope_slice: bool = False,
                  use_flash_attention: bool = False,
+                 use_paged_attention: bool = False,
                  fine_grain_interleave: int = 1,
                  offset: int = 0,
                  checkpoint_name_or_path: str = "",
                  repetition_penalty: float = 1.0,
                  max_decode_length: int = 1024,
+                 block_size: int = 16,
+                 num_blocks: int = 512,
                  top_k: int = 5,
                  top_p: float = 1.0,
                  do_sample: bool = True,
                  **kwargs):
         super(LlamaConfig, self).__init__(**kwargs)
         self.batch_size = batch_size
         self.seq_length = seq_length
@@ -189,7 +196,14 @@
         self.offset = offset
         self.repetition_penalty = repetition_penalty
         self.max_decode_length = max_decode_length
         self.top_k = top_k
         self.top_p = top_p
         self.do_sample = do_sample
         self.theta = theta
+        self.use_paged_attention = use_paged_attention
+        self.block_size = block_size
+        self.num_blocks = num_blocks
+        if use_paged_attention and (batch_size * seq_length // self.block_size > self.num_blocks):
+            logger.warning(
+                f"Argument `num blocks` is less than the maximum possible block numbers. "
+                f"May cause `block pool is out of memory` error")
```

## mindformers/models/llama/llama_layer.py

```diff
@@ -383,15 +383,15 @@
             self.square.shard((strategy_in,))
             self.mean.shard((strategy_in,))
             self.rsqrt.shard((strategy_in,))
             self.add.shard((strategy_in, ()))
             self.mul.shard((strategy_in, strategy_in))
             self.mul2.shard((strategy_in, (1,)))
         else:
-            self.norm.shard((strategy_in,))
+            self.norm.shard((strategy_in, (1,)))
 
 
 class LlamaFeedForward(Cell):
     r"""
     LLaMA FeedForward.
 
     .. math::
```

## mindformers/models/llama/llama_transformer.py

```diff
@@ -17,30 +17,30 @@
 import math
 
 try:
     from mindspore._checkparam import Validator
 except ImportError:
     import mindspore._checkparam as Validator
 
-from mindspore import nn, __version__
+from mindspore import nn, __version__, ops
 import mindspore.common.dtype as mstype
 from mindspore.common.tensor import Tensor
 from mindspore.context import ParallelMode
 from mindspore.ops import operations as P
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
 try:
     from mindspore.nn.layer.flash_attention import FlashAttention
     FLASHATTENTION_VALID = True
 except ImportError:
     FLASHATTENTION_VALID = False
 
 from mindformers.models.llama.llama_layer import LlamaFeedForward, LlamaRMSNorm, LlamaRotaryEmbedding
 from mindformers.modules.layers import _check_input_dtype, Linear
 from mindformers.modules.transformer import TransformerOpParallelConfig
-from mindformers.modules import KVCacheMgr
+from mindformers.modules import KVCacheMgr, PagedAttentionMgr
 
 from mindformers.tools.utils import is_version_ge
 from mindformers.tools.logger import logger
 
 class LLamaAttention(nn.Cell):
     r"""
     This is an implementation of multihead attention in LLaMA.
@@ -116,29 +116,35 @@
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
                  use_rope_slice=False,
                  use_flash_attention=False,
+                 use_paged_attention=False,
+                 block_size: Optional[int] = None,
+                 num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
         self.seq_length = seq_length
         self.hidden_size = dim
         self.n_head = n_heads
         self.head_dim = dim // n_heads
         self.n_kv_head = n_heads if n_kv_heads is None else n_kv_heads
         self.n_rep = self.n_head // self.n_kv_head
         self.kv_dim = self.n_kv_head * self.head_dim
+        self.block_size = block_size
+        self.num_blocks = num_blocks
 
         self.dtype = compute_dtype
         self.softmax_dtype = softmax_compute_dtype
         self.is_first_iteration = True
         self.use_past = use_past
         self.use_flash_attention = use_flash_attention and FLASHATTENTION_VALID
+        self.use_paged_attention = use_paged_attention
         self.qkv_concat = qkv_concat
 
         if self.hidden_size % self.n_head != 0:
             raise ValueError("For 'MultiHeadAttention', the class variable 'hidden_size' must be a multiple "
                              "of 'n_head', but got the hidden_size is {} and the n_head is {}."
                              .format(self.hidden_size, self.n_head))
         if self.n_kv_head % parallel_config.model_parallel != 0:
@@ -239,21 +245,30 @@
             self.use_flash_attention = False
             logger.info("Current MindSpore do not support flash attention, please upgrade to 2.2.0 or higher")
         if self.use_flash_attention:
             self.flash_attention = FlashAttention(self.head_dim, n_heads, dp=dp, mp=mp, next_block_num=0,
                                                   high_precision=True)
 
         if self.use_past:
-            self.kvcache_mgr = KVCacheMgr(self.n_kv_head, self.head_dim,
-                                          max_batch_size=batch_size,
-                                          max_seq_length=seq_length,
-                                          compute_dtype=compute_dtype,
-                                          is_dynamic=is_dynamic,
-                                          use_kvcache_op=use_kvcache_op,
-                                          is_flexible_shape=is_flexible_shape)
+            if self.use_paged_attention:
+                self.kvcache_mgr = PagedAttentionMgr(self.n_head,
+                                                     self.head_dim,
+                                                     self.hidden_size,
+                                                     n_kv_heads=self.n_kv_head,
+                                                     block_size=self.block_size,
+                                                     num_blocks=self.num_blocks,
+                                                     compute_dtype=compute_dtype)
+            else:
+                self.kvcache_mgr = KVCacheMgr(self.n_kv_head, self.head_dim,
+                                              max_batch_size=batch_size,
+                                              max_seq_length=seq_length,
+                                              compute_dtype=compute_dtype,
+                                              is_dynamic=is_dynamic,
+                                              use_kvcache_op=use_kvcache_op,
+                                              is_flexible_shape=is_flexible_shape)
             self.kvcache_mgr.shard(parallel_config)
 
     def construct(self, x: Tensor, freqs_cis: Tuple[Tensor, Tensor], mask=None, kvcache_inputs=None):
         """Forward process of the MultiHeadAttention"""
         ori_dtype = x.dtype
         # [bs, seq/1, hidden_dim]
         bs, seq_len, _ = self.shape(x)
@@ -284,24 +299,33 @@
             query = self.transpose(query, (0, 2, 1, 3))
             key = self.transpose(key, (0, 2, 1, 3))
             value = self.transpose(value, (0, 2, 1, 3))
         # [bs, n_head/n_kv_head, seq/1, head_dim]
         query, key = self.apply_rotary_emb(query, key, freqs_cis) # dp, mp, 1, 1
         # kv cache: [bs, n_kv_head, 1, head_dim] -> [bs, n_kv_head, seq, head_dim]
         if self.use_past:
-            key, value = self.kvcache_mgr(key, value, kvcache_inputs)
+            if self.use_paged_attention:
+                _, _, slot_mapping = kvcache_inputs
+                key_out = self.kvcache_mgr(key, value, slot_mapping)
+                query = ops.depend(query, key_out)
+            else:
+                key, value = self.kvcache_mgr(key, value, kvcache_inputs)
         # kv share: [bs, n_kv_head, seq, head_dim] -> [bs, n_head, seq, head_dim]
         key = self._repeat_kv(key, self.n_rep)
         value = self._repeat_kv(value, self.n_rep)
         # q, k, v: [bs, n_head, seq/1, head_dim], [bs, n_head, seq, head_dim], [bs, n_head, seq, head_dim]
         if self.use_flash_attention:
             attention = self.flash_attention(query, key, value, mask)
             attention = self._merge_heads(attention)
         else:
-            attention = self._attn(query, key, value, mask)
+            if not self.is_first_iteration and self.use_paged_attention:
+                batch_valid_length, block_tables, _ = kvcache_inputs
+                attention = self.kvcache_mgr.paged_attn(query, batch_valid_length, block_tables)
+            else:
+                attention = self._attn(query, key, value, mask)
         # [bs, seq/1, hidden_dim] or [bs * seq/1, hidden_dim]
         output = self.wo(attention) # dp, mp -> dp, 1 / dp * mp, 1
         output = self.cast(output, ori_dtype)
 
         return output
 
     def _repeat_kv(self, x, rep):
@@ -440,14 +464,17 @@
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
                  use_rope_slice=False,
                  use_flash_attention=False,
+                 use_paged_attention=False,
+                 block_size: Optional[int] = None,
+                 num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
         if batch_size or use_past:
             Validator.check_positive_int(batch_size)
         self.batch_size = batch_size
 
         self.seq_length = seq_length
@@ -480,14 +507,17 @@
                                         qkv_has_bias=qkv_has_bias,
                                         use_past=use_past,
                                         is_dynamic=is_dynamic,
                                         use_kvcache_op=use_kvcache_op,
                                         is_flexible_shape=is_flexible_shape,
                                         use_rope_slice=use_rope_slice,
                                         use_flash_attention=use_flash_attention,
+                                        use_paged_attention=use_paged_attention,
+                                        block_size=block_size,
+                                        num_blocks=num_blocks,
                                         parallel_config=parallel_config)
         self.feed_forward = LlamaFeedForward(dim=self.hidden_size,
                                              intermediate_size=intermediate_size,
                                              hidden_dim=4 * self.hidden_size,
                                              multiple_of=multiple_of,
                                              ffn_dim_multiplier=ffn_dim_multiplier,
                                              compute_dtype=compute_dtype,
```

## mindformers/modules/__init__.py

```diff
@@ -13,12 +13,13 @@
 # limitations under the License.
 # ============================================================================
 """MindFormers Transformers API."""
 from .transformer import *
 from .layers import *
 from .local_block_sparse_attention import *
 from .kvcache_mgr import *
+from .paged_attention_mgr import *
 
 __all__ = []
 __all__.extend(transformer.__all__)
 __all__.extend(layers.__all__)
 __all__.extend(local_block_sparse_attention.__all__)
```

## mindformers/modules/kvcache_mgr.py

```diff
@@ -19,14 +19,15 @@
 from mindspore import nn, Parameter, ops
 import mindspore.common.dtype as mstype
 from mindspore.ops import operations as P
 
 
 class KVCacheMgr(nn.Cell):
     """KVCache Manager."""
+
     def __init__(self,
                  n_head,
                  head_dim,
                  max_batch_size=8,
                  max_seq_length=4096,
                  compute_dtype=mstype.float16,
                  is_dynamic=False,
@@ -122,18 +123,18 @@
                                 self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
             self.prompt_kvcache(self.value_past, value_update, batch_valid_length, batch_index_pad,
                                 self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
             return None
 
         key_cache = self.key_past
         value_cache = self.value_past
-        self.decoder_kvcache(self.key_past, key_update, batch_valid_length, batch_index_pad,
-                             self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
-        self.decoder_kvcache(self.value_past, value_update, batch_valid_length, batch_index_pad,
-                             self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
+        key_update = self.decoder_kvcache(self.key_past, key_update, batch_valid_length, batch_index_pad,
+                                          self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
+        value_update = self.decoder_kvcache(self.value_past, value_update, batch_valid_length, batch_index_pad,
+                                            self.seqlen_axis_tensor_pad, seq_length_tensor_pad, seq_length_tensor_pad)
         key_cache = ops.depend(key_cache, key_update)
         value_cache = ops.depend(value_cache, value_update)
         return key_cache, value_cache
 
     def manual_caching(self, key_update, value_update, valid_length_vector, batch_size):
         """use assign to cache key, value"""
         # key_update shape: [real_bs, n_head, 1, head_dim]
@@ -189,25 +190,29 @@
             key, value = self.trimming(key, value, zactivate_len, batch_size=batch_size)
 
         return key, value
 
 
 class KVCachePreprocess(nn.Cell):
     """KVCache Manager."""
+
     def __init__(self,
                  max_batch_size=8,
                  max_seq_length=4096,
                  is_dynamic=False,
                  use_kvcache_op=False,
                  is_flexible_shape=False,
+                 use_paged_attention=False
                  ):
         super().__init__()
         self.is_dynamic = is_dynamic
         self.use_kvcache_op = use_kvcache_op
         self.is_flexible_shape = is_flexible_shape
+        self.use_paged_attention = use_paged_attention
+
         self.max_cache_length = max_batch_size * max_seq_length
         range_len = self.max_cache_length if self.is_flexible_shape else max_seq_length
         self.range = Tensor(np.arange(range_len).reshape((1, 1, -1)), mstype.int32)
         self.cache_length_tensor = Tensor([max_batch_size * max_seq_length], dtype=mstype.int32)
         self.cache_pad_tensor = Tensor([3], dtype=mstype.int64)
         self.seq_length_tensor = Tensor([max_seq_length], dtype=mstype.int32)
         self.seq_length_tensor_pad = Tensor([max_seq_length, 3], dtype=mstype.int64)
@@ -217,16 +222,22 @@
         self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
         self.equal = P.Equal().shard(((1, 1, 1), (1, 1, 1)))
         self.less = P.Less().shard(((1, 1, 1), (1, 1, 1)))
         self.expand_dims = P.ExpandDims().shard(((1, 1, 1),))
         self.div = P.Div()
         self.concat = P.Concat(axis=0)
 
-    def construct(self, batch_size, batch_valid_length=None, batch_index=None, zactivate_len=None):
+    def construct(self, batch_size, batch_valid_length=None, batch_index=None, zactivate_len=None,
+                  block_tables=None, slot_mapping=None):
         """precompute kvcache inputs"""
+        if self.use_paged_attention:
+            cur_pos = batch_valid_length + 1
+            kvcache_inputs = (cur_pos, block_tables, slot_mapping)
+            return kvcache_inputs
+
         seq_range = self.range
         if self.is_dynamic and self.is_flexible_shape and not self.use_kvcache_op:
             seq_range = self.slice(seq_range, (0, 0, 0), (1, 1, self.max_cache_length // batch_size), (1, 1, 1))
 
         if self.use_kvcache_op:
             if batch_index is None:
                 batch_index = ops.arange(0, batch_size, 1)
```

## mindformers/pipeline/text_generation_pipeline.py

```diff
@@ -113,15 +113,16 @@
                              " requires for a tokenizer.")
 
         super().__init__(model, tokenizer, **kwargs)
         self.model_name = kwargs.get("model_name", None)
         self.use_past = False
         if hasattr(self.network.config, "use_past"):
             self.use_past = self.network.config.use_past
-        if hasattr(self.network.config, "use_past") and self.network.config.batch_size is not None:
+        # only when incremental generate, set batch size as model config bs
+        if self.use_past and hasattr(self.network.config, "batch_size") and self.network.config.batch_size is not None:
             self._batch_size = self.network.config.batch_size
 
     def _sanitize_parameters(self, **pipeline_parameters):
         r"""Sanitize Parameters
 
         Args:
             pipeline_parameters (Optional[dict]):
```

## mindformers/tools/export.py

```diff
@@ -27,17 +27,14 @@
 from mindformers.models import build_model
 from mindformers.pet import get_pet_model
 from mindformers.tools.register import MindFormerConfig
 # pylint: disable=W0611
 from research.baichuan2.baichuan2_7b import Baichuan7BV2ForCausalLM
 from research.baichuan2.baichuan2_13b import Baichuan13BV2ForCausalLM
 
-# pylint: disable=W0611
-import research.qwen.qwen_model
-import research.qwen.qwen_config
 
 def get_glm_prefill_model_input(batch_size, seq_length):
     """get glm model input tuple."""
     x = ms.Tensor(np.ones([batch_size, seq_length]).astype(np.int32))
     position_ids = ms.Tensor(np.ones([batch_size, 2, seq_length]).astype(np.int32))
     attention_mask = ms.Tensor(np.ones([batch_size, 1, seq_length, seq_length]).astype(np.int32))
     return x, position_ids, attention_mask
@@ -150,28 +147,26 @@
     "llama": get_llm_common_prefill_model_input,
     "llama2": get_llm_common_prefill_model_input,
     "glm": get_glm_prefill_model_input,
     "gpt2": get_gpt2_model_input,
     "glm2": get_glm2_prefill_model_input,
     "glm3": get_glm2_prefill_model_input,
     "baichuan2": get_llm_common_prefill_model_input,
-    "qwen": get_llm_common_prefill_model_input
 }
 
 INCREMENT_MODEL_INPUT_MAP = {
     "bloom": get_bloom_inc_model_input,
     "llama": get_llama_inc_model_input,
     "llama2": get_llama_inc_model_input,
     "glm": get_glm_inc_model_input,
     "gpt2": get_gpt2_model_input,
     "glm2": get_glm2_inc_model_input,
     "glm3": get_glm2_inc_model_input,
     "codegeex2": get_glm2_inc_model_input,
     "baichuan2": get_baichuan2_inc_model_input,
-    "qwen": get_llama_inc_model_input
 }
 
 
 def export_single_model(config, batch_size, model_type: str = 'MINDIR', model_dir=None):
     """
     export no kvcache model.
     Args:
```

## mindformers/trainer/base_trainer.py

```diff
@@ -407,14 +407,21 @@
         if pp > 1:
             micro_batch_num = self.config.parallel_config.micro_batch_num
             network = PipelineCell(network, micro_size=micro_batch_num)
         if parallel_mode in ["semi_auto_parallel", "auto_parallel"]:
             network = _VirtualDatasetCell(network)
         return network
 
+    def wrap_eval_network_with_tool_cells(self, network):
+        """For evaluate in training process, warp the network with some tool cells."""
+        parallel_mode = ms.context.get_auto_parallel_context("parallel_mode")
+        if parallel_mode in ["semi_auto_parallel", "auto_parallel"]:
+            network = _VirtualDatasetCell(network)
+        return network
+
     def create_image_processor(self, default_args: dict = None):
         """Create the image processor for predict."""
         logger.info(".........Build Image Processor From Config..........")
         self.image_processor = build_processor(
             self.config.processor.image_processor, default_args=default_args)
         return self.image_processor
 
@@ -604,30 +611,37 @@
             callbacks: Optional[Union[Callback, List[Callback]]] = None,
             **kwargs):
         """Train or Fine-tune for BaseTrainer in MindFormers."""
         self.kwargs = kwargs
         is_full_config = kwargs.get("is_full_config", False)
         config = self.set_config(config, is_full_config)
 
+        # build dataset
+        logger.info(".........Build Dataset For Train..........")
+        if dataset is None:
+            dataset = self.create_train_dataset()
+
+        append_info = None
         if config.resume_training and config.load_checkpoint:
             logger.info(".............Start load resume context from checkpoint..................")
-            load_resume_context_from_checkpoint(config)
+            load_resume_context_from_checkpoint(config, dataset)
+            resume_dict = {
+                "step_num": config.runner_config.initial_step,
+                "epoch_num": config.runner_config.initial_epoch,
+            }
+            if config.runner_wrapper.scale_sense is not None:
+                resume_dict["loss_scale"] = config.runner_wrapper.scale_sense.loss_scale_value
             logger.info("initial epoch: %d", config.runner_config.initial_epoch)
             logger.info("initial step: %d", config.runner_config.initial_step)
+            append_info = [resume_dict]
+            dataset.set_init_step(config.runner_config.initial_step)
         else:
             config.runner_config.initial_epoch = 0
             config.runner_config.initial_step = 0
 
-        # build dataset
-        logger.info(".........Build Dataset For Train..........")
-        if dataset is None:
-            dataset = self.create_train_dataset()
-        if config.runner_config.initial_step:
-            dataset.set_init_step(config.runner_config.initial_step)
-
         self.set_train_dataset(dataset)
         check_runner_config(config, dataset)
 
         # check rules
         check_rules(config, mode='train', network=network, dataset=dataset)
 
         # build network
@@ -644,14 +658,15 @@
         self._check_training_network_no_use_past(network)
 
         eval_network = None
         if network is not None:
             eval_network = network
             # warp network for training
             network = self.wrap_network_with_tool_cells(eval_network)
+            eval_network = self.wrap_eval_network_with_tool_cells(eval_network)
             self.set_network(network, is_train=True)
         if wrapper is not None:
             self.set_model_wrapper(wrapper)
 
         self.count_parameters()
 
         # build optimizer
@@ -665,24 +680,35 @@
             wrapper = self.create_model_wrapper(network, optimizer)
         elif wrapper is None and self.model_wrapper is not None:
             logger.info(".........Using The Existing Model Wrapper: %s", self.model_wrapper.__class__.__name__)
             wrapper = self.model_wrapper
 
         # build callback
         logger.info(".........Build Callbacks For Train..........")
-        default_callbacks = self.create_callbacks(default_args={
-            "learning_rate": optimizer.learning_rate if optimizer else wrapper.optimizer.learning_rate,
-            "origin_epochs": config.runner_config.origin_epochs,
-            "dataset_size": config.data_size,
-            "micro_batch_interleave_num": config.micro_batch_interleave_num,
-            "micro_batch_num": config.parallel_config.micro_batch_num,
-            "initial_epoch": config.runner_config.initial_epoch,
-            "initial_step": config.runner_config.initial_step,
-            "global_batch_size": self.global_batch_size,
-            "gradient_accumulation_steps": self.config.runner_config.gradient_accumulation_steps})
+        default_callbacks = []
+        if self.config.profile:
+            default_callbacks.append(self.config.profile_cb)
+        for callback in self.config.callbacks:
+            default_args = None
+            if "type" in callback and callback["type"] == "MFLossMonitor":
+                default_args = {
+                    "learning_rate": optimizer.learning_rate if optimizer else wrapper.optimizer.learning_rate,
+                    "origin_epochs": config.runner_config.origin_epochs,
+                    "dataset_size": config.data_size,
+                    "micro_batch_interleave_num": config.micro_batch_interleave_num,
+                    "micro_batch_num": config.parallel_config.micro_batch_num,
+                    "initial_epoch": config.runner_config.initial_epoch,
+                    "initial_step": config.runner_config.initial_step,
+                    "global_batch_size": self.global_batch_size,
+                    "gradient_accumulation_steps": self.config.runner_config.gradient_accumulation_steps
+                }
+            elif "type" in callback and callback["type"] == "CheckpointMointor":
+                default_args = {"append_info": append_info}
+
+            default_callbacks.append(build_callback(callback, default_args=default_args))
         if callbacks is not None:
             if isinstance(callbacks, list):
                 default_callbacks.extend(callbacks)
             if isinstance(callbacks, Callback):
                 default_callbacks.append(callbacks)
         callbacks = default_callbacks
```

## mindformers/trainer/utils.py

```diff
@@ -255,39 +255,41 @@
     else:
         raise FileNotFoundError(f"{checkpoint_dir} is not found.")
     checkpoint_dict = load_checkpoint(distribute_checkpoint_path, specify_prefix=specify_prefix)
     logger.info("Distribute load is success.")
     return checkpoint_dict
 
 
-def load_resume_context_from_checkpoint(config):
+def load_resume_context_from_checkpoint(config, dataset):
     """resume training, load training info from checkpoint to config"""
     if not os.path.realpath(config.load_checkpoint) or \
             not os.path.exists(config.load_checkpoint):
         raise FileNotFoundError(f"The load_checkpoint must be correct, "
                                 f"but get {config.load_checkpoint}")
 
     if os.path.isdir(config.load_checkpoint):
         resume_dict = load_distributed_checkpoint(config.load_checkpoint, ["loss_scale", "epoch_num", "step_num"])
     else:
         resume_dict = load_checkpoint(config.load_checkpoint, specify_prefix=["loss_scale", "epoch_num", "step_num"])
 
+    if "step_num" in resume_dict:
+        config.runner_config.initial_step = int(resume_dict["step_num"])
+    else:
+        config.runner_config.initial_step = 0
+
     if "epoch_num" in resume_dict:
         if config.runner_config.sink_mode:
             config.runner_config.initial_epoch = int(resume_dict["epoch_num"])
         else:
-            config.runner_config.initial_epoch = int(resume_dict["epoch_num"]) - 1
+            data_size = dataset.get_dataset_size()
+            not_last_step_in_epoch = int(config.runner_config.initial_step % data_size != 0)
+            config.runner_config.initial_epoch = int(resume_dict["epoch_num"]) - not_last_step_in_epoch
     else:
         config.runner_config.initial_epoch = 0
 
-    if "step_num" in resume_dict:
-        config.runner_config.initial_step = int(resume_dict["step_num"])
-    else:
-        config.runner_config.initial_step = 0
-
     for callback in config.callbacks:
         if "type" in callback and callback["type"] == "CheckpointMointor":
             if config.runner_wrapper.scale_sense is not None and "loss_scale" in resume_dict:
                 config.runner_wrapper.scale_sense.loss_scale_value = resume_dict["loss_scale"]
             break
 
 
@@ -351,30 +353,36 @@
     soft_link_dir = os.path.join(get_output_root_path(), "softlink_ckpt")
     rank_id = get_real_rank()
     if (not rank_id) or (rank_id % 8 == 0 and check_in_modelarts()):
         if os.path.exists(soft_link_dir):
             shutil.rmtree(soft_link_dir)
             logger.info("Find exist softlink dir %s and delete it.", os.path.join(os.getcwd(), soft_link_dir))
         if os.path.isdir(ckpt_dir):
-            if check_ckpt_file_exist(ckpt_dir):
-                for ckpt_file in os.listdir(ckpt_dir):
-                    soft_link = os.path.join(soft_link_dir, os.path.splitext(ckpt_file)[0])
-                    ckpt_file = os.path.join(ckpt_dir, ckpt_file)
-                    make_softlink(soft_link, ckpt_file)
-            elif glob(os.path.join(ckpt_dir, "rank*")):
+            if check_rank_folders(ckpt_dir, 0):
+                if check_ckpt_file_exist(ckpt_dir):
+                    logger.warning(f"Find both ckpt files and rank folder under {ckpt_dir}, "
+                                   "the rank folder will be used for checkpoint transform.")
                 os.makedirs(soft_link_dir, exist_ok=True)
                 if ckpt_dir.endswith('/'):
                     ckpt_dir = ckpt_dir[:-1]
                 soft_link = os.path.join(soft_link_dir, os.path.basename(ckpt_dir))
                 logger.info("Make soft link of checkpoint file from %s to %s", ckpt_dir, soft_link)
                 if not os.path.exists(soft_link):
                     os.symlink(ckpt_dir, soft_link)
                 else:
                     os.remove(soft_link)
                     os.symlink(ckpt_dir, soft_link)
+            elif check_ckpt_file_exist(ckpt_dir):
+                for ckpt_file in os.listdir(ckpt_dir):
+                    if ckpt_file.endswith('.ckpt'):
+                        soft_link = os.path.join(soft_link_dir, os.path.splitext(ckpt_file)[0])
+                        ckpt_file = os.path.join(ckpt_dir, ckpt_file)
+                        make_softlink(soft_link, ckpt_file)
+            else:
+                raise ValueError(f"No rank_0 folder or ckpt files are found under {ckpt_dir}.")
         else:
             if ckpt_dir.endswith('.ckpt'):
                 ckpt_file = ckpt_dir
                 soft_link = os.path.join(soft_link_dir, os.path.splitext(os.path.basename(ckpt_file))[0])
                 make_softlink(soft_link, ckpt_file)
             else:
                 raise ValueError(f"The value of load_checkpoint must be a folder or a file with suffix '.ckpt', "
```

## mindformers/wrapper/adaptive_loss_scale.py

```diff
@@ -63,15 +63,15 @@
 @MindFormerRegister.register(MindFormerModuleType.WRAPPER)
 class AdaptiveLossScaleUpdateCell(Cell):
     r"""
     Adaptive Loss scale update cell.
 
     For loss scaling training, the initial loss scaling value will be set to be `loss_scale_value`.
     A scale window list which will be used to control loss scale adaptively will be initialized
-    according tp 'max_scale_window'.
+    according to 'max_scale_window'.
     In each training step, the loss scaling value will be decreased by `loss_scale`/`scale_factor`
     when there is an overflow. And it will be increased by `loss_scale` * `scale_factor` if there is no
     overflow for a continuous `scale_window` steps. Moreover, the scale window will be increased to next
     level if loss_scale increases three times during current scale window. The scale
     window will be decreased to '1' if loss_scale decreases three times consecutively.
 
     Args:
```

## Comparing `mindformers-1.0.0.dist-info/LICENSE` & `mindformers-1.0.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mindformers-1.0.0.dist-info/METADATA` & `mindformers-1.0.1.dist-info/METADATA`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mindformers
-Version: 1.0.0
+Version: 1.0.1
 Summary: mindformers platform: linux, cpu: x86_64
 Home-page: https://www.mindspore.cn
 Download-URL: https://gitee.com/mindspore/mindformers/tags
 Author: The MindSpore Authors
 Author-email: contact@mindspore.cn
 License: Apache 2.0
 Project-URL: Sources, https://gitee.com/mindspore/mindformers
@@ -62,71 +62,66 @@
 - 提供Trainer、pipeline、AutoClass等高阶易用性接口；
 - 提供预置SOTA权重自动下载及加载功能；
 - 支持人工智能计算中心无缝迁移部署；
 
 如果您对MindSpore Transformers有任何建议，请通过issue与我们联系，我们将及时处理。
 
 - **[MindFormers教程文档](https://mindformers.readthedocs.io/zh_CN/latest)**
-- [模型README](https://gitee.com/mindspore/mindformers/tree/dev/docs/model_cards)
-- [任务README](https://gitee.com/mindspore/mindformers/tree/dev/docs/task_cards)
+- [大模型支持列表](https://mindformers.readthedocs.io/zh-cn/latest/docs/model_support_list.html#llm)
 - [MindPet指导教程](docs/feature_cards/Pet_Tuners.md)
 - [AICC指导教程](docs/readthedocs/source_zh_cn/docs/practice/AICC.md)
 
 目前支持的模型列表如下：
 
-|                     模型                     |                      任务（task name）                       | 模型（model name）                                           |
-| :------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------- |
-|     [LLama2](docs/model_cards/llama2.md)     |    [text_generation](docs/task_cards/text_generation.md)     | llama2_7b <br>llama2_13b <br>llama2_7b_lora <br>llama2_13b_lora <br>llama2_70b |
-|       [GLM2](docs/model_cards/glm2.md)       |    [text_generation](docs/task_cards/text_generation.md)     | glm2_6b<br>glm2_6b_lora                                      |
-|  [CodeGeex2](docs/model_cards/codegeex2.md)  |    [text_generation](docs/task_cards/text_generation.md)     | codegeex2_6b                                                 |
-|      [LLama](docs/model_cards/llama.md)      |    [text_generation](docs/task_cards/text_generation.md)     | llama_7b <br>llama_13b <br>llama_7b_lora                     |
-|        [GLM](docs/model_cards/glm.md)        |    [text_generation](docs/task_cards/text_generation.md)     | glm_6b<br>glm_6b_lora                                        |
-|      [Bloom](docs/model_cards/bloom.md)      |    [text_generation](docs/task_cards/text_generation.md)     | bloom_560m<br>bloom_7.1b <br>                                |
-|       [GPT2](docs/model_cards/gpt2.md)       |    [text_generation](docs/task_cards/text_generation.md)     | gpt2_small <br>gpt2_13b <br>                                 |
-| [PanGuAlpha](docs/model_cards/pangualpha.md) |    [text_generation](docs/task_cards/text_generation.md)     | pangualpha_2_6_b<br>pangualpha_13b                           |
-|      [BLIP2](docs/model_cards/blip2.md)      | [contrastive_language_image_pretrain](docs/task_cards/contrastive_language_image_pretrain.md)<br> [zero_shot_image_classification](docs/task_cards/zero_shot_image_classification.md) | blip2_stage1_vit_g                                           |
-|       [CLIP](docs/model_cards/clip.md)       | [contrastive_language_image_pretrain](docs/task_cards/contrastive_language_image_pretrain.md)<br> [zero_shot_image_classification](docs/task_cards/zero_shot_image_classification.md) | clip_vit_b_32<br>clip_vit_b_16 <br>clip_vit_l_14<br>clip_vit_l_14@336 |
-|       [BERT](docs/model_cards/bert.md)       | masked_language_modeling<br>[text_classification](docs/task_cards/text_classification.md) <br>[token_classification](docs/task_cards/token_classification.md) <br>[question_answering](docs/task_cards/question_answering.md) | bert_base_uncased <br>txtcls_bert_base_uncased<br>txtcls_bert_base_uncased_mnli <br>tokcls_bert_base_chinese<br>tokcls_bert_base_chinese_cluener <br>qa_bert_base_uncased<br>qa_bert_base_chinese_uncased |
-|         [T5](docs/model_cards/t5.md)         |                         translation                          | t5_small                                                     |
-|   [sam](docs/model_cards/sam.md)             |        [segment_anything](docs/model_cards/sam.md)        | sam_vit_b <br>sam_vit_l  <br>sam_vit_h                       |
-|        [MAE](docs/model_cards/mae.md)        |                    masked_image_modeling                     | mae_vit_base_p16                                             |
-|        [VIT](docs/model_cards/vit.md)        | [image_classification](docs/task_cards/image_classification.md) | vit_base_p16                                                 |
-|       [Swin](docs/model_cards/swin.md)       | [image_classification](docs/task_cards/image_classification.md) | swin_base_p4w7                                               |
-
-目前在research中支持的模型列表如下：
-
-|                        模型                        |                   任务（task name）                   | 模型（model name）                                                           |
-| :------------------------------------------------: | :---------------------------------------------------: | :--------------------------------------------------------------------------- |
-|       [skywork](research/skywork/skywork.md)       | [text_generation](docs/task_cards/text_generation.md) | skywork_13b                                                                  |
-|    [Baichuan2](research/baichuan2/baichuan2.md)    | [text_generation](docs/task_cards/text_generation.md) | baichuan2_7b <br>baichuan2_13b  <br>baichuan2_7b_lora <br>baichuan2_13b_lora |
-|     [Baichuan](research/baichuan/baichuan.md)      | [text_generation](docs/task_cards/text_generation.md) | baichuan_7b <br>baichuan_13b                                                 |
-|           [Qwen](research/qwen/qwen.md)            | [text_generation](docs/task_cards/text_generation.md) | qwen_7b <br>qwen_14b                                                         |
-| [Wizardcoder](research/wizardcoder/wizardcoder.md) | [text_generation](docs/task_cards/text_generation.md) | wizardcoder_15b                                                              |
-|     [Internlm](research/internlm/internlm.md)      | [text_generation](docs/task_cards/text_generation.md) | Internlm_7b                                                                  |
-|           [ziya](research/ziya/ziya.md)            | [text_generation](docs/task_cards/text_generation.md) | ziya_13b                                                                     |
-|    [VisualGLM](research/visualglm/visualglm.md)    |                     image_to_text                     | visualglm                                                                    |
+|                         模型                         | model name                                                         |
+|:--------------------------------------------------:|:-------------------------------------------------------------------|
+|        [LLama2](docs/model_cards/llama2.md)        | llama2_7b, llama2_13b, llama2_7b_lora, llama2_13b_lora, llama2_70b |
+|          [GLM2](docs/model_cards/glm2.md)          | glm2_6b, glm2_6b_lora                                              |
+|     [CodeLlama](docs/model_cards/codellama.md)     | codellama_34b                                                      |
+|     [CodeGeex2](docs/model_cards/codegeex2.md)     | codegeex2_6b                                                       |
+|         [LLama](docs/model_cards/llama.md)         | llama_7b, llama_13b, llama_7b_lora                                 |
+|           [GLM](docs/model_cards/glm.md)           | glm_6b, glm_6b_lora                                                |
+|         [Bloom](docs/model_cards/bloom.md)         | bloom_560m, bloom_7.1b                                             |
+|          [GPT2](docs/model_cards/gpt2.md)          | gpt2, gpt2_13b                                                     |
+|    [PanGuAlpha](docs/model_cards/pangualpha.md)    | pangualpha_2_6_b, pangualpha_13b                                   |
+|         [BLIP2](docs/model_cards/blip2.md)         | blip2_stage1_vit_g                                                 |
+|          [CLIP](docs/model_cards/clip.md)          | clip_vit_b_32, clip_vit_b_16, clip_vit_l_14, clip_vit_l_14@336     |
+|            [T5](docs/model_cards/t5.md)            | t5_small                                                           |
+|           [sam](docs/model_cards/sam.md)           | sam_vit_b, sam_vit_l, sam_vit_h                                    |
+|           [MAE](docs/model_cards/mae.md)           | mae_vit_base_p16                                                   |
+|           [VIT](docs/model_cards/vit.md)           | vit_base_p16                                                       |
+|          [Swin](docs/model_cards/swin.md)          | swin_base_p4w7                                                     |
+|       [skywork](research/skywork/skywork.md)       | skywork_13b                                                        |
+|    [Baichuan2](research/baichuan2/baichuan2.md)    | baichuan2_7b, baichuan2_13b, baichuan2_7b_lora, baichuan2_13b_lora |
+|     [Baichuan](research/baichuan/baichuan.md)      | baichuan_7b, baichuan_13b                                          |
+|           [Qwen](research/qwen/qwen.md)            | qwen_7b, qwen_14b, qwen_7b_lora, qwen_14b_lora                     |
+| [Wizardcoder](research/wizardcoder/wizardcoder.md) | wizardcoder_15b                                                    |
+|     [Internlm](research/internlm/internlm.md)      | internlm_7b, internlm_20b, internlm_7b_lora                        |
+|           [ziya](research/ziya/ziya.md)            | ziya_13b                                                           |
+|    [VisualGLM](research/visualglm/visualglm.md)    | visualglm                                                          |
+|[iFlytekSpark](research/iflytekspark/iflytekspark.md)    | iflytekspark_13b, iflytekspark_13b_lora                                               |
 
 ## 二、mindformers安装
 
 - 方式1：Linux源码编译安装
 
 支持源码编译安装，用户可以执行下述的命令进行包的安装
 
 ```bash
-git clone -b dev https://gitee.com/mindspore/mindformers.git
+git clone -b r1.0 https://gitee.com/mindspore/mindformers.git
 cd mindformers
 bash build.sh
 ```
 
 - 方式2：镜像
 
 docker下载命令
 
 ```shell
-docker pull swr.cn-central-221.ovaijisuan.com/mindformers/mindformers0.8.0_mindspore2.2.0:aarch_20231025
+docker pull swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125
 ```
 
 创建容器
 
 ```shell
 # --device用于控制指定容器的运行NPU卡号和范围
 # -v 用于映射容器外的目录
@@ -147,29 +142,27 @@
 --device=/dev/devmm_svm \
 --device=/dev/hisi_hdc \
 -v /etc/localtime:/etc/localtime \
 -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
 -v /var/log/npu/:/usr/slog \
 -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
 --name {请手动输入容器名称} \
-swr.cn-central-221.ovaijisuan.com/mindformers/mindformers0.8.0_mindspore2.2.0:aarch_20231025 \
+swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125 \
 /bin/bash
 ```
 
 ## 三、版本匹配关系
 
-当前支持的硬件为Atlas 800训练服务器 与 [Atlas 800T A2](https://www.hiascend.com/hardware/ai-server?tag=900A2)训练服务器
+当前支持的硬件为Atlas 800训练服务器 与 [Atlas 800T A2](https://www.hiascend.com/hardware/ai-server?tag=900A2)训练服务器。
 
-| MindFormers | MindPet |                           MindSpore                           | Python |                                                                                                                                       CANN                                                                                                                                        |                                                                  驱动固件                                                                   |                                                                  镜像链接                                                                   | 备注                 |
-| :---------: | :-----: | :-----------------------------------------------------------: | :----: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------: | -------------------- |
-|     dev     |  1.0.2  |                            master                             |  3.9   |                                                                                                                                         /                                                                                                                                         |                                                                      /                                                                      |                                                                      /                                                                      | 开发分支(非稳定版本) |
-|     dev     |  1.0.2  | [2.2.0](https://repo.mindspore.cn/mindspore/mindspore/daily/) |  3.9   |       7.0.0.beta1: [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run) [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run)        | [固件驱动获取链接](https://www.hiascend.com/hardware/firmware-drivers/community?product=4&model=26&cann=7.0.RC1.beta1&driver=1.0.RC3.alpha) |                                                                      /                                                                      | 开发分支(非稳定版本) |
-|     0.8     |  1.0.2  |                             2.2.1                             |  3.9   |                                                                                                                                         /                                                                                                                                         |                                                                      /                                                                      |                                                                      /                                                                      | 发布版本分支         |
-|     0.8     |  1.0.2  |          [2.2.0](https://www.mindspore.cn/install/)           |  3.9   |   7.0.RC.beta1: [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.RC1/Ascend-cann-toolkit_7.0.RC1_linux-aarch64.run) [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.RC1/Ascend-cann-toolkit_7.0.RC1_linux-x86_64.run)   | [固件驱动获取链接](https://www.hiascend.com/hardware/firmware-drivers/community?product=4&model=26&cann=7.0.RC1.beta1&driver=1.0.RC3.alpha) | [物理机](http://mirrors.cn-central-221.ovaijisuan.com/detail/109.html) [AICC](http://mirrors.cn-central-221.ovaijisuan.com/detail/109.html) | 发布版本分支         |
-|     0.7     |  1.0.1  |          [2.1.1](https://www.mindspore.cn/install/)           |  3.9   | 6.3.RC2.alpha005: [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%206.3.RC2/Ascend-cann-toolkit_6.3.RC2_linux-aarch64.run) [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%206.3.RC2/Ascend-cann-toolkit_6.3.RC2_linux-x86_64.run) |      [固件驱动获取链接](https://www.hiascend.com/hardware/firmware-drivers/community?product=4&model=10&cann=All&driver=1.0.19.alpha)       |                                                                      /                                                                      | 旧版本分支           |
+当前套件建议使用的Python版本为3.9。
+
+| MindFormers | MindPet |                 MindSpore                  |                                                                                                                                     CANN                                                                                                                                     |                               驱动固件                               |                               镜像链接                               | 备注     |
+| :---------: | :-----: | :----------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------: | :------------------------------------------------------------------: | -------- |
+|    r1.0     |  1.0.3  | [2.2.11](https://www.mindspore.cn/install) | 7.0.0.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run) | [链接](https://www.hiascend.com/hardware/firmware-drivers/community) | [链接](http://mirrors.cn-central-221.ovaijisuan.com/detail/118.html) | 版本分支 |
 
 其中CANN，固件驱动的安装需与使用的机器匹配，请注意识别机器型号，选择对应架构的版本
 
 ## 四、快速使用
 
 MindFormers套件对外提供两种使用和开发形式，为开发者提供灵活且简洁的使用方式和高阶开发接口。
 
@@ -178,26 +171,26 @@
 用户可以直接clone整个仓库，按照以下步骤即可运行套件中已支持的任意`configs`模型任务配置文件，方便用户快速进行使用和开发：
 
 - 准备工作
 
     - step1：git clone mindformers
 
   ```shell
-  git clone -b dev https://gitee.com/mindspore/mindformers.git
+  git clone -b r1.0 https://gitee.com/mindspore/mindformers.git
   cd mindformers
   ```
 
     - step2:  准备相应任务的数据集，请参考`docs`目录下各模型的README.md文档准备相应数据集
 
     - step3：修改配置文件`configs/{model_name}/run_{model_name}_***.yaml`中数据集路径
 
     - step4：如果要使用分布式训练，则需提前生成RANK_TABLE_FILE
 
   ```shell
-  # 不包含8本身，生成0~7卡的hccl json文件
+  # 不包含8本身，生成0~7卡的hccl json文件。注意：不支持在镜像容器中执行该命令，请在容器外执行。
   python mindformers/tools/hccl_tools.py --device_num [0,8)
   ```
 
 - 单卡启动：统一接口启动，根据模型 CONFIG 完成任意模型的单卡训练、微调、评估、推理流程
 
 ```shell
 # 训练启动，run_mode支持train、finetune、eval、predict四个关键字，以分别完成模型训练、评估、推理功能，默认使用配置文件中的run_mode
```

## Comparing `mindformers-1.0.0.dist-info/RECORD` & `mindformers-1.0.1.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-configs/README.md,sha256=EvBJOlz8Brk_4SYuGevl33NgDO25ZRXw3IwqlzwKR-Q,14400
+configs/README.md,sha256=MbH9Wvik_fIHbH41_l2qWuTlRr5-jh5SKSbgxYEF0vk,14676
 configs/bert/run_bert_base_uncased.yaml,sha256=iWOw49tacAz3fmtLRlHq04Ioo0RZ0ozTLoolf_PwgnY,4241
 configs/bert/run_bert_tiny_uncased.yaml,sha256=A0ccR79xeoDmiVQcfTXJexO_w9QmcgUom101gvm8_Z0,4257
 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml,sha256=dmd4upjSosmlfmDYHpW6NvrKfcG-TyYOJWuIne-SRJA,6074
 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml,sha256=1Bdi0SPLXgSwkSozh3LEAq10nqaiOeNeVNKjKZTrVo4,6139
 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml,sha256=YDNI1hyn6qpKGAa4crFpsdwzxC_ipomfBulS6dW7dM8,4951
 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml,sha256=mpQ2UidIQmeismh7uEq7BitX0lO_tLOCrLcWDYw7wI8,6214
 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml,sha256=joW0qsvJ6oJjL8Ti2Ulrtli6kM6uLsZ014BeUxW4arc,4705
@@ -18,18 +18,18 @@
 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml,sha256=KvSiCqSz6MaxdLDFCQTxF7PipKlXR3GdjXrbh0Ef8x4,4138
 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml,sha256=Rp6mL0fIkQptCx5EuYNz2Wid3uCCBlsqg1mbJk0eXIw,4285
 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml,sha256=DTdns0SGRmNyCr2MPfxOK_oTwcDFqVrN66LiXZ_LFQE,4148
 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml,sha256=yS8ik9FlngbsYsw30XWyMgoMJWCsm7z0sxUuYvIXITo,4295
 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml,sha256=uU7eMPTS6aeZPNCQmXeCrJ2bhkDyFI4c5aDx6AloHLU,4139
 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml,sha256=LHHp3kZa6z_34yTYqANZX1hLML9O6Q-fvgaC_6Birbk,4286
 configs/codegeex2/run_codegeex2_6b.yaml,sha256=xluiExAoTNkKsDHZN6Nq0ijAxezJAc-WI16Elwaq248,6097
-configs/codegeex2/run_codegeex2_6b_eval.yaml,sha256=TdgauE91Hfb9Xw4c4WN9c9ntBBHVwV174SD74dzR_ac,5815
+configs/codegeex2/run_codegeex2_6b_eval.yaml,sha256=mz9coO4OY9EvU3qZ8P9mDwP8Ak2pIbPlGcLXCj4_LRE,5811
 configs/codegeex2/run_codegeex2_6b_finetune.yaml,sha256=JLChiOOMBRMAEBt7Ygkqq-oR9y7AZIDye5CYjI9yl2k,5866
 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml,sha256=OXgCP_YX10mJxk9LNIBVY_tA4ptI8QFehlb8B1d_cbA,5870
-configs/codellama/predict_codellama_34b_910b.yaml,sha256=qeAcSduRHR_xt6eFtiyLDR8N9hoMcIty4PTBf4y9Va4,3827
+configs/codellama/predict_codellama_34b_910b.yaml,sha256=lULuMs3rdRcMG_P9WeGOZVfNOF-8OAscgXRTFV8RZzI,4009
 configs/codellama/run_codellama_34b_910b.yaml,sha256=w9kJcXUJcAse0KTfGKVOpomObojK10jjOAUIDVqPnNA,5330
 configs/general/run_general_task.yaml,sha256=U2W8mpKxZeLMNGY-QU3AzR-_sRoA0qt0097x_F9G80E,2637
 configs/glm/run_glm_6b_finetune.yaml,sha256=jvFDWtYM47R0lHv1XnXSL6zmciY95k9cQPyH1AFlKsM,5756
 configs/glm/run_glm_6b_infer.yaml,sha256=LonnLMP4yQo7cvcHOE7yESGnhe0UzhLQrxoJO68GZ7Q,5677
 configs/glm/run_glm_6b_lora.yaml,sha256=0TlyQatKyxosS7PvX6v-Rl1Jxhl4OndMY4jre3OvhWA,5954
 configs/glm/run_glm_6b_lora_infer.yaml,sha256=8eq2ufiwvd1aG9CzlTXbYKOWs6VnvOnLQeL3zABq4FE,5861
 configs/glm2/export_glm2_6b.yaml,sha256=_dPxIJE6FcUwZSMHhuEsNmlgRubV-JiBw4beZ-y9TB0,1564
@@ -57,17 +57,17 @@
 configs/gpt2/run_gpt2_xl.yaml,sha256=pEKDfDXlictWLA-yw1yZhc6fY6oDw6RvFP1a1vqlD8A,4631
 configs/gpt2/run_gpt2_xl_lora.yaml,sha256=3WYUKPhtDS4nIcZSOeuCAkgfJcAhgpvZ3qN6FipPOxQ,4824
 configs/llama/run_llama_13b.yaml,sha256=FmiET1N2Dq94NGhLPgp7O2ylji7cfvfvxj-dAclHItM,5173
 configs/llama/run_llama_13b_910b.yaml,sha256=uFDLSPspQxIM8zRUoWulgisCC2PkaxcbDZZ8jWPj4Fs,5176
 configs/llama/run_llama_7b.yaml,sha256=3xCwffaPpiNS45fjWCz40sPYPEYlAf3XN6t7hcn9u6g,5169
 configs/llama/run_llama_7b_910b.yaml,sha256=CGbewna6u1Tq_pUdBBl8LzN-_VOKSnCU063RMamNH00,5167
 configs/llama/run_llama_7b_lora.yaml,sha256=CxhPIfVrqGnkI6sejAtLw4z145pnl8rpPMmAS0ckctM,5625
-configs/llama2/export_llama2_13b.yaml,sha256=1FC5rj4Py2FYW8ZurfIVkiYKugZ6YAvKYcru0PmvWgU,2708
-configs/llama2/export_llama2_7b.yaml,sha256=bx0N6YBKTOpO70R3slnuh9nVU7S9rKUkR4npSRUjcfI,2539
-configs/llama2/predict_llama2_70b_910b.yaml,sha256=PvEDdGQHLXIgiat4PcOabQ1Xw1xIz_H45PDxLX6P9gQ,3875
+configs/llama2/export_llama2_13b.yaml,sha256=etkMnV6erB8s3764A2etAs4lleio2LPXXobGhxALets,2890
+configs/llama2/export_llama2_7b.yaml,sha256=5J0gR3dMzGDvv8vThlCQjw6CV8vNoluNZtqMldygUJg,2721
+configs/llama2/predict_llama2_70b_910b.yaml,sha256=gOc3WXZh8KBJrSvuuRMJqiGDahGPLqNg5sKpa21qmVY,4057
 configs/llama2/run_llama2_13b.yaml,sha256=hbMoLjEj1Su7st_2EXi7-FsvT-15BWUWDEf63JkbmZ0,5424
 configs/llama2/run_llama2_13b_910b.yaml,sha256=xkNqDwO85dch6hysr_ie650DOdfKznPnKDBHJB7na4E,5087
 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml,sha256=sfT8kakOTCqYE6PwklYehDKUsEbLeBoZvTgVsWY-IAI,5655
 configs/llama2/run_llama2_13b_910b_finetune.yaml,sha256=vgSM_1k889p7ddfG6nJzbr9dRCorAj8MXlCMV_IQQTQ,5137
 configs/llama2/run_llama2_13b_lora_910b.yaml,sha256=OTnJ2ihN7AMk41BUqJNk3eB2tKziim6UTrJUmTNSNWc,5420
 configs/llama2/run_llama2_70b.yaml,sha256=U-Brmax84WmO_GRm_RLWi4SEvaw2VDWNNvMhFrtSo0U,5473
 configs/llama2/run_llama2_70b_910b.yaml,sha256=KX8l4exgMnQZhEdWMdBwZ6HXn7oV2tPuar4SEVsWtSo,5332
@@ -91,25 +91,25 @@
 configs/t5/run_t5_small_on_wmt16.yaml,sha256=JSE8TBEnCYM6RAZd53vBpIU8HlyisrPmdBLDz0h2DgU,4494
 configs/t5/run_t5_tiny_on_wmt16.yaml,sha256=YcBNgrSl0YWkfXSP9YrQnXIMtY3EB1jqWkhe_rfqLz4,4455
 configs/tokcls/run_tokcls_bert_base_chinese.yaml,sha256=itd5Wk3zGYHunpUVI_oic1SKK1gwe3nwJIa4d8Y_Frs,5772
 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml,sha256=GeRdztBCVDejR5t-lsw7rtqelOB1PHA2rVVygfxHhEE,5788
 configs/txtcls/run_txtcls_bert_base_uncased.yaml,sha256=gnymRAUiIEFLScDD_cwvhuKvBMe7AnYg0eFN4357VOg,4428
 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml,sha256=7ePBhRKSAbhuiVJOSAAhRZWBKuiGcubkO3FKDiZ0lws,4438
 configs/vit/run_vit_base_p16_224_100ep.yaml,sha256=pU6CMikVOpJxeuXHvwzvsqElZPlEbmQr2NB_oPAkZAU,6020
-mindformers/.commit_id,sha256=TWXGWhxq8O9XBQwKWMmcEBeGlCLQ-hISuxHn-skbR0g,275
+mindformers/.commit_id,sha256=Q8nt0idOsPFltX9blHR02PF1JQc6XUkN-BCGzmEdE8A,277
 mindformers/__init__.py,sha256=JNBu8QJXpvwn62yczNVecdnNrlj-0ZVQU2cuvjiQ8ZE,1402
 mindformers/auto_class.py,sha256=D2dhoxNbl4g_-6UKhvsvhj0cXevaWaQyxRxYBX69B7s,39712
 mindformers/mindformer_book.py,sha256=12rKSeMC0E1sNrHHmEwHxqXm4hG7OTeuQxnk6UUNy70,67667
-mindformers/version_control.py,sha256=l1R6oh6lGmQvwincgVxU8LPkCCb5hI1LAdomhbHSYXg,9793
+mindformers/version_control.py,sha256=levUodQFe4Vg-dPzI_kkNyl_UXtWVgj6NVIRD_axtWU,11096
 mindformers/core/__init__.py,sha256=vrKepDLPXIu_WLypOsUf-bJw4PNZCAASienEdzFPBtk,1297
 mindformers/core/clip_grad.py,sha256=_jkBQhKakCIjZf_LBkzCQnEGJIz6wd8oeoB9l-9t_r4,3956
 mindformers/core/parallel_config.py,sha256=EUIbF8huDgkDgooqV43GLn_mGY5g_bLop6FCy3wlqNU,2995
 mindformers/core/callback/__init__.py,sha256=IldgJVA6wzO2z6K1Ee4ok0nchHEfdWpi1E7AASPt8iQ,809
 mindformers/core/callback/build_callback.py,sha256=yeP_njaaoNXyR1VOSyMbyIbo3f5Dd8_FHfnJ3f4tbFI,3309
-mindformers/core/callback/callback.py,sha256=I-PVwee2aDiQhuVAZGZmPe-YWHrS5N4vmfOf0gr6lY4,37103
+mindformers/core/callback/callback.py,sha256=PpKTi9vLVVjBr3SLI9NRrXtGYE1KOYUA1SD9kzfcSsM,37078
 mindformers/core/context/__init__.py,sha256=f_J_0_HrDgZaeg04q93vN2gSgZWQoTuv6nk3rTYs-4E,833
 mindformers/core/context/build_context.py,sha256=PcVyRjMgpvbJDbpzdNWGdpIwp3D4LrgNyrSDRdCeLCA,8659
 mindformers/core/loss/__init__.py,sha256=L6BFg1ZN13rN4PZAlmC_rbAE32PzL4HQwemF7elOIIc,789
 mindformers/core/loss/build_loss.py,sha256=1ekYinU7IO8gunqD-aWqwZXvzLOZJpNWiGmO5-X3sxA,2866
 mindformers/core/loss/loss.py,sha256=eYXNCe5wr5gi1QRM3CTMQDKLjCELSkWzwLzEK50aw20,18164
 mindformers/core/lr/__init__.py,sha256=r-LXKf2VvEH_utdhKI_Qr69tsW3o6bQUCxJHqtZYNEo,802
 mindformers/core/lr/build_lr.py,sha256=i5pPuKQgUINtYyxeEvdsdaJ1TBABzSZHzaKPzRUpLxw,3844
@@ -170,21 +170,22 @@
 mindformers/generation/generation_config.py,sha256=qNXgSpHLrtHFGlpl6jsPI6lhs87VCjW0zoH1uzNerGU,8660
 mindformers/generation/logits_process.py,sha256=aI37Wsqwv0ACdX53ISmSmszSry3R6GAkAA_JaqX3uKA,9729
 mindformers/generation/streamers.py,sha256=i-kyCLMX51U-_1ZX4qKUk505m-Qkmk533hrmNVArQpA,11746
 mindformers/generation/text_generator.py,sha256=2bq0mddizgyoHopUVyHdbdVCKBDhfU55MPeUsBTd19s,56513
 mindformers/generation/utils.py,sha256=pdc0-bwGdWMpTe6eHyvtljgJBmtSKbu5y65TXUjVh6Y,2956
 mindformers/inference/__init__.py,sha256=jp5KKZOJp_XZt8Ac2y8LR_df5uK9QL-1Bjbh1QthEKw,862
 mindformers/inference/context.py,sha256=pWsZLvUY5XICQCZmE46F43c_LzQKmQAEgNofVJkVv0s,1406
-mindformers/inference/infer_config.py,sha256=Iep2bY3Sba7NxY3o-tVH0GmDUmu1-R3DD1Y5vmViKWk,2267
+mindformers/inference/infer_config.py,sha256=Zfzp8WRVvH-W4Xt3cm_rsYy5DPKZGn70NeGFc1nvCNw,2533
 mindformers/inference/infer_task.py,sha256=z09VDeZ9RxbcxMaxiX4vgsvXgKa8SK9G9KX2DYMNbvw,1930
 mindformers/inference/pipeline.py,sha256=gC5FlTXU2xWPA1zMkcMm9nfSiUntqJYcDQxl5022ZmE,10700
 mindformers/inference/postprocess_sampler.py,sha256=CmjGPt5ofAqjPIIGo3PxNIPJXTOj9KfCdh67Lu7uscA,2561
 mindformers/inference/infers/__init__.py,sha256=bZH2bUaomEs9XwJxjGs6VPC1B1aBADKInSKu90vGmuc,704
-mindformers/inference/infers/base_infer.py,sha256=HOV5JPMlFfvGRj1m-Wfb-ShHW3XxSrheqKLklfEk6j4,8334
-mindformers/inference/infers/text_generator_infer.py,sha256=F9VWET-2mn6d4NhyshlCoGslqWyf72L4NW7i8QN2Jbw,26511
+mindformers/inference/infers/base_infer.py,sha256=c6k6fUe_1RiPSksy8fp5IzlYQCPKZ622JkcAxGedutc,8435
+mindformers/inference/infers/cache_engine.py,sha256=cDZroa8oifXDBYNigkqcXXJTPfQpZ1r5LGvHBsHwVIA,3234
+mindformers/inference/infers/text_generator_infer.py,sha256=NbJEEYlrpgS1rryHS0wYljaYhGJE6QEtMPVRzGvmDaA,31700
 mindformers/models/__init__.py,sha256=eR3B5cW2iEXUn56K6C1xeJOXNGgzKY0q8o_2otYcGXM,1912
 mindformers/models/base_config.py,sha256=4XepQsvgGAduTKNclHhnv4fR0eoj26MKLCKMGamy96Q,10530
 mindformers/models/base_fast_tokenizer.py,sha256=TbS6dL9b8dveqXdj8ylMrVyuDrve2VKX39cW1PdtMms,38049
 mindformers/models/base_model.py,sha256=Ak08xuZu6AWiPI151SU15oZE4D9VFzLCDozsys2MJOg,17280
 mindformers/models/base_processor.py,sha256=8cqiSmSbK4L5m4vNH7gkfH7212nQgmpTE9R2YL9Duak,13391
 mindformers/models/base_tokenizer.py,sha256=cMrZIJBchZ1pZ0AwkFlNqVQId9u8yGzVMYrMb4BfXDQ,207274
 mindformers/models/build_config.py,sha256=Q401sHTaRZtWYYnUUZZb88CbWgPedHCqQ-kC1us0F2Y,2681
@@ -253,22 +254,22 @@
 mindformers/models/gpt2/gpt2_config.py,sha256=zQYjvieIVX7yVuMR-CFlCN8igSPf2xhuB1e4ttSQjAs,8920
 mindformers/models/gpt2/gpt2_processor.py,sha256=eMN6bJFM8IZQ8dl1TVrBEfb6yq-_W5DAsbJLFIj_kwc,3430
 mindformers/models/gpt2/gpt2_tokenizer.py,sha256=ituwMCo-bUGIR4gY32AKJOkqbzURds5fZfN0yIFJrAE,14380
 mindformers/models/gpt2/gpt2_tokenizer_fast.py,sha256=sfHgjgo1_dSAq7ugSbYvB7nJ0zr0N5uPV07G5ot3WRI,9074
 mindformers/models/gpt2/gpt_modules.py,sha256=8Vn28SfRoeoeuzWtITvUkxgcIJqt_hTZPG1B1MIqJRc,11091
 mindformers/models/llama/__init__.py,sha256=5IFM9yzk7bwbZqL8jAeI5JjFV4cDcabf920vNJfqPDs,1061
 mindformers/models/llama/convert_weight.py,sha256=89i6rkYmJj6ya_WfdvfWpJWFNsi-j4kZrLc88ZbH_70,6999
-mindformers/models/llama/llama.py,sha256=KFoyDptB4SMyIVKO4z0gwGkO2X-l2mLcM8qo0Xxd1RU,23716
-mindformers/models/llama/llama_config.py,sha256=Fc_gJfCToVV-8iE40cWQN1M9G7eEa188pA6FGoPfNxk,9984
+mindformers/models/llama/llama.py,sha256=_ZDn_BihoOH1zXC4nbqfnBvL4WnC1b4t0PQpLWcr0cc,25605
+mindformers/models/llama/llama_config.py,sha256=BUdu7QwHgwFN0YZkPrREY6RpJwOKLXjFXHmiDaxoyIQ,10886
 mindformers/models/llama/llama_interleave.py,sha256=0pnIqLN2t64tiJJAAFIYe-vO9P3HFlpcDMWo5Z3zNOQ,34676
-mindformers/models/llama/llama_layer.py,sha256=yYQ0mXHvTUVUh5wNcypL-9sWEuumH4wD1LUNsVnE210,24852
+mindformers/models/llama/llama_layer.py,sha256=f02O49-6HVMkeCjCDLpkXoa2v8IBPQDa8AHi4d5B23Y,24857
 mindformers/models/llama/llama_processor.py,sha256=P_cgtSS_6J7irSjeL0NTNzfbA-5WnG4t_D2K4SzooJc,3440
 mindformers/models/llama/llama_tokenizer.py,sha256=6JblvHMlvRktCDqE80kIxmL6PRIsIVm-j6VGm4vmovM,17102
 mindformers/models/llama/llama_tokenizer_fast.py,sha256=o8ahHzZirlO1rI6rWUPf2A6v1N-tcEOCWPOge6Y59Bw,9034
-mindformers/models/llama/llama_transformer.py,sha256=7Mz3cbbya6lpfOCTM4POdADPp-IJ8nkjBfKYjLpjxaY,29200
+mindformers/models/llama/llama_transformer.py,sha256=MXyXgdadSABJVJ7SgSmlzQ8USfV0IKgRwWEu6xFkvoo,30964
 mindformers/models/mae/__init__.py,sha256=IB4gzMH574udRoFXuX0SZ8G_x9nv-cwE7yE4vaEMYnI,878
 mindformers/models/mae/convert_weight.py,sha256=b4wrOj3NUomuH3pn0EImZkNq4IPZuYA5P208ywL37QE,3416
 mindformers/models/mae/mae.py,sha256=v4Y3utC4Q5iz4keT38ZNqLYy7GqibHk5SymIWzN0XrA,18082
 mindformers/models/mae/mae_config.py,sha256=nxvMd9M_kIl_dd_UgLtSH99kr_GAan1J80fj66b-KiQ,7343
 mindformers/models/mae/mae_modules.py,sha256=DI1ZoWxa2l4-3aVRRsWhRFyBYj1hXmLYNRGDPH6aI-0,36274
 mindformers/models/mae/mae_processor.py,sha256=Mu_c86HleGDfo1N5dONRxsjMHxASauCnnqsNQgPw-Mg,6194
 mindformers/models/pangualpha/__init__.py,sha256=PqPG5THtOAbcpdo-81VvIlVLyI-4Y4CHnbSaJ83S47o,1022
@@ -303,19 +304,20 @@
 mindformers/models/t5/t5_tokenizer_fast.py,sha256=VZb0c06TZaxF4VjbQOlFfacvYxBrKz1foxhyKWW6ni8,9881
 mindformers/models/vit/__init__.py,sha256=lwRcVq4u16W9ZpF4R_DkFOOb3iUfry9ZDniUTdnZeog,948
 mindformers/models/vit/convert_weight.py,sha256=zhmsxx8Unr3iCYo6VjTcKYMKpeteyt_MROnICVm1xfg,3364
 mindformers/models/vit/vit.py,sha256=Zv0fNSe_BgEzF0iQ4LlnLhmaDCZG6LUfD6yObGHiU88,13411
 mindformers/models/vit/vit_config.py,sha256=4VgQBoOXQFh3LWBNFTquJPCAkdSFXMh_D-riCtKnVLk,8231
 mindformers/models/vit/vit_modules.py,sha256=3INeOVqVZH6JcWzJg4ZhRE-isPFoSJbpi1UPP-05aJQ,37548
 mindformers/models/vit/vit_processor.py,sha256=HSc6Cd9XogUlE88IE3vPOfFuW55G8ridwmJ545fwSxc,4925
-mindformers/modules/__init__.py,sha256=Msc-U4WeTqW_l2ZQka-t0QuqVQQ_1H8UbyGD9FuCpNU,957
+mindformers/modules/__init__.py,sha256=AFMSj9ZmfVDqslkGhxADVjJk4qequ-s9cxYQFt7f7Zw,992
 mindformers/modules/activation.py,sha256=KWbuLz7-vJTPisX8tstP9b7ibloEqlcysgX6tki__n0,50360
-mindformers/modules/kvcache_mgr.py,sha256=O2qQ0-ABLuKvkyEHLvSz24ISBKc9jIrA_eybEsdxIKo,12959
+mindformers/modules/kvcache_mgr.py,sha256=UBDn1aCujrYninEBHmLPZEvn7xaIIG9k6gMMkxw3hok,13356
 mindformers/modules/layers.py,sha256=LRLjztr2HgYYfzH6DIphUngXKzcC3V9sdK2M9HmLlMs,48060
 mindformers/modules/local_block_sparse_attention.py,sha256=Zc_Q0DMOVjBTKpRbcokuoWwiBG7lrgvXiroIJcUV2ZM,14414
+mindformers/modules/paged_attention_mgr.py,sha256=OYE4BgcJPBGdqnLGRSLakbQMlYZizenhwc352ZKumOY,4545
 mindformers/modules/transformer/__init__.py,sha256=m_AJH_u4x0yS5K9WLAQs739JFzCszpkTdZDE2D-0EF8,1335
 mindformers/modules/transformer/moe.py,sha256=uCBj9A6JzNf6uPs2BfcBmXR9sF1ZGgZLdIAh3RkOJvI,39235
 mindformers/modules/transformer/op_parallel_config.py,sha256=dd72P0OFMvJK6MuFaBZ6NXrMlEMzzvFJGTe3Fw6wiTU,8481
 mindformers/modules/transformer/transformer.py,sha256=lfGHlaK8ulLYZq4lWPCi2U1m0sbZ36di_W1eKS9PhJo,202706
 mindformers/pet/__init__.py,sha256=mRgbD5cOL1zTOdrivVS6xXMjZIOwScMM8PNciYTPcnE,897
 mindformers/pet/constants.py,sha256=XMxgWJ9Ma0aG1woOs_FjL9lnGyscb8iH3kPAO6kekMg,1176
 mindformers/pet/pet_config.py,sha256=jDjGdPJ8RmA0Aw3Gpa5ugC9mYqoaiGbhQdvv6s1JQ-g,4920
@@ -337,44 +339,44 @@
 mindformers/pipeline/image_classification_pipeline.py,sha256=2eOf-ZWujkdJkw2ErnxMYNI5T3mLhRfpAXy7QYlS858,7544
 mindformers/pipeline/image_to_text_generation_pipeline.py,sha256=dPUqdHr_pjF-hjGrRR-lIUwnc3PTaX5B7VgVP8ABGrc,7168
 mindformers/pipeline/masked_image_modeling_pipeline.py,sha256=-u2OS2ssojeviQaAiybSD2oaNcKMbzE8cdFyxMYTt4E,6591
 mindformers/pipeline/pipeline.py,sha256=u_fOspNtTBRACju9GjC6XPMAdcbanfvRDOlFF2MAqto,6226
 mindformers/pipeline/question_answering_pipeline.py,sha256=AodrigqXCENEChmVQ4W1TuFnVS-es5_uv_I_ZbBEVAM,18559
 mindformers/pipeline/segment_anything_pipeline.py,sha256=grZ3bsxpRKx8vJiTaBvmAGF3X263ALKOS74VuKcN-e0,27170
 mindformers/pipeline/text_classification_pipeline.py,sha256=__1SFkMvGvk6IyZ9jKZv3-ziFoPAM4jPJULkLDCyG0U,10648
-mindformers/pipeline/text_generation_pipeline.py,sha256=2Bq3Id8zdjtWAoCcLu_J3mTh2ykVUDKZYJ-yNJ07bOA,10875
+mindformers/pipeline/text_generation_pipeline.py,sha256=UYIA5pBb6X419ZPdQR_0ZJockYj0qUHXP82H8T0IYGU,10971
 mindformers/pipeline/token_classification_pipeline.py,sha256=TU3VoHVvDSVIoYiqALiKPQVexaqKGNe6jhcwfbA1P5M,10121
 mindformers/pipeline/translation_pipeline.py,sha256=ss4nyAzqtEI1pwZHDhbQGfGp5NeNBfP5bHXsMeq0w9c,8167
 mindformers/pipeline/zero_shot_image_classification_pipeline.py,sha256=ePHlHQoJQtT22h07Lrs3eUUHyH4OS2iRAr9ADgHof2k,9276
 mindformers/tools/__init__.py,sha256=ZbtDJmB8EVDBalSdDZKgeS_RpWjGKqXFkNffvBQOmAg,1127
 mindformers/tools/check_rules.py,sha256=uBT9PbICELdFj5qYqdkgeTUzTWoVeuEL-I0HsL_ziV4,10202
 mindformers/tools/download_tools.py,sha256=P26bZQX101wIOUqm_2k0v_KUa81wfWu0Z7ttSxQS02U,4399
 mindformers/tools/download_tools_multithread.py,sha256=jQWWx4Qgikw5AGWDjSw5bPxvvPtGVGNU5BBpbHlrfic,6243
-mindformers/tools/export.py,sha256=4XrAQ8uiyCZGpVSrKKmj8BH4P5t2Azlfp7KlfdqcGLU,12212
+mindformers/tools/export.py,sha256=RJWeG7C8oKm1d0ls23VI8vaU1U1-tchHxHeTGqKiM1I,12033
 mindformers/tools/hccl_tools.py,sha256=oKJw__LN1fE29NH76OEdyvnZF_cKxob6EbJxWMz2vrY,6520
 mindformers/tools/image_tools.py,sha256=FAgyl164Emo1V-tAxE0jUXU1OGl9lLwi9-FGbtYAeLY,1925
 mindformers/tools/logger.py,sha256=hQBTXKEkP5syL4f4_LV8g4R8RtZ-FEeIMdbYnhefkLc,22670
 mindformers/tools/merge_hccl.py,sha256=LygpRvgIJz2_WkNVMES5j5JpHGmYZmqZcP1IsA_GbDY,2316
 mindformers/tools/moe_token_distribution_tools.py,sha256=riNrEXrm9zqAzfVfqNsgM2W0apcAFoppBM5kG_2aelo,5598
 mindformers/tools/transform_ckpt.py,sha256=UzPq2hyzu4WWHTi62GBBdFRTA8wFTyFHfQUcYL-kMNM,3164
 mindformers/tools/utils.py,sha256=E37ZGEov2zbIPqwlLKHOe9DSCTGOtT1qJcXVTO1ylJo,13716
 mindformers/tools/cloud_adapter/__init__.py,sha256=Sj46iusOGBVYh5mx1SE8j1JVNYIxj96toskg4QezNi8,842
 mindformers/tools/cloud_adapter/cloud_adapter.py,sha256=URV7BYDEtL0FsCv5UxsLON6CIrVUVdh3ILUfXdwafls,8522
 mindformers/tools/cloud_adapter/cloud_monitor.py,sha256=Bwx9lvzDXMkcHOmizg4AuNrykic7RPUMUJ-g8e4ILMo,3464
 mindformers/tools/register/__init__.py,sha256=04DTJlQ3pYL8VAJWKS9fKKN15JEFkPXk7CiljsnYObU,905
 mindformers/tools/register/config.py,sha256=0ww03GFoeni3DwI5H71eAjPX44pFWF5Y6ZkFv1bXlBw,11037
 mindformers/tools/register/register.py,sha256=QJGTgZaoPB7EMAOYb5d0UIa6mFOYrvEaCGi1ixh62nE,7056
 mindformers/trainer/__init__.py,sha256=uzTXgoCNl08_kxVpUAtZ6xCsLkv4pfBXh17XPOgm3U0,1982
-mindformers/trainer/base_trainer.py,sha256=X2_xaXwLS7fq4nKrLRKBKm7vTOldqSFy_dU_9A-nNS0,51109
+mindformers/trainer/base_trainer.py,sha256=zmP9YfnGvWs4A31H7eOF_wAXdOBQCDCh5ijmjmdjJNs,52479
 mindformers/trainer/build_trainer.py,sha256=RvcstU-InuwHWkO0c_rV4B08kxH0xlU6frIGxSOC83I,4428
 mindformers/trainer/config_args.py,sha256=DL9aGSp3UCPnQnDyDvLtsz0M29S69r8z38X909hfArg,55851
 mindformers/trainer/optimizer_grouped_parameters.py,sha256=CJ3605L3wWTz41cdPp_GxC0L7SJ4Q6A6xlwMSzNzJas,5925
 mindformers/trainer/trainer.py,sha256=SyKYvMFpTVQl3Mcmr1cCiYWCbupmtR0ySt2pxFlThsc,51978
 mindformers/trainer/training_args.py,sha256=vJYyPFq_bi6oJI_4qrperLd3WGic6mv9XVC33Pyw0KM,15070
-mindformers/trainer/utils.py,sha256=1h6IUxOfHDyBIZdGhNwfAjk_5m517KlslXKlu-VY7bE,34430
+mindformers/trainer/utils.py,sha256=sWBNU1rM9vYaKmJ33Dc536d4LjXA620MpAM3g4g6WjA,35015
 mindformers/trainer/causal_language_modeling/__init__.py,sha256=ajvw2Xr-stOAdwVS56K7n1Ctx6iVIv91kktl_AylBts,821
 mindformers/trainer/causal_language_modeling/causal_language_modeling.py,sha256=G7q9RedMhaI8z8Q3QWzUfnTL7GJZ0H8KHORX9uOftBw,20146
 mindformers/trainer/contrastive_language_image_pretrain/__init__.py,sha256=_2QLmjdq0aYkYYeGQQGdrv2UuRNokZaGgPTlkicFXKM,866
 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py,sha256=vWGBhQyGr-fP9tR96GPn0_pXuudwN623hiaLLHRHO8A,4609
 mindformers/trainer/general_task_trainer/__init__.py,sha256=KQhPQnJv5tkVZ7hsfN5JVG7Vdjd3rDrn0xH9y2htnNQ,795
 mindformers/trainer/general_task_trainer/general_task_trainer.py,sha256=AVKoRs49Gdio0O0hks699zxhJvixBpGm2adSBoX-6zw,9462
 mindformers/trainer/image_classification/__init__.py,sha256=6mp76hxxRWCQ9H2YdoVB8FJ-RTbrfFnHb79rd8h5Qr4,928
@@ -396,15 +398,15 @@
 mindformers/trainer/text_classfication/__init__.py,sha256=QZijWXRX-_iq8tb8kps_IuntZZhOfMthdo6nE0IaVEA,803
 mindformers/trainer/text_classfication/text_classification.py,sha256=tLNVRjDfCc73xzfGG_qPQ_Y53ZGEBv0LQLviF9D9QFo,9386
 mindformers/trainer/token_classification/__init__.py,sha256=a_5Y4Kk-pNCooniku6ewvhhc8zzzirM3xZJ33-q4lxw,807
 mindformers/trainer/token_classification/token_classification.py,sha256=ptx17at-E7J5xWNCYvfygmszr4t-aU2nCQpeiINe2lg,9500
 mindformers/trainer/translation/__init__.py,sha256=ghZF7nb3MoZBOhBg5WJxhzT5-Xzdp8TTZU4e5cjWww8,795
 mindformers/trainer/translation/translation_finetune.py,sha256=92ovnBd81YaZqeLrSwdz3omItgGkbdQBpfdKq5j_znI,6738
 mindformers/wrapper/__init__.py,sha256=XjjgWv6F36VtMMoD6FQDNPCI1AlGFoZqitVPvDVLh0s,913
-mindformers/wrapper/adaptive_loss_scale.py,sha256=jaU0q0GDehvlMvs69ALoxmHWxVlYvb9-FX6Vxu76xzA,14329
+mindformers/wrapper/adaptive_loss_scale.py,sha256=VCQdTjFb5GtkdfUwSMdk6h9dmYVLVQi55FoZowO4qEU,14329
 mindformers/wrapper/build_wrapper.py,sha256=MoEaObzg8VjfaYcZAZp3BwLX9eHLa7COwCrSSpsUJ2U,4117
 mindformers/wrapper/wrapper.py,sha256=RafxPlbwUL4KUmPJMusnsbnSYwEK1pJLiVjDjsCcq90,12204
-mindformers-1.0.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-mindformers-1.0.0.dist-info/METADATA,sha256=_YESW4_g9ezWdhShcJxLGG6-iF7HSw3D90wsMCnHf_E,24851
-mindformers-1.0.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mindformers-1.0.0.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
-mindformers-1.0.0.dist-info/RECORD,,
+mindformers-1.0.1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+mindformers-1.0.1.dist-info/METADATA,sha256=dt5Tka4EbvJlE9CjpNGIP0owKA9ObqIWqMWfdobAXrs,19262
+mindformers-1.0.1.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mindformers-1.0.1.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
+mindformers-1.0.1.dist-info/RECORD,,
```

